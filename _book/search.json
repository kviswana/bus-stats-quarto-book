[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Business Statistics: A Modeling Approach",
    "section": "",
    "text": "Introduction to Business Statistics\nA Modeling and Regression-Based Approach\nThis text presents a modern, modeling-first approach to business statistics. Rather than a traditional probability-first sequence, we begin with data, relationships, patterns, models, and regression — the tools most relevant to business decision-making in marketing, finance, economics, HR, and analytics.\nYou will learn:\n\nhow to visualize data\n\nhow to describe relationships\n\nhow to build regression models\n\nhow to interpret uncertainty\n\nhow to make data-driven decisions\n\nLet’s get started.",
    "crumbs": [
      "Introduction to Business Statistics"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "This book accompanies the course Introduction to Business Statistics (BQUA2811) at Stillman School of Business. It takes inspiration from Lessons in Statistical Thinking by Daniel Kaplan (LST). LST takes a modeling-centric approach and changes the ordering of topics significantly from traditional introductory statistics courses. LST uses almost no mathematical notation, and only provides intuitive explanation for most concepts – which serves a first statistics course for undergraduate business students very well.\nAfter teaching the course for two semesters broadly using the LST approach, I found some need for changes. The following main drivers shaped the changes:\n\nStudents benefit from understanding structure before encountering uncertainty\nA modeling-centric approach helps to provide an overarching course focus\nCourses that depend on BQUA2811 primarily expect students to be competent in linear regression and related statistics concepts.\n\nI wanted to provide more of a business focus and use more business-related data sets than LST does. More importantly, I felt that LST introduces statistical models very early.Even the very first plots – lines and point estimates – that students encounter, show confidence bands. Despite emphasizing the difference between mathematical models and statistical models and providing many examples and illustrations from daily life, I found that students had a very hard time understanding the idea of a statistical model.\nI surmised that teaching the idea of models and concepts relating to uncertainty together made it very challenging for students. Therefore I decided to make a big change and restrict the initial discussion of models to just the data set used for modeling without bringing up the idea of inference from the model. Once students grasp the idea of a least squares model independent of uncertainty, learn what it means to explain variation, and eventually grasp R-squared, we can then layer on uncertainty that arises when we use a model in broader contexts.\nI also learned that even in developing the notion of a model, it is pedagogically very useful to ramp it up gradually starting from mean as the simplest model (when we have no explanatory variables), category means as models when we have a categorical explanatory variable, and finally extending to full-blown least-square models when the explanatory variable is also numerical. Once students grasp the idea of model, it then becomes easier to introduce uncertainty, probability and confidence intervals.\nBecause of this change, I needed the initial plots to only plot lines instead of confidence bands. However the plotting function from the LSTbook package that accompanies LST only provides bands. I therefore created a package named LSTextras to extend the functionality of LSTbook package in a very small way to enable plotting of point estimates and linear models without confidence bands. I also included several business-related data sets in the package. Combined with the data sets already available in LSTbook and the other packages like ggplot2 that it depends on, students using LSTextras will ave no dearth of data to play with and experiment with the concepts that they learn.\nThe book is organized into 14 modules reflecting this story line.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/01-overview.html",
    "href": "modules/01-data-visualization/01-overview.html",
    "title": "Module 1: Data, Visualization & Patterns",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, point plots and relationships. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "Module 1: Data, Visualization & Patterns"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/01-overview.html#learning-goals",
    "href": "modules/01-data-visualization/01-overview.html#learning-goals",
    "title": "Module 1: Data, Visualization & Patterns",
    "section": "",
    "text": "Describe data frames, variables, and instances\n\nPerform simple manipulations of data frames using R\nCreate and interpret scatterplots using the point_plot function\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "Module 1: Data, Visualization & Patterns"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/01-overview.html#structure-of-this-module",
    "href": "modules/01-data-visualization/01-overview.html#structure-of-this-module",
    "title": "Module 1: Data, Visualization & Patterns",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nScatterplots\nRelationships between variables",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "Module 1: Data, Visualization & Patterns"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html",
    "href": "modules/01-data-visualization/data-frames.html",
    "title": "1  Data Frames and Variables",
    "section": "",
    "text": "Learning outcomes\nWe store data in an R object called a Data frame. This chapter introduces data frames, explains their key components and teaches you how to get some basic information from them. We will learn more in later chapters.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#learning-outcomes",
    "href": "modules/01-data-visualization/data-frames.html#learning-outcomes",
    "title": "1  Data Frames and Variables",
    "section": "",
    "text": "After working though this chapter, you will be able to:\n\nExplain the term data frame\nExplain what a column and row of a data frame represent\nName the two types of variables that we will deal with in this course\nDistinguish between numerical and categorical variables and provide examples of each\nExplain what the term level of a categorical variable represents, and provide examples\nExplain why a column is also called a variable\nExplain why a row is called an instance or specimen\nDemonstrate the following skills related to R:\n\nload a data frame from a loaded package\nexplain what the pipe operator does\nlook at the first few rows in a data frame\nlook at the last few rows in a data frame\nretrieve only the rows that satisfy certain conditions\nretrieve a subset of the columns of a data frame\nfind the number of rows and columns in a data frame\ncompute the average of a numeric column in a data frame\nuse the count function to compute the number of rows in a data frame for each level of a categorical variable",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#the-r-software-package-for-statistics",
    "href": "modules/01-data-visualization/data-frames.html#the-r-software-package-for-statistics",
    "title": "1  Data Frames and Variables",
    "section": "1.1 The R Software Package for Statistics",
    "text": "1.1 The R Software Package for Statistics\nIn this course, we will be using the R software package for all computations and visualizations. While R offers a great deal of functionality, we will be using a limited subset that suffices for the course topics. In particular, we will be using two specific R packages that contain all the functions and data we need for the course. You should install R and RStudio using the instructions provided in Chapter 32. That section also shows you how to use RStudio and how to load a package – specifically the LSTbook and LSTextras packages – into R.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#r-data-frames",
    "href": "modules/01-data-visualization/data-frames.html#r-data-frames",
    "title": "1  Data Frames and Variables",
    "section": "1.2 R Data Frames",
    "text": "1.2 R Data Frames\nIn R, we generally represent data in a simple tabular structure named data frame – a table with rows and columns. Table 1.1 shows an example of some initial rows from the Boston_marathon data frame contained in the LSTbook package.\n\n\n\n\nTable 1.1: Boston Marathon data (first 10 rows).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nname\ncountry\ntime\nsex\nminutes\n\n\n\n\n2022\nEvans Chebet\nKenya\n02:06:51\nmale\n127\n\n\n2021\nBenson Kipruto\nKenya\n02:09:51\nmale\n130\n\n\n2019\nLawrence Cherono\nKenya\n02:07:57\nmale\n128\n\n\n2018\nYuki Kawauchi\nJapan\n02:15:58\nmale\n136\n\n\n2017\nGeoffrey Kirui\nKenya\n02:09:37\nmale\n130\n\n\n2016\nLemi Berhanu\nEthiopia\n02:12:45\nmale\n133\n\n\n2015\nLelisa Desisa\nEthiopia\n02:09:17\nmale\n129\n\n\n2014\nMebrahtom “Meb” Keflezighi\nUnited States\n02:08:37\nmale\n129\n\n\n2013\nLelisa Desisa\nEthiopia\n02:10:22\nmale\n130\n\n\n2012\nWesley Korir\nKenya\n02:12:40\nmale\n133\n\n\n\n\n\n\n\n\nWe first define the term data frame and then explain some terms that the definition uses.\n\n\nData frame: A structured data set organized as rows and columns, where rows correspond to individual observations and columns correspond to variables, with each column containing values of the same type.\n\n\nA data frame contains data about some topic. Each row represents an occurrence of the topic. For example, Table 1.1 contains data about the results of the Boston Marathon over the years. Each row represents the result for one year, for one gender. We also refer to a row as an observation, or as an instance, or in a medical or biology context, as a specimen.\nEach column represents some attribute of interest about instances. In Table 1.1, we have columns to represent the year of the race, the name, county and sex of the winner and the finishing time.\nIn statistics, we generally refer to columns of a data frame as variables. We explain why in Section 1.3.\nIn the 1880’s, Francis Galton was developing ways to quantify the heritability of traits. As part of this work, he collected data on the heights of adult children and their parents’ heights. Table 1.2 shows the initial few rows of that data.\n\n\n\n\nTable 1.2: Galton data (first 10 rows).\n\n\n\n\n\n\nfamily\nfather\nmother\nsex\nheight\nnkids\n\n\n\n\n1\n78.5\n67.0\nM\n73.2\n4\n\n\n1\n78.5\n67.0\nF\n69.2\n4\n\n\n1\n78.5\n67.0\nF\n69.0\n4\n\n\n1\n78.5\n67.0\nF\n69.0\n4\n\n\n2\n75.5\n66.5\nM\n73.5\n4\n\n\n2\n75.5\n66.5\nM\n72.5\n4\n\n\n2\n75.5\n66.5\nF\n65.5\n4\n\n\n2\n75.5\n66.5\nF\n65.5\n4\n\n\n3\n75.0\n64.0\nM\n71.0\n2\n\n\n3\n75.0\n64.0\nF\n68.0\n2\n\n\n\n\n\n\n\n\nIn Table 1.2, each row represents an adult child. The variables in this data frame represent the father’s height (father), mother’s height (mother), sex of the adult child (sex), height of the adult child (height), and the number of children (nkids) in the family. The data frame also has a unique number for each family (family).",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#sec-variables",
    "href": "modules/01-data-visualization/data-frames.html#sec-variables",
    "title": "1  Data Frames and Variables",
    "section": "1.3 Variables",
    "text": "1.3 Variables\nIf we look at a column of a data frame, say the column time in Table 1.1, we see that each row can potentially have a different value in this column. More generally, the values in a column of a data frame can vary from row to row, leading to naming columns of a data frame variables. Going forward, we will use the term variable to refer to a column of a data frame.\nIn this course, you will be working with many data frames. You should always be sure to understand what a row of a data frame contains information about. In the case of Table 1.1, each row represents the results of one race. The races for different genders are different races, even though people of both genders run together in marathons . In the case of Table 1.2, each row represents one adult child.\n\nunit of measurement: The object or concept about which each row stores information.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#numerical-and-categorical-variables",
    "href": "modules/01-data-visualization/data-frames.html#numerical-and-categorical-variables",
    "title": "1  Data Frames and Variables",
    "section": "1.4 Numerical and Categorical Variables",
    "text": "1.4 Numerical and Categorical Variables\nIn Table 1.1, the variable minutes has stores numbers. We can such variables numerical variables.\nOn the other hand, the variable sex can take on only the values male or female, which are not numbers Similarly, the variable country can also take on non-numeric values. We call such variables categorical.\nThe individual values possible or allowed for a particular categorical variable are called its levels. Thus the levels for the variable sex are male and female. are the levels of the variable sex. The levels for the variable country are country names like Kenya, Japan, and Ethiopia.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#data-frames-in-r",
    "href": "modules/01-data-visualization/data-frames.html#data-frames-in-r",
    "title": "1  Data Frames and Variables",
    "section": "1.5 Data Frames in R",
    "text": "1.5 Data Frames in R\nThis section teaches you how to use data frames in R. We will predominantly use data frames built in to the LSTbook and LSTextras packages. If you have not already done so, you should install R and RStudio and learn how to use RStudio using the instructions from Chapter 32.\n\n1.5.1 Viewing the first few rows of a data frame\nIn an RStudio command prompt you can enter the name of the data frame to get a preview of its contents.\n\nBoston_marathon\n\n\n  \n\n\n\nLet us understand the output above. The first row says:\n# A tibble: 175 × 6\nThis says that the data frame (also called tibble) has 175 rows and 6 columns.\nThe second line mentions the column (variable) names. The third line has some specific notation within angle brackets below each variable. This is telling us what type of value is stored in each column.\n\n&lt;dbl&gt; means that the value in a column is a double, that is, number that could potentially have a fractional part.\n&lt;chr&gt; means that the column has character or non-numeric values\n&lt;time&gt; tells us that the column stores a time - and so on.\n\nIf the data frame has many columns, and all of them will not fit in the width of your R console, it will only display the columns that will fit and provide information below about the additional columns. In the present case all columns do fit and so there is no additional information provided.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#tidy-data",
    "href": "modules/01-data-visualization/data-frames.html#tidy-data",
    "title": "1  Data Frames and Variables",
    "section": "1.6 Tidy data",
    "text": "1.6 Tidy data\nWe encounter tabular data everywhere, each one formatted for the specific purpose it serves. Not all of them lend themselves easily to analysis, especially with software tools. To be suitable for analysis by software tools, they need to follow some minimal set of conventions. Data analysis packages can work very well with tidy data. In a tidy data frame:\n\n** Rows represent single instances:** Each row represents exactly one instance of the unit of observation. It should not happen that one row represents a person and another row represents a university\n** All values in a column are of the same type:** Each column represents a variable and every row has the same type of value for that variable. That is, if we have a variable named salary then every row represents salary in the same way, for example in dollars and as a number. It cannot be the case that one row represents salary in dollars and another in euros. Or that one row represents salary in dollars and another in thousands of dollars\nCell values are atomic: A cell sits at then intersection of a row and a column. Each cell contains an atomic value that cannot be further decomposed into units that have some specific meaning for the analysis context. For example, in a university context, we cannot have a single cell that contains several course numbers. This would not be allowed because the list of several course numbers can be split up into individual course numbers and these still have specific meaning in a context. What about a cell that contains the name of a product (like ““soap”). That can be split up into the individual letters “s”, “o”, “a”, and, “p”. However, this does not count, since, unlike the course number example, the individual letters do not have any specific context-related meaning.\n\n\n1.6.1 Functions\nYou are likely familiar with the term function as used in mathematics. You provide an input to the function, it performs some computation and provides an output. For example, we might have a function that takes a number as input and produces its square as output. Given the number 5 as input, it will produce the number 25 as its output. Given the number 1.5 as input, it will produce its square 2.25 as its output.\nPictorially we might represent a function as a box that has one or more inputs and produces a single output. (We can also have functions that take no inputs and produce an output, but we will not consider them now.) See Figure 1.1.\n\n\n\n\n\n\nFigure 1.1: A function that squares its input\n\n\n\nWe might have functions that take more than one input. For example, a function might take two inputs x and y, and produce 23 + 3x + 4y as output. Figure 1.2 shows a function with two inputs.\n\n\n\n\n\n\nFigure 1.2: A function that computes 23+3x+4y where x and y are its inputs\n\n\n\nIn computing, we commonly refer to a function’s inputs as its arguments. Figure 1.3 illustrates this.\n\n\n\n\n\n\nFigure 1.3: We generally refer to a function’s inputs as its arguments – this function has two arguments, x and y\n\n\n\nThe functions we considered in our examples thus far in Figure 1.1, Figure 1.2, and Figure 1.3 have been so simple that we were able to show the actual computation in the box representing the function. Most functions we use in real-life are more complex and we give them evocative names. In such cases, it makes more sense to place the name of the function in the box. Figure 1.4 shows a generic representation of a function named cost with three arguments named arg1, arg2, and arg3.\n\n\n\n\n\n\nFigure 1.4: Pictorial representation of a function named cost that has three arguments\n\n\n\nWhen we use a function to perform some computation, we are said to invoke the function. We generally invoke a function by using its name and by supplying the argument values in parentheses, as Figure 1.5 shows. We also use the term passing arguments to functions to refer to the act of supplying the inputs to a function. We also refer to the result of a function as its return value. We will also speak of a function as returning something. Figure 1.5 also shows that a function argument can be numeric or non-numeric.\n\n\n\n\n\n\nFigure 1.5: Invoking a named function and storing the function’s return value in a variable named total\n\n\n\nIn the next section, we look at using functions to perform computing in R. At that time we will look at a different way of passing arguments to functions.\nAlmost all forms of computing, including statistical computing, make significant use of functions. In this course, many of the functions we use take data frame as input and produce a data frame as output. Let us now look at how we use functions in R to work with data and to perform statistical computations.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#using-r-functions",
    "href": "modules/01-data-visualization/data-frames.html#using-r-functions",
    "title": "1  Data Frames and Variables",
    "section": "1.7 Using R functions",
    "text": "1.7 Using R functions\nWe will now look at some common R functions that we will use repeatedly in this course.\n\n1.7.1 nrow\nWe will consider the nrow() function that allows us to find how many rows a data frame contains. We pass a data frame as argument to the nrow() function and it returns a number specifying how many rows the argument data frame has. For example, to find how many rows the Boston_marathon data frame has, we can use the following code.\n\nnrow(Boston_marathon)\n\n[1] 175\n\n\nIn the code above, we used the style of function invocation that Figure 1.5 introduced.\n\n\n1.7.2 The pipe operator\nIn this course we also use another method of passing an argument to a function – via pipes. W first see the code and then dissect it. We will invoke the nrow() function again, but using a different approach. Like the previous code, this also finds the number of rows in the Boston_marathon data frame.\n\nBoston_marathon |&gt; nrow()\n\n[1] 175\n\n\nIn the code above, we are still passing the Boston_marathon data frame as an argument to the nrow() function, but we are passing it along a pipe which appears in the code as “|&gt;”.\nFigure 1.6 explains the different components in the code.\n\n\n\n\n\n\nFigure 1.6: Using the pipe operator to supply the Boston_marathon data frame as an argument to the norw function\n\n\n\nThe pipe operator “|&gt;” has the effect of injecting whatever appears on its left hand side as an argument to the function on the right hand side. Figure 1.7 makes this very explicit by literally depicting the pipe operator “|&gt;” as a physical pipe.\n\n\n\n\n\n\nFigure 1.7: Seeing the pipe operator literally as a pipe though which the argument flows into a function\n\n\n\nGoing forward, we will use this pipe approach extensively, but also mix in the earlier approach in some places. You will easily adjust to our patterns.\n\n\n1.7.3 ncol\nWe can use the ncol() function to find the number of columns in a data frame.\n\nBoston_marathon |&gt; ncol()\n\n[1] 6\n\n\nThe above shows us that the Boston_marathon data frame has 6 columns or variables.\n\n\n1.7.4 head\nYou can use the head() function to preview the first several rows of a data frame. This time we will use the Births2022 data frame from the LSTbook package.\n\nBirths2022 |&gt; head()\n\n\n  \n\n\n\nThe above previewed the first six rows of Births2022 by default because we did not specify how many rows we wanted. We can specify that explicitly. The following previews the first 10 rows.\n\nBirths2022 |&gt; head(10)\n\n\n  \n\n\n\n\n\n1.7.5 names\nWe use this to find the names of the columns in a data frame.\n\nBirths2022 |&gt; names()\n\n [1] \"month\"           \"dow\"            \n [3] \"place\"           \"paternity\"      \n [5] \"meduc\"           \"feduc\"          \n [7] \"married\"         \"fage\"           \n [9] \"mage\"            \"total_kids\"     \n[11] \"interval\"        \"prenatal_start\" \n[13] \"prenatal_visits\" \"mheight\"        \n[15] \"wt_pre\"          \"wt_delivery\"    \n[17] \"diabetes_pre\"    \"diabetes_gest\"  \n[19] \"hbp_pre\"         \"hbp_gest\"       \n[21] \"eclampsia\"       \"induction\"      \n[23] \"augmentation\"    \"anesthesia\"     \n[25] \"presentation\"    \"method\"         \n[27] \"trial_at_labor\"  \"attendant\"      \n[29] \"payment\"         \"apgar5\"         \n[31] \"apgar10\"         \"plurality\"      \n[33] \"sex\"             \"duration\"       \n[35] \"menses\"          \"weight\"         \n[37] \"living\"          \"breastfed\"      \n\n\n\n\n1.7.6 summarize\nWe often calculate summaries (like the average of a variable) of the data contained in a data frame.\nThe code below shows how to compute the average of the time variable in the Boston_marathon data frame.\n\nBoston_marathon |&gt; \n  summarize(avg_time = mean(time))\n\n\n  \n\n\n\nIn the above code, we passed the Boston_marathon data frame via a pipe to the summarize function. Within the summarize function, we have used the R function mean to compute the average by passing the variable time as an argument to the mean function. Note that we did not use the pipe to pass an argument to the mean function.\nWe chose to name the return value from the mean function as average time. You do not necessarily need to name the summaries we compute in the summarize function, but we strongly recommend it.\nAt appropriate points in the course, we will discuss other examples of summaries. We will see variance, standard deviation, and, correlation coefficient, among others, at appropriate points in the course.\nFor most of the summaries we compute we will use the general summarize function. Within the summarize function, we use specific functions depending on the kind of summary we compute. In the previous example, we used the mean function for the average. Later we will see other functions to use within the summarize function.\n\n\n1.7.7 count\nThe count() function comes in handy when we want to count how many occurrences of each distinct value are present in a variable. For example, suppose we want to find how many rows are there for each sex in the Boston_marathon data frame, we can do the following.\n\nBoston_marathon |&gt; count(sex)\n\n\n  \n\n\n\nSimilarly, if we wanted to find out how many times each country occurs, we can do this.\n\nBoston_marathon |&gt; count(country)\n\n\n  \n\n\n\nAs you can see, the count() function comes in handy when a variable is categorical. It also works for numerical variables, but will apply much more rarely in these cases.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#sec-missing-values",
    "href": "modules/01-data-visualization/data-frames.html#sec-missing-values",
    "title": "1  Data Frames and Variables",
    "section": "1.8 Missing values (NAs)",
    "text": "1.8 Missing values (NAs)\nSuppose the HR department of a company has a data frame of information about employees with variables (columns) employee_id, first_name, last_name, base_salary, phone_no, and base_office. It is not essential that we have data on every variable for every row. For example, it could be the case that the company does not have a phone_no for some employees and does not have a base_office for some. These are called missing values. A missing value denotes something whose value we do not know. A missing value is not the same as a blank character value or a zero numeric value. Blanks and zeros are valid and known values for variables. A missing value is different – we do not know the value.\nTable 1.3 shows a data frame with no missing values. In each row, we see values for every variable.\n\n\n\n\nTable 1.3: A data frame with no missing values – in every row, every variable has a value\n\n\n\n\n\n\nchannel\nadvertising_spend_k\nweekly_sales_k\n\n\n\n\nSearch Ads\n28.1\n152\n\n\nSearch Ads\n30.6\n152\n\n\nSearch Ads\n93.4\n369\n\n\nSearch Ads\n51.3\n289\n\n\nRetail Promo\n54.6\n175\n\n\nSearch Ads\n86.6\n342\n\n\nRetail Promo\n46.5\n169\n\n\nRetail Promo\n41.4\n142\n\n\nSearch Ads\n78.1\n322\n\n\nRetail Promo\n41.1\n182\n\n\n\n\n\n\n\n\nOn the other hand Table 1.4 shows a small data frame with missing values.\n\n\n\n\nTable 1.4: A data frame with missing values – the NAs in columns phone_no and base_office represent missing values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nemployee_id\nfirst_name\nlast_name\nbase_salary\nphone_no\nbase_office\n\n\n\n\n10210\nBetty\nChu\n67000\n+1 (777) 555-1212\nNew Jersey\n\n\n24532\nJason\nFingleton\n85000\n+1 (732) 555-1212\nNA\n\n\n56437\nShim\nSung\n76000\nNA\nNew York\n\n\n20320\nRavi\nShankar\n100000\n+91 81487 03210\nNew Delhi\n\n\n67582\nEmily\nWeitz\n87000\nNA\nSan Francisco",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#codebook",
    "href": "modules/01-data-visualization/data-frames.html#codebook",
    "title": "1  Data Frames and Variables",
    "section": "1.9 Codebook",
    "text": "1.9 Codebook\nBefore using a data frame, we need to understand it well. That means, at the very least, that we know the unit of observation, the meaning of each variable and the units of measurement for a variable where applicable. The codebook for a data frame provides this information. You can use the ? operator for this purpose as the example code below shows.\n\n?Boston_marathon\n\nThis command will show the codebook in the bottom right pane of RStudio.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html",
    "href": "modules/01-data-visualization/point-plots.html",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "",
    "text": "Learning outcomes\nThis chapter covers the basics of generating and interpreting scatterplots.\nThe term statistics often conjures up thoughts of numerical computations. However, before we perform various computations, we often get a feel for the data and the underlying patterns by first visualizing the data. In this course we will use only a few standard plots to communicate essential statistical ideas.\nIn fact, use a single function – point_plot – to generate all of our plots. Although R offers numerous, sophisticated plotting features to plot almost anything we can imagine, we will keep things simple and focus only on what we need to support the concepts that our course covers.\nInterpret point plots - Explain what a point means (x value and y value). - Map between plot ↔︎ row in a data frame. - Describe direction/strength/absence of relationship.\nUse R to generate point plots\nExplain and interpret correlation - Explain what correlation measures (direction + strength of linear association) and interpret sign/magnitude.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#learning-outcomes",
    "href": "modules/01-data-visualization/point-plots.html#learning-outcomes",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "",
    "text": "Use point_plot(y ~ x) for numeric–numeric and point_plot(y ~ x_cat) for numeric–categorical.\nUse + color_var and + facet_var in the formula.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#plotting-the-relationship-between-two-numerical-variables",
    "href": "modules/01-data-visualization/point-plots.html#plotting-the-relationship-between-two-numerical-variables",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "2.1 Plotting the relationship between two numerical variables",
    "text": "2.1 Plotting the relationship between two numerical variables\nWe will rely extensively on a single plotting function named point_plot(). It lives in the LSTbook package. We will be loading both the LSTbook package and the LSTextras package. Once we load these packages the point_plot() function becomes available for us to use.\nLet us first look at the price_demand data frame that is built into the LSTextras package and is therefore available as soon as you load the package.\nA hypothetical company has gathered data for a single product. Over time, and across very similar sales areas, the company has charged different prices for the same product, It has recorded the monthly sales for each price. The data frame price_demand contains the price and the corresponding sales quantities. We would like to study if the two variables price and sales_qty are related in any way.\nFirst let us see some rows from the data frame.\n\n\n\n\nTable 2.1: Price Demand data (first 15 rows)\n\n\n\n\n\n\nprice\nsales_qty\n\n\n\n\n11.8\n1260\n\n\n12.1\n1215\n\n\n13.6\n1214\n\n\n12.3\n1298\n\n\n12.4\n1160\n\n\n13.7\n1138\n\n\n12.6\n1174\n\n\n11.2\n1220\n\n\n11.7\n1313\n\n\n11.9\n1283\n\n\n13.3\n1170\n\n\n12.6\n1247\n\n\n12.6\n1283\n\n\n12.3\n1263\n\n\n11.8\n1259\n\n\n\n\n\n\n\n\nThe full data frame has 500 rows. With so many rows, we cannot easily get a handle on the overall pattern by looking at the data directly. Visualizing the data might help us. We will do several things in this section:\n\ngenerate the plot using R code\nunderstand how each element of the plot is related to the original data frame\nunderstand the various elements of the code so that you can use similar code to generate plots that you many need\ninterpret the plot\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIn the price_demand data frame, what does one row represent?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe price charged during a particular month and the corresponding sales quantity..\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIn the price_demand data frame, which variable does a company have control over or that it can set and which one is the outcome variable?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nprice is the input/explanatory variable; sales quantity is the outcome/response\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIf the data frame has 500 rows, how many (price, sales_qty) pairs exist?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\n500\n\n\n\nThe code and the plot appear below. For now, ignore the code. We will discuss it in detail after analyzing the plot itself.\n\nprice_demand |&gt; \n  point_plot(sales_qty ~ price)\n\n\n\n\n\n\n\nFigure 2.1: Price demand relationship\n\n\n\n\n\nFigure 2.1 shows the relationship between price and sales_qty.\n\n\n\n\n\n\nQuick Check\n\n\n\nFigure 2.1 which variable appears on the x-axis and which one on the y-axis?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nx-axis: price y-axis: sales_qty\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat two numbers determine the location of a point in Figure 2.1?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIts x-value (price) and its y-value (sales_qty)\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIf a point is at x = 14 and y = 1100, what does it mean in context?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThere was an observation (row in the price_demand data frame) where the price was about 14 and the monthly sales quantity was about 1100.\n\n\n\nNote the following about the plot:\n\nThe plot has two axes – the x-axis along the width and the y-axis along the height.\nThe plot shows the values of price on the x-axis and the values of sales_qty on the y-axis.\nIf you take any single point on the plot, you can drop a vertical line from the point to the x-axis and the point where it intersects the x-axis is the price corresponding to the point.\nSimilarly, if you draw a horizontal line from the point to intersect the y-axis, you get the sales_qty corresponding to that point.\nEach point of the plot corresponds to one row of the data frame. So how many points are on the plot? 500, since the data frame has 500 rows.\nThus given a point on the plot, we should be able to identify a corresponding row of the data frame\nGiven a row of the data frame, we should be able to identify a corresponding point on the plot.\n\n\n\n\n\n\n\nQuick Check\n\n\n\nTrue or False: Two different rows can produce the exact same point.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nTrue. If two rows have identical price and sales_qty, they would overlap at the same location.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIf you see an extreme point far above the rest, what does that imply about its row in the data frame?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe row on which the point is based has an unusually high sales_qty value relative to the others (for its price).\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhy might it be hard to identify the exact row for a point just from the plot?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nMany points can be close together or overlapping, and the plot shows approximate positions rather than exact values.\n\n\n\n\n2.1.1 Finding the point corresponding to a row of the data frame\nLet us now try to find the point on the plot corresponding to the first row in Figure 2.1.\nThe first row of the data frame has price = 11.8 and sales_qty = 1260. Therefore, we can find the point by drawing a vertical line from 11.8 on the x-axis to intersect a horizontal line from 1260 on the y-axis. Figure 2.2 illustrates this. We have highlighted the point by enlarging it.\n\n\n\n\n\n\nFigure 2.2: Finding the first row of the price_demand data frame on the plot: price = 11.8, sales_qty = 1260\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nSuppose a row has price = 12.5 and sales_qty = 1200. Describe how to locate its point on the plot.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nFind 12.5 on the x-axis, move up to where y is 1200, and mark the intersection.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIf you draw a vertical line at price = 13, you will cut through several points. What is common to all of these?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThey all correspond to rows that have price = 13.\n\n\n\nSimilarly, let us locate the point corresponding to row 11 (price = 13.3, sales_qty = 1170).\nfollowing the same approach we arrive at Figure 2.3.\n\n\n\n\n\n\nFigure 2.3: Finding the eleventh row of the price_demand data frame on the plot: price = 13.3, sales_qty = 1170\n\n\n\n\n\n2.1.2 Finding the price and sales_qty corresponding to a point on the plot\nWe can reverse the above process to find the variable values (and hence the actual row) corresponding to a point on the plot. We will simply drop a vertical line from the point down to the x-axis to get the price and draw a horizontal line from the point to the y-axis to determine the sales_qty. The figure will look similar to the prior two figures.\n\n\n2.1.3 Dissecting the code\nFigure 2.4 decodes the various elements of the code that we used to generate out first point plot in Figure 2.1. We see that we pass the data frame price_demand to the point_plot function through the pipe operator. We add the tilde expression for the plot as an additional argument.\nFigure 2.5 shows us that the point_plot function maps the variable on the left of the tilde operator to the y-axis of the plot and maps the variable to the right of the tilde expression to the x-axis of the plot.\n\n\n\n\n\n\nFigure 2.4: Passing the price_demand data frame as argument to the point_plot function through the pipe operator\n\n\n\n\n\n\n\n\n\nFigure 2.5: Using a tilde expression to map variables to the axes\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIn a y ~ x formula for point_plot, what does the tilde mean operationally?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIt separates the y-variable (left) from the x-variable (right), mapping them to the plot axes.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWrite the formula you would use to plot weekly_sales_k on the y-axis against advertising_spend_k on the x-axis. Write only the tilde expression.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nweekly_sales_k ~ advertising_spend_k\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIf you accidentally write price ~ sales_qty, what changes in the appearance of the plot?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe axes swap: price becomes the y-axis and sales quantity becomes the x-axis.\n\n\n\n\n\n2.1.4 Interpreting the plot\nFigure 2.6 repeats the plot of price versus sales_qty.\n\nprice_demand |&gt; \n  point_plot(sales_qty ~ price)\n\n\n\n\n\n\n\nFigure 2.6: Price demand relationship (again)\n\n\n\n\n\nWe can see a clear trend showing that, in general, higher prices correspond to lower values of sales quantity.\nTo be sure, this is only a trend and not a strict rule. That is, given any two points, we do not have a guarantee that the point with the higher price will always have a lower sales quantity. However, this will be true for most of the pairs of points. Figure 2.7 illustrates this with some randomly selected pairs of points. We see that most of the lines point down and to the right – meaning that for most pairs of points, the one with higher price has a lower sales_qty. However, we also see a few examples where the opposite is true.\nThis is why we only calls this a trend and not a strict rule about the relationship between price and sales_qty.\n\n\n\n\n\n\nFigure 2.7: Line segments connecting some pairs of points showing that for most pairs of points the point with higher price has lower sales_qty\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nDescribe a quick visual procedure to decide if the relationship is positive, negative, or neither.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nLook at the overall slope of the cloud from left to right: up = positive, down = negative, no slope = neither.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIn the price vs sales plot, why do we call it a trend and not a rule?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nBecause points vary; higher price usually means lower sales, but not for every single pair of observations.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat would a “no linear relationship” scatterplot look like?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nA roughly pattern less cloud with no clear upward or downward slope.\n\n\n\n\n\n2.1.5 Positive and negative relationships\nIn general, when we discuss the relationship between two numerical variables, if the general trend is that higher values of one correspond to higher values of the other, then we have a positive relationship. On the other hand, is higher values of one variable correspond to lower values of the other then we have a negative relationship.\nWhat kind of relationship does Figure 2.6 show – positive or negative?\nYou got it right if you said negative – higher values of price correspond to lower values of sales_qty.\nFigure 2.8 shows two positively correlated variables. Here, higher values of one variable are generally related to higher values of the other. We used the advertising_sales_channel data frame for this plot.\n\nadvertising_sales_channel |&gt; \n  point_plot(weekly_sales_k ~ advertising_spend_k)\n\n\n\n\n\n\n\nFigure 2.8: Weekly sales and advertising spend have a positive relationship – higher advertising generally has higher weekly sales\n\n\n\n\n\nThe data frame returns_dpo contains hypothetical data on various firms’ daily stock returns and the number of days they take to pay their suppliers. Figure 2.9 shows the relationship between these variables. We cannot spot any trend. Higher or lower values of one variable do not indicate higher or lower values of the other. Hence there is neither a positive relationship nor a negative one.\n\nreturns_dpo |&gt; \n  point_plot(daily_return ~ dpo)\n\n\n\n\n\n\n\nFigure 2.9: Days payment outstanding (dpo) does not have a relationship to the daily stock return\n\n\n\n\n\nHow to decide visually if there’s a relationship\nImagine the point cloud as a blob\nAsk: does it slope up, slope down, or neither? Ask: tight blob or wide blob?\n\n\n\n\n\n\nQuick Check\n\n\n\nLabel each plot as positive, negative, or no clear relationship: (a) price vs sales_qty, (b) advertising vs weekly sales, (c) dpo vs daily return.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\n\nnegative, (b) positive, (c) no clear relationship.\n\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIf a scatterplot has a positive relationship, what do you expect most left-to-right line segments between random pairs of points to do?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nPoint up and to the right.\n\n\n\n\n\n2.1.6 Putting a number on the relationship: The correlation coefficient\nThus far, we have been talking about relationships between variables. Exact sciences like mathematics and statistics like to be precise and assign numbers where possible. Statistics commonly uses the correlation coefficient to quantify the relationship between two numerical variables. This section just introduces the correlation coefficient and shows you how to compute it using R. We will get into the mechanics of the actual computation later in the book.\nThe correlation coefficient can take a value between -1 and +1. A value of 0 signifies no linear relationship. -1 signifies a perfect negative correlation and +1 signifies a perfect positive correlation. We discuss these ideas below.\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat does the sign of the correlation coefficient tell you?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nDirection: positive means upward trend (as we go to the right); negative means downward trend (as we go to the right).\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat does the magnitude (absolute value, that is, value without the sign) tell you?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nStrength of the linear pattern: closer to 1 means points lie closer to a straight line.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nTrue or False: r = 0 proves there is no relationship of any kind.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nFalse. It indicates no linear relationship; there could still be a curved or other non-linear relationship. We show an example later.\n\n\n\nWe look at some examples now. Revisiting the price_demand data frame, let us use R to compute the correlation coefficient between the variables price and sales_qty.\n\nprice_demand |&gt; \n  summarize(price_sales_qty_correlation = cor(price, sales_qty))\n\n\n  \n\n\n\nWe are computing a summary and hence use the summarize function (see Section 1.7.6). Since the summary we are computing now is the correlation coefficient, we use the cor function within the summarize function. We have chosen to call the computed result price_sales_qty_correlation. We could have named it anything we wanted, but chose the sensible approach of giving it a meaningful name.\nFigure 2.1 had already shown us the negative relationship between price and sales_qty. The correlation confirms it, … and makes it more precise.\n\n\n\n\n\n\nQuick Check\n\n\n\nIf the scatterplot slopes down and the correlation is -0.75, do these agree?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nYes. Negative sign matches the downward trend; 0.75 magnitude suggests a fairly strong linear relationship.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIf two variables have correlation coefficient close to +1, what will the scatterplot look like?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nPoints clustered tightly around a line sloping upward.\n\n\n\nLet us compute the correlation coefficient between advertising spend and the weekly sales from the advertising_sales_channel data frame. From Figure 2.8 we saw that these two variables are positively related. Let us see what the correlation coefficient says.\n\nadvertising_sales_channel |&gt; \n  summarize(ad_sales_corr = cor(advertising_spend_k, weekly_sales_k))\n\n\n  \n\n\n\nSure enough, we see a positive correlation coefficient.\n\n\n2.1.7 Perfect correlation\nConsider Figure 2.10 showing strong, but imperfect, correlation.\n\nprice_demand |&gt; \n  point_plot(sales_qty ~ price)\n\n\n\n\n\n\n\nFigure 2.10: Price and sales_qty are not perfectly correlated – correlation coefficient is -0.75\n\n\n\n\n\nWe have already established that price and sales_qty are negatively related. That is, there is a general pattern that higher values of price have lower values of sales_qty.\nIn Figure 2.10, if we look only at the points corresponding to a particular value of price, say 13, the points along this vertical line range from approximately 1100 to 1320. That is, given a value of price, we cannot be sure about the exact value of the corresponding value of sales_qty. However, knowing price does give us some idea about sale_qty. This shows relationship, but not perfect correlation.\nWhen the correlation coefficient is +1 or -1, we have perfect correlation. In this case, knowing the value of one variable enables us to precisely know the value of the other variable as well. This happens when all the points lie on a straight line instead of being dispersed as a cloud as in Figure 2.10.\n\n\n\n\n\n\nFigure 2.11: Perfect correlation arises when all the points lie perfectly on a straight line – the variables in this plot have a correlation coefficient of +1\n\n\n\nFigure 2.11 shows a situation when all points lie on a perfect straight line. In this case, given a value for one variable, we can determine exactly the value of the other. We can determine it visually or by plugging the value of the known variable into the following equation and calculating the value of the other variable.\n\ntotal_cost = 10000 + 45*units_produced\n\nIn Figure 2.11 the line slopes upward as it goes from left to right. When the line on which all points fall slopes down as it goes right, the correlation is still perfect, but the line slopes down as it goes to the right and so we have a correlation coefficient of -1.\n\n\n2.1.8 How the point cloud looks for various correlation coefficients\nFigure 2.12 shows the point plots for various values of correlation coefficients between -1 and +1.\n\n\n\n\n\n\nFigure 2.12: Point plots for various correlation-coefficients (r)\n\n\n\nYou can see the following: - in the plot with correlation coefficient = -1 (r = -1), the points align perfectly on a straight line sloping downwards as the x values increase - at +1 they align perfectly on a line sloping upwards as the x values increase - as the coefficient goes from -1 to 0, the points get more scattered away from perfect alignment on a straight line until at 0 there is no pattern whatsoever - as the correlation coefficient increases from 0 to +1, we see that the tendency to align on a straight line increases until perfect alignment at r = 1.\n\n\n\n\n\n\nMagnitude and sign of the correlation coefficient\n\n\n\nSign: Direction (positive/negative)\nMagnitude: Tightness around a line (near 0 = no linear pattern; near ±1 = points nearly on a line)\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat does “perfect correlation” mean visually?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nAll points lie exactly on a straight line.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIf correlation is -1, does that mean the variables are weakly related?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nNo. It means perfectly related but in a negative (downward) direction.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nAs r moves from 0 toward 1, what changes visually?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe point cloud becomes more tightly aligned around an upward-sloping line.\n\n\n\n\n\n\n\n\n\nCorrelation vs. Causation\n\n\n\nIn Figure 2.10, we can see that as price increases, sales_qty decreases. We have said that these two variables are correlated. From this alone, can we infer that higher price caused lower sales_qty?\nWe need to be very careful here. Figure 2.10 only shows the correlation. From that correlation alone we should infer anything more. Specifically, we cannot infer that the price changes cause changes in sales.\nOf course, in the real world, we know that price increases generally lead to drops in sales. But that knowledge lies outside of this data. It is quite possible that, in this situation something else might be playing a role. We just don’t know from the data we are seeing.\nEstablishing causation is part of causal inference which uses other rigorous techniques to establish that something actually caused something else. We do not go into causal inference in this course.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#plotting-a-numerical-variable-against-a-categorical-one",
    "href": "modules/01-data-visualization/point-plots.html#plotting-a-numerical-variable-against-a-categorical-one",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "2.2 Plotting a numerical variable against a categorical one",
    "text": "2.2 Plotting a numerical variable against a categorical one\nIn this course, we will only assign a numerical variable to the y-axis. Thus, if a plot has a categorical variable, it will only occupy the x-axis.\nNumerical and categorical variables differ in how they behave in plots. In Figure 2.8 we have the variable advertising_spend_k on the x-axis. Being a numerical variable, it can potentially take on any value on the x-axis. On the other hand, look at Figure 2.13.\n\n\n\n\n\n\nFigure 2.13: Plotting a categorical variable against a numerical one – using the data frame acct_type_balance (not plotted using the point_plot function)\n\n\n\nHere we have the variable bank_account_type on the x-axis and it can take on only one of two values “Checking” and “Savings”. This is why each row falls at only one of two x positions: one for Checking, and one for Savings. So points stack vertically. If the row corresponds to a “Checking” account, it falls on the left stack of points and on the right stack otherwise.\n\n2.2.1 A Caveat\nCorrelation measures the strength of a linear relationship; a curved pattern can have low correlation coefficient even if there is a strong relationship.\nFigure 2.14 shows a strong non-linear (loosely means “not a straight line”) relationship. The points align very close to a curved line. This has a low correlation coefficient because if you try to run a straight line through these points, no matter how hard you try, you cannot draw a line that is even reasonably close to all points. The correlation coefficient is -0.125 – close to zero!\n\n\n\n\n\n\nFigure 2.14: Strong non-linear relationship, but weak correlation coefficient\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIf bank_account_type has two categories, how many distinct x positions exist conceptually?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nTwo: one for Checking and one for Savings.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhy do points form vertical “stacks” for each category?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nBecause many rows share the same category (same x position) but differ in balance (y).\n\n\n\n\n\n2.2.2 point_plot and “jittering”\nIf we have two Checking accounts with the same or very similar balance, their points will overlap. Because of this over plotting, the plot in Figure 2.13 does not enable us to look at all the points.\nTo overcome this problem, the point_plot function plots this chart a bit differently. See Figure 2.15.\nImportant: Jittering changes only the display position of points to reduce overlap; it does not change the underlying data values\n\nacct_type_balance |&gt; \n  point_plot(balance ~ bank_account_type)\n\n\n\n\n\n\n\nFigure 2.15: Plot of account type against account bakance using the point_plot function: it jitters the points along the x-axis to reduce overplotting\n\n\n\n\n\nTo enable us to see the points more clearly, the point_plot function randomly “jitters” the points along the x-axis to more clearly separate points that might be over plotted or close together.\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat problem does jittering solve?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nOver plotting. Points hiding under other points.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nDoes jittering change the data?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nNo. It only changes where points are drawn to make overlap visible.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIn a jittered plot, if we see two points with the same y-value and the same x-category, what would have happened to them on an un-jittered plot??\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThey would have overlapped and we would only have a single point visible.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#bringing-a-third-variable-into-the-plot",
    "href": "modules/01-data-visualization/point-plots.html#bringing-a-third-variable-into-the-plot",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "2.3 Bringing a third variable into the plot",
    "text": "2.3 Bringing a third variable into the plot\nTwo-dimensional plots can normally accommodate only two variables – one on each axis. However, we can often get more insights if we manage to get more variables into our plots. The point_plot functions allows us to bring one more dimension to our plots through color.\nIn Figure 2.8, we saw the positive relationship between weekly_sales_k and advertising_spend_k in the advertising_sales_channel data frame. That data frame has another variable channel. Each row corresponds to one of two sales channels – Search Ads and Retail Promo. We might want to study separately for each channel the relationship between advertising spend and the weekly sales.\n\nadvertising_sales_channel |&gt; \n  point_plot(weekly_sales_k ~ advertising_spend_k + channel)\n\n\n\n\n\n\n\nFigure 2.16: Determining the color of points based on a third variable – the last variable in the tilde expression for the plot achieves this\n\n\n\nFigure 2.16 inserts, through color, the variable channel into the plot of weekly_sales_k against advertising_spend_k from the advertising_sales_channel data frame. From it we can clearly see that the points corresponding to each sales channel clearly shows the positive relationship that we saw earlier. But this plot also shows that for a given value of advertising_spend_k the Retail Promo channel generally has a comparatively higher weekly_sales_k value. Adding the variable channel through color enabled us to see more into the data than before.\n\n2.3.1 Understanding the tilde expression to add a third variable\nFigure 2.17 shows the tilde expression we used in a point plot of two variables to color the points based on a third variable.\n\n\n\n\n\n\nFigure 2.17: To color the points based on the value of a third variable, we just add it to the end of the tilde-expression after a plus sign\n\n\n\nFigure 2.18 shows another example. In this, we use the mpg data frame which contains data on cars. We plot the city mileage of cars (variable cty) against their engine displacement (variable displ). We color each point based on the class of the vehicle (variable class).\n\nmpg |&gt; \n  point_plot(cty ~ displ + class)\n\n\n\n\n\n\n\nFigure 2.18: The scatterplot shows the relationship between displ and cty from the mpg data frame – the variable class in the tilde expression determines the color of the points\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIn weekly_sales_k ~ advertising_spend_k + channel, what does channel control?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe color of points.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat new insight can color reveal that the two-variable plot cannot?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nDifferences in level or pattern between groups (e.g., Retail Promo tends to have higher sales at the same ad spend).",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#bringing-a-fourth-variable-into-play",
    "href": "modules/01-data-visualization/point-plots.html#bringing-a-fourth-variable-into-play",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "2.4 Bringing a fourth variable into play",
    "text": "2.4 Bringing a fourth variable into play\nWe can bring a fourth variable into play as well through facets. Replacing the tilde expression above with:\nhwy ~ displ + class + drv\ngenerates Figure 2.19.\n\n\n\n\n\n\nFigure 2.19: The scatterplot shows the relationship between displ and cty from the mpg data frame – the variable class in the tilde expression determines the color of the points and the variable drv divides the plot into facets\n\n\n\nIn this figure, we use the mpg data frame and generate a basic scatterplot of the relationship between the highway mileage (variable hwy) and the engine displacement (variabledispl). The third variable class determines the color of the points. The fourth variable drv segments the plots into facets – one for each possible value of drv.\n\n\n\n\n\n\nImportant\n\n\n\nTilde expression summary\ny ~ x → y on vertical axis, x on horizontal axis\ny ~ x + color → color encodes the third variable\ny ~ x + color + facet → facet splits into panels by the fourth variable",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#mini-review-scatterplots-and-relationships",
    "href": "modules/01-data-visualization/point-plots.html#mini-review-scatterplots-and-relationships",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "Mini-Review: Scatterplots and Relationships",
    "text": "Mini-Review: Scatterplots and Relationships\n\n\n\n\n\n\nNote\n\n\n\nAnswer the questions below to check your understanding of how scatterplots represent data and relationships between variables.\n\n\n\nQuestion 1\nIn a scatterplot of sales_qty ~ price, what does the x-coordinate of a point represent?\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe x-coordinate represents the value of price for that observation.\n\n\n\n\n\n\nQuestion 2\nIn the same scatterplot, what does the y-coordinate of a point represent?\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe y-coordinate represents the value of sales_qty for that observation.\n\n\n\n\n\n\nQuestion 3\nIf a scatterplot is based on a data frame with 500 rows, how many points appear in the plot?\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\n500 points — one for each row of the data frame.\n\n\n\n\n\n\nQuestion 4\nDescribe how you would locate on the plot the point corresponding to a row with\nprice = 12.5 and sales_qty = 1200.\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nLocate 12.5 on the x-axis, move vertically until reaching 1200 on the y-axis, and mark the intersection.\n\n\n\n\n\n\nQuestion 5\nWhat visual pattern indicates a negative relationship between two numerical variables?\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nA downward-sloping cloud of points: as x increases, y generally decreases.\n\n\n\n\n\n\nQuestion 6\nWhat does it mean if a scatterplot shows no clear trend?\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThere is no apparent linear relationship — higher or lower values of one variable do not predict higher or lower values of the other.\n\n\n\n\n\n\nQuestion 7\nWhat does the sign of the correlation coefficient tell you?\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIt tells you the direction of the linear relationship: positive for upward trends and negative for downward trends.\n\n\n\n\n\n\nQuestion 8\nA correlation coefficient close to 0 means what?\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThere is little or no linear relationship between the variables, even though a nonlinear relationship may still exist.\n\n\n\n\n\n\nQuestion 9\nWhy does point_plot() jitter points when the x-axis variable is categorical?\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nTo reduce over plotting so that points with the same category and similar values do not overlap visually.\n\n\n\n\n\n\nQuestion 10\nIn the formula\nweekly_sales_k ~ advertising_spend_k + channel,\nwhat role does channel play in the plot?\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIt determines the color of the points, allowing different groups to be distinguished visually.\n\n\n\n\n\n\nQuestion 11\nWhat additional role does a fourth variable play when added as\ny ~ x + color_var + z?\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIt divides the plot into separate panels (facets), one for each value of the fourth variable.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/02-overview.html",
    "href": "modules/02-variability/02-overview.html",
    "title": "Module 2: Variation",
    "section": "",
    "text": "Learning Goals\nVariation plays a very important role in statistics. Some even go to the extent of viewing the discipline of statistics itself as the study of variation.\nThis module discusses variation generally by visualizing it through violin plots. It then introduces the key statistical concepts of variance and standard deviation and how to compute these using R.",
    "crumbs": [
      "Variation",
      "Module 2: Variation"
    ]
  },
  {
    "objectID": "modules/02-variability/02-overview.html#learning-goals",
    "href": "modules/02-variability/02-overview.html#learning-goals",
    "title": "Module 2: Variation",
    "section": "",
    "text": "Explain what variability in a variable represents\nPlot and interpret violin plots\nIdentify broad characteristics of distribution shapes\nCompute variance and standard deviation of a small set of numbers by hand\nUse R to compute the variance and standard deviation of variables in a data frame",
    "crumbs": [
      "Variation",
      "Module 2: Variation"
    ]
  },
  {
    "objectID": "modules/02-variability/02-overview.html#structure-of-this-module",
    "href": "modules/02-variability/02-overview.html#structure-of-this-module",
    "title": "Module 2: Variation",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nViolin plots\nVariance and standard deviation",
    "crumbs": [
      "Variation",
      "Module 2: Variation"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html",
    "href": "modules/02-variability/violin-plots.html",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "",
    "text": "Learning outcomes\nThis chapter talks about visualizing variability.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html#learning-outcomes",
    "href": "modules/02-variability/violin-plots.html#learning-outcomes",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "",
    "text": "After completing this chapter you will be able to:\nDemonstrate these outcomes related to shapes of distributions:\n\nGiven a distribution as histogram, explain what the height of a bar represents\nGiven a distribution as a violin plot, explain what the width of a violin at any point represents\nGiven a violin plot, identify the point(s) having the highest and lowest densities\nIdentify whether a given distribution shown either as a histogram or as a violin plot or as a density plot is\n\nuniform\nsymmetric bell shaped\nmulti-modal\nskewed and in which way\n\n\nDemonstrate these outcomes related to generating violin plots using R:\n\nGiven a data frame and a numerical variable, write R code to generate a violin plot to study the distribution of the variable\nExplain the two important parts of the plot area in a violin plot\nIn the plot, identify which part represents the data and which part represents the annotation\nExplain the tilde expression used to generate violin plots of single variables\nGenerate a violin plot to compare the distributions of a single numerical variable corresponding to a categorical variable\n\nWe have already looks at the concept of variable. We have used the term to refer to a column of a data frame. We explained the rationale for the name from the fact that the values in a single column of a data frame vary. That is, not all the rows of a column have the same value. For example, we see in Table 3.1 that the values in the column minutes vary – that is, not all the values in the column are the same. The same is true of every column.\n\n\n\n\nTable 3.1: Boston Marathon data (20 randonmly selected rows).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nname\ncountry\ntime\nsex\nminutes\n\n\n\n\n1994\nCosmas Ndeti\nKenya\n02:07:15\nmale\n127\n\n\n1975\nLiane Winter\nGermany\n02:42:24\nfemale\n162\n\n\n1973\nJon Anderson\nUnited States\n02:16:03\nmale\n136\n\n\n1949\nKarl Gosta Leandersson\nSweden\n02:31:50\nmale\n152\n\n\n1988\nRosa Mota\nPortugal\n02:24:30\nfemale\n144\n\n\n2002\nMargaret Okayo\nKenya\n02:20:43\nfemale\n141\n\n\n1981\nToshihiko Seko\nJapan\n02:09:26\nmale\n129\n\n\n1921\nFrank T. Zuna\nUnited States\n02:18:57\nmale\n139\n\n\n1992\nIbrahim Hussein\nKenya\n02:08:14\nmale\n128\n\n\n1983\nJoan Benoit\nUnited States\n02:22:43\nfemale\n143\n\n\n1919\nCarl W. A. Linder\nUnited States\n02:29:13\nmale\n149\n\n\n2019\nWorknesh Degefa\nEthiopia\n02:23:31\nfemale\n144\n\n\n1995\nUta Pippig\nGermany\n02:25:11\nfemale\n145\n\n\n2003\nRobert Kipkoech Cheruiyot\nKenya\n02:10:11\nmale\n130\n\n\n1961\nEino Oksanen\nFinland\n02:23:39\nmale\n144\n\n\n1945\nJohn A. Kelley\nUnited States\n02:30:40\nmale\n151\n\n\n1993\nOlga Markova\nRussia\n02:25:27\nfemale\n145\n\n\n1982\nAlberto Salazar\nUnited States\n02:08:52\nmale\n129\n\n\n2019\nLawrence Cherono\nKenya\n02:07:57\nmale\n128\n\n\n2016\nAtsede Baysa\nEthiopia\n02:29:19\nfemale\n149\n\n\n\n\n\n\n\n\nTake care to note that we are not saying that every value in a column has to be distinct. Values can repeat. However we have variety in a column as long as all the values are not the same. Very rarely will you come across data frames in which all the values of a column are the same. In this case the column does not serve any useful purpose for data analysis.\nIn this course, we deal only with variability in numerical variables. Statistics deals with variability in categorical variables as well, but this book does not consider this in any depth.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html#distribution",
    "href": "modules/02-variability/violin-plots.html#distribution",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "3.1 Distribution",
    "text": "3.1 Distribution\nConsider a variable with maximum value of 200 and minimum value 10. Let us suppose that we have 500 values for this variable (500 rows in the data frame). This means that the 500 values are all in the range 10 to 200.\nWe are interested in knowing how these 500 numbers are distributed across the range of 10 to 200.\n\nIt could be the case that the 500 numbers are more or less uniformly distributed over their range with approximately the same number of occurrences throughout the range.\n\nIt could also be the case that the numbers are mostly concentrated in the middle – near 105 or so and as we go further away in either direction, the concentration of numbers drops off.\nOr the numbers could be crowded near, the lower extreme, sat at around 50 or so and be much more sparse away from this point.\nOr it could be that there is heavy concentration around 50 and 150 and less elsewhere.\n\nThe pattern of concentration of values of a variable across its entire range is called the distribution of the variable.\nIn this chapter we focus on visualization alone. The next chapter goes into measuring variation precisely.\n\n\n\n\n\n\nQuick Check\n\n\n\nIn your own words, what does it mean to say a variable has a distribution?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nA distribution describes how often different values of a variable occur and how those values are spread out across their range.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhy is it misleading to describe a data set using only a single number like the mean?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nBecause very different data sets can share the same mean while having different spreads, shapes, or extreme values.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nHow does a distribution answer the question: “What values are typical?”\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIt shows where values cluster most densely and which values occur rarely.\n\n\n\n\n3.1.1 Uniform distribution\nSuppose we have 1000 numbers with each one being an integer between 1 and 100. Some of the numbers in this data might be 45, 23, 46, 89, 1, 23, 45, 56, and so on. If each number occurs exactly 10 times – that is we have 10 ones, 10 twos and so on up to 10 hundreds, we have what is called as the uniform distribution. In a uniform distribution, each number in the range of the list occurs the same number of times.\nFigure 3.1 shows a histogram of a set of numbers. A histogram breaks up the whole range of the data points into bins. In our example, the numbers range from 1 to 100. A histogram might break this up into some number of bins. If the number of bins is 10, then the numbers between 1 and 10 will fall unto 1 bin. The numbers between 11 and 20 into the next bin and so on. In this example, the bin-width is 10 because each bin spans a range of size 10. The x-axis of the histogram shows the number values and a bar for each bin. The y axis shows how many of the numbers fall into each bin and so the pattern formed by the heights of the bars helps us to see the distribution of the numbers.\nFigure 3.1 shows numbers that are perfectly uniformly distributed. We have used a bin-width of 1 to have each individual integer fall into its own bin. In a uniform distribution every number in the overall range of the numbers occurs exactly the same number of times.\n\n\n\n\n\n\nFigure 3.1: Perfect uniform distribution – each number between 1 and 100 occurs the same number of times (10 times), as you can see from the height of the bars\n\n\n\nOur current example deals with integers, but in general when we have numerical variables, we will deal with numbers that also have a fractional part. Histograms work in the same way for those too, except that we cannot have one bin for each distinct value as there is potentially an infinite number of values in any range.\nFigure 3.2 shows an example of a set of numbers distributed approximately uniformly. For example, Each number does not occur exactly the same number of times, but they are loosely similar. For example, the number 2 seems to appear around 13 times and the number 25 seems to occur around 18 times. In practice, uniformly distributed numbers will look more like Figure 3.2 than Figure 3.1.\n\n\n\n\n\n\nFigure 3.2: Approximate uniform distribution – each number between 1 and 100 occurs more or less the same number of times\n\n\n\nA numerical variable does not necessarily have to be distributed uniformly. That is each number in the range does not have to occur an equal number of times exactly or approximately. We will now look at some other common possibilities.\n\n\n\n\n\n\nQuick Check\n\n\n\nWould you expect to find truly uniform distributions in real business data??\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nNo. Most business processes involve constraints, preferences, or natural variation that cause some values to occur more often than others.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nGive one business example where a roughly uniform distribution might appear.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nRandomly assigned identification numbers (like school student ids) or lottery numbers.\n\n\n\n\n\n3.1.2 Bell shaped distributions\nFigure 3.3 and Figure 3.4 show two bell shaped distributions. Both have their peaks around the middle of the range between 1 and 100. Which means that numbers closer to the middle – or closer to 50 – occur more frequently than those far away from 50.\n\n\n\n\n\n\nFigure 3.3: Symmetric bell shaped distribution with a peak at around 50\n\n\n\n\n\n\n\n\n\nFigure 3.4: Symmetric bell shaped distribution with a peak at around 50 and higher spread than in Figure 3.3\n\n\n\nBoth Figure 3.3 and Figure 3.4 show bell shaped distributions that are also symmetric. That is, the shape to the left of the peak is approximately similar to the shape on the right.\nSo, both of these are bell shaped and symmetric, and yet they are obviously different? Can you describe how they are different?\nWe see that the spread* of the numbers is quite different. In Figure 3.3 the numbers are more concentrated near the middle than is the case in Figure 3.4. Consequently, the peak in the first figure is higher. For example, the numbers 47 and 51 occur nearly 50 times. In Figure 3.4, the highest frequency is around 35. We call the ends of the bell shape as tails. We see that Figure 3.4 has fatter tails.\nWe have seen three different possibilities for how a set of 1000 numbers in the range 1 to 100 can be distributed. Even though the averages of a set of numbers and even their range might be the same they can still be distributed very differently.\n\n\n3.1.3 Multi-modal distributions\nIn Figure 3.3 and Figure 3.4 we saw distributions that each had a single peak. This does not have to be the case. shows a case where we have two peaks.\n\n\n\n\n\n\nFigure 3.5: Distribution with more than one peak – multi-modal distribution\n\n\n\n\n\n3.1.4 Skewed distributions\nThus far, we have seen distributions in which the shape has always been symmetric – explicitly so in the bell-shaped examples, but also true for the uniform examples. Figure 3.6 and Figure 3.7 show examples of asymmetric distributions. The first has a long tail on the right and is said to be right-skewed and the second has its tail on the left and is said to be left-skewed.\n\n\n\n\n\n\nFigure 3.6: Asymmetric distribution with a right skew\n\n\n\n\n\n\n\n\n\nFigure 3.7: Asymmetric distribution with a left skew",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html#violin-plots",
    "href": "modules/02-variability/violin-plots.html#violin-plots",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "3.2 Violin plots",
    "text": "3.2 Violin plots\nNow that we have clarified the idea of a distribution let us look at distributions of numerical variables through violin plots. These plots serve the same function as the histograms we have looked at in the previous section, but we can do more.\n\nA violin plot is not a new idea. It is a different lens on the same distribution concepts you already saw in the previous section.\n\nLet us start off by looking at a violin plot of the variable minutes from the Boston_marathon data frame\n\nBoston_marathon |&gt; \n  point_plot(minutes ~ 1, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.8: Violin plot of the minutes variable from the Boston_marathon data frame\n\n\n\n\n\nThe width of the violin at any point on the y-axis shows the relative frequency of data values close to that point. The widest part of the violin occurs at around 145 minutes. This means that the maximum concentration of winning times in the data set is close to 145 minutes. The plot does not tell us exactly what the frequency is. The width only shows the relative frequency. Looking at where the violin is very narrow, we can say that very few runners took more than 175 minutes to complete the race. Also, relatively low numbers of people took less than about 125 minutes. We can see a higher density of points around the broad regions of the violin and a low density in the narrow areas of the violin.\n\n\n\n\n\n\nImportant:\n\n\n\nThe width of a violin does not represent the number of observations at an exact value. It represents a smoothed estimate of how densely values occur near that value as compared to the density of values near other values.\n\n\n\n3.2.1 Elements of the plot\nUntil now, the plots we have generated using point plot have only shown the actual points. However, in Figure 3.8 we see two distinct elements:\n\na jittered plot of the individual points\na violin-shaped solid area – an annotation encompassing the points themselves\n\nThe points represent the raw data and the violin adds an annotation that interprets the data in some way. Here, the interpretation is a violin plot that helps us visualize the distribution of the values.\n\n\n3.2.2 Examining the code\nWe examine the code now. In the code for Figure 3.8, we can see two arguments for the point_plot function. The first argument specifies the tilde expression mapping the axes. The second specifies the type of annotation we want. Figure 3.9 shows the code with the two arguments labeled.\n\n\n\n\n\n\nFigure 3.9: The code to generate Figure 3.8 passes two arguments to the point_plot function – a tilde expression and an annotation specification\n\n\n\nFigure 3.10 explains the tilde expression used for the plot. Mapping of minutes to the y-axis conforms to what we did before. This plot deals with just a single variable – we have no variable on the x-axis. When there is no variable on the x-axis, we conventionally that by just using “1” for the x-axis variable.\n\n\n\n\n\n\nFigure 3.10: The tilde expression of Figure 3.9 we place minutes on the y-axis and since the x-axis has no variable, we just state “1”\n\n\n\nFigure 3.11 explains the second argument in generating a violin plot.\n\n\n\n\n\n\nFigure 3.11: We specify that we want a violin annotation",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html#violin-plots-and-distribution-shapes",
    "href": "modules/02-variability/violin-plots.html#violin-plots-and-distribution-shapes",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "3.3 Violin plots and distribution shapes",
    "text": "3.3 Violin plots and distribution shapes\nThe violin plot displays vertical reflectional symmetry. We can get all the information that the plot conveys just by slicing the plot vertically down the middle and looking at either of the two chunks.\nWhen we first talked about the various distribution shapes like uniform, symmetric bell-shaped, and skewed, we used histograms to convey the ideas. We can deduce the same information from violin plots as well. In the next section, we will see a few more examples. We will discuss distribution shapes in the context of those examples.\n\n\n\n\n\n\nWhat to look for in a violin plot\n\n\n\n\nWhere are most values concentrated?\nAre there one or multiple peaks?\nIs the distribution symmetric or skewed?\nAre there long tails or unusual shapes?\n\n\n\n\n3.3.1 More examples\nWe see a few more examples to reinforce the idea. Figure 3.12 shows a violin plot of the variable price from the price_demand data frame.\n\nprice_demand |&gt; \n  point_plot(price ~ 1, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.12: Violin plot of the price variable from the price_demand data frame\n\n\n\n\n\nIf we look at only the left or right half of Figure 3.12, turn it right by 90 degrees and get rid of the display of the individual points, we get Figure 3.13 – a symmetric bell-shaped distribution.\n\n\n\n\n\n\nFigure 3.13: Violin from Figure 3.12 split and rotated 90 degrees right to reveal the distribution in the form we had seen before\n\n\n\nFigure 3.14 shows a violin plot of the balance variable from the acct_type_balance data frame.\n\nacct_type_balance |&gt; \n  point_plot(balance ~ 1, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.14: Violin plot of the balance variable from the acct_type_balance data frame\n\n\n\n\n\nWe can see from Figure 3.12 that price is bell-shaped and unimodal,but not symmetric. It is skewed towards the lower values because the tail is at the bottom. If we slice and rotate it as before then we would say that it is skewed left.\nNext we look at the distribution of advertising_spend_k from the advertising_sales_channel data frame. Figure 3.15 shows the violin plot. From it we see that advertising_spend_k follows a nearly uniform distribution, but not perfectly so. It is mildly bi modal.\n\nadvertising_sales_channel |&gt; \n  point_plot(advertising_spend_k ~ 1, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.15: Violin plot of the advertising_spend_k variable from the advertising_sales_channel data frame\n\n\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhere would the violin be widest for a symmetric bell-shaped distribution?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nNear the center, where values occur most frequently.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIf you have a symmetric bell-shaped violin plot what does it say about values on the high and low ends of the range?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nExtreme high and low values occur with similar rarity.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWould you generally expect bell-shaped distributions in measurement based data?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nYes. Take people’s heights for instance. They will tend to be bell shaped with the maximum density around the average height. Because many small, independent influences combine to produce values near an average. This is actually a very important theorem in Statistics and is called the Central Limit Theorem. We will not be studying it in this course. This is not to say that most real-life data tend to be symmetric bell-shaped. For instance, incomes tend to be skewed towards higher values (or to the right). A few common distributions capture a surprisingly large number of real-world phenomena.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nDescribe how mulch-modal distributions look on a violin plot.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nAs multiple bulges or wide regions separated by narrower sections.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat might cause a business data set to have more than one mode?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe data may combine different subgroups, such as entry-level and senior employees. Or data for different products and so on. We will encounter these later in the course.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat would the violin plot look like for a distribution wish is skewed towards the higher values?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nA violin with its broad portion down at the bottom and a long stem towards the top.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat do you think skew does to the average or mean?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe average gets pulled towards the direction of the skew. A small number of extreme values can pull the mean away from where most observations lie.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html#comparing-distributions-with-violin-plots",
    "href": "modules/02-variability/violin-plots.html#comparing-distributions-with-violin-plots",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "3.4 Comparing distributions with violin plots",
    "text": "3.4 Comparing distributions with violin plots\nThus far, we have examined distributions of individual variables in isolation. In statistics we often want to compare distributions. For example, in the acct_type_balance data frame, how does the distribution of balance for Checking accounts compare with that for Savings accounts?\n\nacct_type_balance |&gt; \n  point_plot(balance ~ bank_account_type, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.16: Violin plots of the balance variable for different accoiunt types – from the price_demand data frame\n\n\n\n\n\nSince we want to compare the two violins corresponding to the two different account types, we now have bank_account_type on the x-axis and our tilde expression reflects this.\nFigure 3.16 shows several things: - account balances in Savings accounts are generally larger - account balances of Savings accounts are distributed in an almost symmetrical bell shape with the maximum density around $6,000. - account balances of Checking accounts are generally smaller than those of Savings accounts - account balances of Checking accounts are bell shaped, but skewed towards lower values with the maximum density around $5,400 or so.\n\nAs another example of comparing distributions, let us use the advertising_sales_channel data frame to compare the distribution of weekly_sales_k for different channels .\n\n\nadvertising_sales_channel |&gt; \n  point_plot(weekly_sales_k ~ channel, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.17: Violin plots of the weekly_sales_k variable for different sales channels – from the advertising_sales_channel data frame\n\n\n\n\n\nFigure 3.17 tells us the following: - weekly_sales_k values are generally lower for the Retail Promo channel - for the Retail Promo channel weekly_sales_k is distributed systematically with three short peaks - weekly_sales_k values are almost uniformly distributed for the Search Ads channel and have a much larger range than the Retail Sales channel",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html",
    "href": "modules/02-variability/variance-and-sd.html",
    "title": "4  Variance and standard deviation",
    "section": "",
    "text": "Learning outcomes\nThis chapter discusses the key statistical concepts of variance and standard deviation.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#learning-outcomes",
    "href": "modules/02-variability/variance-and-sd.html#learning-outcomes",
    "title": "4  Variance and standard deviation",
    "section": "",
    "text": "After completing this chapter you will be able to:\n\nGiven two sets of numbers with 10 or less numbers, identify which has higher variance and explain why\nGenerate a set of numbers with zero variance\nGiven a set of no more than five numbers (at most two digits) compute their variance by hand\nExplain the units of measurement for variance\nExplain the relationship between variance and standard deviation\nExplain the units of measurement for standard deviation\nUse R to compute the variance and standard deviation of a numerical variable of a data frame",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#variable",
    "href": "modules/02-variability/variance-and-sd.html#variable",
    "title": "4  Variance and standard deviation",
    "section": "4.1 Variable",
    "text": "4.1 Variable\nWe use the term “variable” to describe a column of a data frame. Why? Let us make things more concrete. A data frame with data on employees of a company might have a column named “salary” to store the salaries of employees. Salaries of employees vary. That is, we know that not everyone in the company has the same salary (generally speaking). So, it makes eminent sense to call a column a “variable.”\nSimilarly, the same data frame might have another column named “Position” that stores the position of each employee. Some example values might be “Manager”, “Sales associate”, and “IT Specialist.” Here too we see “variability” because not all the values in the column are the same. So the term “variable” applies to columns whether they contain numeric or categorical values. However, for the rest of this document, we concern ourselves only with numerical variables.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#why-means-averages-are-not-enough",
    "href": "modules/02-variability/variance-and-sd.html#why-means-averages-are-not-enough",
    "title": "4  Variance and standard deviation",
    "section": "4.2 Why means (averages) are not enough",
    "text": "4.2 Why means (averages) are not enough\nIn business, averages are often the first numbers we look at. Average revenue, average delivery time, average customer wait time, average return on investment—these figures are easy to compute and easy to communicate.\nBut averages alone can be dangerously misleading.\nTo see why, consider the following examples.\n\n4.2.1 Example 1: Two stores, same average sales\nSuppose two retail stores each reports average daily sales of $10,000.\n\nStore A has daily sales that are usually between $9,500 and $10,500.\nStore B has daily sales that swing wildly, ranging from $3,000 on slow days to $25,000 on busy days.\n\nFrom the average alone, the two stores appear identical. From a management perspective, they are not.\nStore A is predictable. Staffing, inventory, and cash flow planning are relatively straightforward. Store B is risky. Some days it is overstaffed and overstocked; on other days it cannot meet demand.\nThe average hides this difference. What distinguishes the two stores is variability, not the mean.\n\n\n4.2.2 Example 2: Average delivery time\nA logistics manager is choosing between two suppliers. Both advertise an average delivery time of 5 days.\n\nSupplier X delivers in 4–6 days almost every time.\nSupplier Y sometimes delivers in 2 days and sometimes in 12 days.\n\nIf you only look at averages, the suppliers appear almost equivalent.\nIf your business depends on reliable delivery schedules, they are not.\nIn this context, variability directly affects:\n\nProduction planning\nInventory holding costs\nCustomer satisfaction\n\nOnce again, the average tells only part of the story.\n\n\n4.2.3 Example 3: Investment Returns\nTwo investment portfolios have the same average annual return of 8%.\n\nPortfolio A produces returns close to 8% year after year.\nPortfolio B alternates between very high gains and significant losses.\n\nMany investors would consider Portfolio B riskier, even though the averages are identical. That perception of risk comes from how spread out the returns are, not from the mean itself.\n\n\n4.2.4 The Key Idea\nAverages describe the center of the data, but they say nothing about how the data are spread out.\nIn business settings, variability affects:\n\nRisk\nReliability\nPredictability\nPlanning and decision-making\n\nTo fully understand a data set, we need measures that describe how much values differ from the average. That is the role of variance and standard deviation.\n\n\n\n\n\n\nQuick Check\n\n\n\nTwo companies have the same average monthly revenue. Company A’s revenue is very stable from month to month, while Company B’s revenue fluctuates widely. Why might a manager prefer Company A, even though the averages are the same?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nA manager might prefer Company A because its stable revenue makes planning easier and reduces risk. Predictable revenue helps with budgeting, staffing, inventory management, and cash flow. Company B’s wide fluctuations introduce uncertainty, even though its average revenue is the same.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nTwo suppliers both report an average delivery time of five days. One supplier delivers in a narrow range of times, while the other sometimes delivers very early and sometimes very late. What important information does the average delivery time fail to capture?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe average delivery time does not capture how consistent or reliable deliveries are. It ignores variability. A supplier with unpredictable delivery times can disrupt production schedules and increase costs, even if the average delivery time is acceptable.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nExplain why averages alone are insufficient for assessing risk in investment decisions.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nAverages do not show how much returns vary from year to year. Two investments can have the same average return but very different levels of risk. Higher variability in returns generally means greater uncertainty and risk for the investor.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nIn your own words, explain what variability means in a business context.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nVariability refers to how much outcomes differ from their average value. In business, it reflects unpredictability in areas such as sales, delivery times, costs, or returns, which can affect planning and decision-making.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nGive one example of a business decision where variability might matter more than the average.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nExamples include delivery times in a supply chain, customer wait times in a service operation, investment returns, manufacturing defect rates, or daily demand for a product. In these cases, high variability can cause operational problems even if the average appears acceptable.\n\n\n\nIn the sections that follow, we will develop these measures and learn how to interpret them in practical business contexts.\n\n\n\n\n\n\nGrasp this\n\n\n\nVariance and standard deviation quantify how much data values typically deviate from the mean.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#amount-of-variation",
    "href": "modules/02-variability/variance-and-sd.html#amount-of-variation",
    "title": "4  Variance and standard deviation",
    "section": "4.3 Amount of Variation",
    "text": "4.3 Amount of Variation\nMeasuring the amount of variation plays an important role in statistics. If a column has no variation, then we cannot make much use of the information in the column. When a column does have variation, we often want to know the extent of variation in the values of the column. We want to ascribe a specific number – that is, measure the variation in a column. We call this measure the variance of the values in a column. When the numbers in a column are all the same, then we say that the column has no variance – that is, the variance is zero.\nConsider the following three sets of 8 numbers each: - Set 1: (1, 2, 2, 1, 2, 1, 2, 1) - Set 2: (2, 6, 8, 2, 9, 2, 10, 9) - Set 3: (1, 3, 2, 3, 2, 2, 3, 4)\nWhich of the three sets has the highest amount of variation? That is, which set has numbers that differ by the most (among the three sets)?\nCan you order the sets in increasing order of their overall variability?\nOverall, we can see that the numbers in Set 1 are much closer to each other than those in the other two sets. The numbers in Set 2 are the furthest apart from each other and those in Set 3 are in-between. Therefore the order is: Variance of Set 1 &lt; Variance of Set 3 &lt; Variance of Set 2",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#measuring-variation-through-variance",
    "href": "modules/02-variability/variance-and-sd.html#measuring-variation-through-variance",
    "title": "4  Variance and standard deviation",
    "section": "4.4 Measuring Variation through variance",
    "text": "4.4 Measuring Variation through variance\nFrom the foregoing, we see that the variance of a set of numbers is a measure of how much the numbers differ from each other. A set of numbers in which the elements are generally very close together has a low variance and a set that contains numbers that differ from each other by a lot has high variance. Over and above just describing what low and high variance look like, we would like to assign a specific number to the variance of a set of numbers.\nIn reality we will compute the variance using the R function var. We describe two procedures below just to give you a good feel for what the variance represents.\n\n4.4.1 Computing Variance: Method 1 – Using deviations from the mean\nWe had earlier mentioned that variance is the extent to which the numbers in a collection deviate from the average. We use the following steps.\n\nFind the mean\nFind each number’s difference from the mean\nSquare the differences\nAverage the squared differences (but use (n-1) in the denominator. We show a worked example below.\n\nAgain, we use the same set of numbers (2, 4, 5, 9). Let us suppose that these numbers represent the prices of some items in a shop (in the US) and therefore they represent USD amounts.\n\n4.4.1.1 Steps to compute variance using deviations from the mean\n\nFind the average of the numbers.\nThe sum of the numbers is:\n2 + 4 + 5 + 9 = 20\nThe average is: 20 / 4 = 5\nFind the squared deviations from the mean for each number.\n\n\n\nNumber\nDifference from mean\nDifference squared\n\n\n\n\n2\n3\n9\n\n\n4\n1\n1\n\n\n5\n0\n0\n\n\n9\n4\n16\n\n\n\nDivide the total of the squared deviations from the prior step by (n − 1), where n is the number of elements.\nThe sum of the squared deviations is: 9 + 1 + 0 + 16 = 26\nDividing by (n − 1): 26 / 3 = 8.66667\n\n\n\n\n\n\n\nZero variance?\n\n\n\nCallout content goes here.\n\n\nWhat are the characteristics of a set of numbers with zero variance? Think a little before reading on.\nIf the variance has to be zero then. the average of the differences also has to be zero, which means each of the differences has to be zero. That in turn means that all the numbers have to be the same. This makes a lot of sense, because if there is zero or no variance, then the numbers do not vary!\n\n\n\n4.4.2 Computing Variance: Preferred Method – Using R\nWe have a data frame named variance_example that has a variable num with the values (2, 4, 5, 9). We can use the following R code to compute its variance.\n\nvariance_example |&gt; \n  summarize(num_var = var(num))\n\n\n  \n\n\n\nHere is another example using the mpg data frame to compute the variance of one of its variables.\n\nmpg |&gt;\n  summarize(hwy_var = var(hwy))\n\n\n  \n\n\n\nIn the previous section we computed the variance of a set of numbers that represented USD values. What are the units for variance?\n\n\n4.4.3 Unit of measurement for variance\nWhenever we measure anything, we measure it in terms of some unit of measurement. For example, we might use meter as the unit of measurement for people’s heights, and USD as the unit of measurement for US company profits. In fact, in any data frame, whenever we see a numerical variable, we need to be aware of its unit of measurement.\nIn the two methods where we computed the variance, we found some differences in each step. These differences were differences between USD amounts and hence the differences have units of USD. We then squared the differences. The unit of measurement for the squares would be USD-squared. We then averaged these in one approach and divided by a number in the other approach to get the variance. Therefore, variance also has USD-squared as its unit of measurement.\nIn general, when we compute the variance of a variable measured in some units (say, u, the variance has u-squared as its unit of measurement. That is, if we have a variable expressed in meters, then the variance of the variable will be in meters-squared.\nWe generally have a clear mental notion of units like USD and meters, but not units like USD-squared or meters-squared.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#another-measure-of-spread-standard-deviation",
    "href": "modules/02-variability/variance-and-sd.html#another-measure-of-spread-standard-deviation",
    "title": "4  Variance and standard deviation",
    "section": "4.5 Another measure of spread: Standard Deviation",
    "text": "4.5 Another measure of spread: Standard Deviation\nWe have seen that variance has squared units and we cannot easily relate to these. Statisticians have therefore given us another measure of variability that has the same units as the original variable. This is the standard deviation and we compute it as the square-root of the variance.\nIf a variable is measured in inches, its variance has inches-squared as its unit. However, when we compute its square root, we get the standard deviation with inches as units.\nWe can now mentally relate the original variable values to the standard deviation and therefore statisticians use this measure very widely. This is not to say that variance is not widely used as well. It is, and we will use it extensively in this course as well.\n\n\n\n\n\n\nWhat information does the standard deviation give us\n\n\n\nThe standard deviation tells us the typical distance between a data value and the mean.\n\n\n\n4.5.1 Computing standard deviation using R\nWe can use the sd function inside the summarize function to compute the standard deviation\nLet us compute the standard deviation of the city mileage (variable cty) in the mpg data frame.\n\nmpg |&gt;\n  summarize(cty_sd = sd(cty))\n\n\n  \n\n\n\nJust to confirm that the computed standard deviation is in fact the square root of the variance let us compute both.\n\nmpg |&gt;\n  summarize(cty_var = var(cty), cty_sd = sd(cty))",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#optional-enrichment-topic",
    "href": "modules/02-variability/variance-and-sd.html#optional-enrichment-topic",
    "title": "4  Variance and standard deviation",
    "section": "4.6 Optional enrichment topic",
    "text": "4.6 Optional enrichment topic\nWe saw two ways to compute variance. There is another method, which, of course, gives the same result, but is quite intriguing, but intuitive.\n\n4.6.1 Computing Variance: Intriguing Method 1 – Using pairwise differences\nWe first look at an intuitive (but impractical) method for computing the variance of a set of numbers. Variance represents how different each member of a set of numbers is from the rest. So the difference between each number and the rest of them plays a central role. Our first approach to computing the variance involves looking at every possible pair of numbers in our set and seeing how different the two numbers of the pair are from each other. We then combine all of these pairwise differences into a single number to represent the overall variability in the set. #### Steps to compute variance using pairwise differences\nLet us assume that we want to compute the variance of the set of 4 numbers (2, 4, 5, 9)\n\nIdentify each possible pair of numbers from the set.\n(2, 4), (2, 5), (2, 9)\n(4, 5), (4, 9)\n(5, 9)\nThere are 6 such pairs.\nCompute the difference for each pair and square it.\n\n\n\nPair\nDifference\nDifference squared\n\n\n\n\n(2, 4)\n2\n4\n\n\n(2, 5)\n3\n9\n\n\n(2, 9)\n7\n49\n\n\n(4, 5)\n1\n1\n\n\n(4, 9)\n5\n25\n\n\n(5, 9)\n4\n16\n\n\n\nFind the average of the squared differences.\nThe sum of the squared differences is:\n4 + 9 + 49 + 1 + 25 + 16 = 104\nThe average is:\n104 / 6 = 17.33333\nDivide the average from the prior step by 2.\n17.33333 / 2 = 8.66667\nSo, the variance of the set (2, 4, 5, 9) is 8.66667.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/03-overview.html",
    "href": "modules/03-introduction-to-models/03-overview.html",
    "title": "Module 3: Introduction to models",
    "section": "",
    "text": "Learning Goals\nThis module gently introduces the idea of a model, and then discusses the simplest possible model – the mean. It also introduces the concepts of outcome and explanatory variables. It extends the idea of the mean as the model with no explanatory variable to cover the case of a model with one categorical explanatory variable and shows that in this case the model is the category mean.\n**After completing this module, you will be able to:",
    "crumbs": [
      "Introduction to models",
      "Module 3: Introduction to models"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/03-overview.html#learning-goals",
    "href": "modules/03-introduction-to-models/03-overview.html#learning-goals",
    "title": "Module 3: Introduction to models",
    "section": "",
    "text": "Explain the term model, generally and in our context\nIntuitively explain why the mean is the best model in the absence of any other information",
    "crumbs": [
      "Introduction to models",
      "Module 3: Introduction to models"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/03-overview.html#structure-of-this-module",
    "href": "modules/03-introduction-to-models/03-overview.html#structure-of-this-module",
    "title": "Module 3: Introduction to models",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nMean as a model\nCategory mean as model",
    "crumbs": [
      "Introduction to models",
      "Module 3: Introduction to models"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models-story.html",
    "href": "modules/03-introduction-to-models/path-to-models-story.html",
    "title": "5  The Simplest Model",
    "section": "",
    "text": "Learning outcomes\nThis chapter helps you to dip your toes into the vast and important topic of models.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models-story.html#learning-outcomes",
    "href": "modules/03-introduction-to-models/path-to-models-story.html#learning-outcomes",
    "title": "5  The Simplest Model",
    "section": "",
    "text": "After completing this chapter you will be able to:\n\nExplain the term model from a general perspective, and from the perspective of statistics\nExplain why, given the values of only one variable and in the absence of any other information, the mean is the best model\nExplain in what sense the mean is the best model\nGiven a model equation, identify the outcome variable and the explanatory variable(s)\nDescribe the terms outcome variable and **explanatory* variable\nMatch the terminology between the two pairs (outcome variable, explanatory variable) and (independent variable, dependent variable)\nGiven a model with a single numerical explanatory variable and a value for the explanatory variable, compute the model value.\nExplain why we place a hat on the outcome variable in a model expression\nExplain why we call a model’s output an estimate\nList and describe the two uses of models covered in this chapter",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models-story.html#companies-commit-when-they-make-decisions",
    "href": "modules/03-introduction-to-models/path-to-models-story.html#companies-commit-when-they-make-decisions",
    "title": "5  The Simplest Model",
    "section": "5.1 Companies commit when they make decisions",
    "text": "5.1 Companies commit when they make decisions\nGood decision-making lies at the heart of effective business management.\nIn practice, companies must commit to decisions before they know how it will play out in the real-world. Many important business decisions cannot be revised freely once they are made. Examples include setting prices, determining how much inventory to stock, choosing staffing levels, or deciding where to locate a factory.\nOnce a decision is made, it influences many individual events that unfold over time.\nConsider pricing. A company sets a single price for a product, and then thousands of customers independently decide whether or not to buy at that price. One decision by the firm affects many separate outcomes.\nThis has an important implication: we cannot judge the quality of a decision based on a single outcome.\nSuppose a company sets a price of $10 and the first customer who walks in buys the product. Was $10 the right price? We cannot really say. That single outcome may simply be good luck. If we observe the decisions of 1,000 customers, however, a clearer picture begins to emerge. Over many independent events, good decisions tend to perform well on average.\nIn business, then, decisions must be made in advance, under uncertainty, and their quality must be judged by how well they balance out across many realizations and not by whether they happen to succeed in one particular instance.\nThis perspective will guide how we think about models in this chapter. A good model is not one that gets lucky once. It is one that performs reliably across many possible outcomes.\n\n\n\n\n\n\nDefinition of model for our present purposes\n\n\n\nA model is a simple rule or equation that uses data to produce predictions or explanations, while deliberately ignoring some details of reality.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models-story.html#sec-mean-game",
    "href": "modules/03-introduction-to-models/path-to-models-story.html#sec-mean-game",
    "title": "5  The Simplest Model",
    "section": "5.2 Staffing kiosks with incomplete data: Deciding under uncertainty",
    "text": "5.2 Staffing kiosks with incomplete data: Deciding under uncertainty\nAmanda, the owner of Random Treats walks into a conference room with a small group of business school interns.\n“I run two small sales kiosks,” she says. “One at the Beach. One at the Mall. I need to set staffing levels for the next month.”\nAmanda holds up a messy spreadsheet printout.\n“Customer counts from the last 20 days,” she says. “But the person who collected the data forgot to record which kiosk each number came from. My problem is that I have to set the staffing level for the next month at my kiosks and I have to base that on the expected number of customers who will show up at the kiosks.”\nSomeone laughs nervously. Amanda does not.\n“I don’t have time to redo data collection,” she says. “So we work with what we have. But first, I want to see how you think.\nBased on the data we have, I want to settle on a single number for how many customers will show up at the kiosks each day. I will use that number to set the staffing level for the kiosks – same for both kiosks.”\nShe grabs a marker.\n“Here’s a tiny practice version of the problem, with small numbers that you can wrap your head around. Instead of the messy numbers on the spreadhseet, assume that these small numbers were the number of customers who showed up over the past several days at a kiosk (beach or mall). Now tell me what single number I should use as the representative demand at a kiosk?”\nShe writes the following set of 10 numbers on the board:\n4, 2, 6, 1, 8, 1, 3, 9, 2, 4\nYou should think of this set as representing possible outcomes—number of customers who show up at a kiosk. I am keeping the numbers small so that we can first arrive at a method. We can then apply the chosen method to the messy real-world numbers. You do not know which value will actually occur on any given day, but you have reason to believe that the overall pattern will be similar to these numbers.”\nAngela asks “But why should you keep the same number each day? Can we not adjust staffing day by day?”\nAmanda says “Good thinking, Angela. I would love to be efficient, but I cannot do that. Firstly, I want to give some stability to the staff so that they can rest asusred that they have guaranteed employment for the whole month. Secondly, I run many businesses and I can only commit a few hours to this business. So today is the day I make the staffing decision for the next month for this business. I can revisit this decision only next month.”\n“Got it” says Angela.\nAmanda … “OK, guys, put on your thinking hats.”.\n“Here are the rules of the game:\n\nYou as a team commit to a single number for expected sales on any given day at either kiosk. This is your guess, and you must choose it before any outcomes – actual sales – are revealed over the month. You can assume that the pattern will be similar to the data I have given you.\nI then pretend to be the real-world, and determine the first day’s demand by randomly selecting one number (customer demand) from our set. This number I pick might be the same as, lower than, or higher than your committed number. The closer it is to the number I pick randomly, the better. I will compute the difference between what you committed to and the real-world – what I picked randomly.\nNow comes the twist. I square the difference to obtain the penalty for that round.\nWe repeat steps 2 and 3, twenty times just to get a reasonable number of trials, and add up all the penalties.\n\nThe result of step 4 is your total penalty.\nObviously, your goal is to keep this total penalty as small as possible to keep the number you committed to close to the actual number of customers (the numbers I picked randomly from our representative data) .\nSo, remember, you commit once and the real world plays out day after day. At the end of 20 days, we want to see the overall penalty. It is no good doing really well on a few days and being way off on others. Consistency is key.”\nSteve seemed perplexed. “But Amanda, why not just add the differences? Why square them and then add?”\n“I like that Steve. You are thinking well about this already” said Amanda. In our business we can afford small deviations from reality, but the farther our decisions are from reality, we pay a far heftier price. So I want to really avoid large deviations from reality and squaring the differences and choosing the solution that produces the lowest penalty take care of this aspect. For instance, if we do not square the differences, then an error of 10 is only twice as bad as an error of 5. Whereas qith squaring, an error of 10 is 4 times as bad as an error of 5. I really want to minimize big errors.”\n“Oh, I get it.” Steve said. ” We did something similar when we calculated variances in our statistics class.”\nAmanda: “Nice connection Steve!”\n“OK team. Why don’t you discuss for 10 minutes and give me a number while I grab a coffee?”\n10 minutes later Dave announces “Amanda, we have discussed. Our number is 6”.\n“Thanks, Dave. I will now randomly generate the number of customers for 20 days based on our data and we can jointly compute the total penalty.”\nTable 5.1 shows the computation and the results. The total penalty was 219.\n\n\n\nTable 5.1: Decision, random choices, and resulting penalties (Decision = 6)\n\n\n\n\n\nDecision =\n6\n\n\nrandom_choice\ndiff\npenalty\n\n\n\n\n9\n-3\n9\n\n\n3\n3\n9\n\n\n9\n-3\n9\n\n\n4\n2\n4\n\n\n2\n4\n16\n\n\n6\n0\n0\n\n\n2\n4\n16\n\n\n8\n-2\n4\n\n\n4\n2\n4\n\n\n1\n5\n25\n\n\n2\n4\n16\n\n\n2\n4\n16\n\n\n2\n4\n16\n\n\n2\n4\n16\n\n\n3\n3\n9\n\n\n9\n-3\n9\n\n\n2\n4\n16\n\n\n6\n0\n0\n\n\n9\n-3\n9\n\n\n2\n4\n16\n\n\nTotal penalty =\n219\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nExplain how the penalty of 9 was computed for the first row of Table 5.1.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe decision is 6. The random choice selected 9 customers. The difference is -3 (or 3 depending on what you placed first in computing the difference. Since we are squaring, this does not matter.) The squares of -3 and 3 are both 9.\n\n\n\nAmanda: “The total penalty was 219. Can you reduce the penalty?”\nPaul: “Let’s try 7.”\nTable 5.2 shows the results. The total penalty this time is slightly higher at 230.\n\n\n\nTable 5.2: Decision, random choices, and resulting penalties (Decision = 7)\n\n\n\n\n\nDecision =\n7\n\n\nrandom_choice\ndiff\npenalty\n\n\n\n\n4\n3\n9\n\n\n4\n3\n9\n\n\n3\n4\n16\n\n\n4\n3\n9\n\n\n4\n3\n9\n\n\n8\n-1\n1\n\n\n8\n-1\n1\n\n\n9\n-2\n4\n\n\n4\n3\n9\n\n\n3\n4\n16\n\n\n2\n5\n25\n\n\n3\n4\n16\n\n\n2\n5\n25\n\n\n3\n4\n16\n\n\n8\n-1\n1\n\n\n4\n3\n9\n\n\n8\n-1\n1\n\n\n1\n6\n36\n\n\n4\n3\n9\n\n\n4\n3\n9\n\n\nTotal penalty =\n230\n\n\n\n\n\n\nAmanda: “Well that raised the penalty slightly. Any ideas how to proceed further? If we keep trying different numbers, we could be here all night and still not know if we have the best solution!\nWhat number will produce the lowest penalty? Honestly, I do not know and I need your bright minds to help me with this.\nI am going to leave you all to work through this as a team for an hour while I attend another meeting. When I come back, I hope you will be able to give me a good solution and convince me that it is indeed good!.”\nIgor had been silent all along and finally spoke up tentatively.\n“I think we should try many numbers from 1 to 9 – the range of this data – in steps of 0.25 or something small. For each number, we should compute the total penalty by doing something like what Amanda had us do earlier. But trying just 20 times for each number leaves us at the mercy the random selection process giving freak results. Instead we should aim for a large number of trials – like 1000. Then the chance factor will be considerably reduced.\nWe can then plot a chart showing the total penalty for each decision and directly see the decision for which total penalty is the lowest.”\nDave wanted to clarify. “Are you saying, Igor, that you will first pick 1 as the choice and then generate 1000 random choices from the data set. For each random choice you will compute the difference from the random choice and 1 and square it and add them all up to get the total penalty. That would be the total penalty for the choice 1.\nYou wll then repeat the procedure for 1.25 and get its total penalty. And then 1.5, and so on until 9 (which is the largest number in our data). You will then have a table with two columns: choice and total_penalty. You will then plot this.”\n“Exactly” said Igor and sketched a rough plot (Figure 5.1) on the back of an envelope. “This is what it will look like. Of course, I have not done the computations and so my sketch is just an idea now. I suspect that the total penalty will be lowest at some decision and increase on either side of it. Once we have the plot, we can then find the decision at which the total penalty is the smallest.”\n\n\n\n\n\n\nFigure 5.1: Igor’s back of the envelope sketch of the plot that he thought would help them identify the best decision\n\n\n\n“If we can generate this plot, we might be able to sell the idea to Amanda” Steve said.\n\n\n\n\n\n\nQuick Check\n\n\n\nHow do you think Igor’s sketch in Figure 5.1 might be useful in finding the best decision?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIf the plot does in fact come out to have this shape, then the decision where the total penalty is lowest would be good. The plot once actually generated can help us to find the best decision.\n\n\n\n“But Igor, this is easier said than done. How can we complete all these computations in an hour? In fact we might not finish in a week!” bemoaned Angela. Igor agreed. Perhaps he was being too much in the clouds.\nSuzie joined the fray. “Angela, If what I am thinking of is correct, this might not be as hard as it seems. I learned R in a class. I think I can do this in R quickly and get a nice plot that Igor is talking about. We can have something ready for Amanda by the time she comes back from her meeting.”\nEveryone exulted “You can do that? Wow!” and Suzie and Igor got to work.\nThey defined the problem clearly to an AI chatbot and asked it to generate a program in the R language that could generate the plot. They described precisely what they wanted the program to do. This part took some time as they wanted to be very sure that the chatbot had the right information.\nOnce they were done, the chatbot then gave them the code in less than a minute. She looked over it carefully, fixed a few things and then ran it.\nSoon Suzie had a nice plot of the total errors for every possible value of the decision from 1 through 9 in intervals of 0.2. In fact she had made 10,000 trials for each value instead of just 1,000 as Igor had suggested. It looks like a guess of 4 would produce the smallest total penalty. The team had its answer for Amanda! Figure 5.2 shows the plot. The plot shows that a decision of 4 generates the lowest total penalty.\n\n\n\n\n\n\nFigure 5.2: Results of Suzie’s program showing the various possible decisions and their total penalties: the lowest total penalty occurs at a decision of 4\n\n\n\nHigh-fives all round.\n\n\n\n\n\n\nQuick Check\n\n\n\nHow many values of decision did Suzie’s program try out?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nShe tried the values 1, 1.2, 1.4, 1.6, 1.8, or 5 values between 1 and 2. Similarly 5 between 2 and 3 and so on That would be 8 times 5 or 40 up to 8.75. and then there is 9. So she tried a total of 41 different decision values.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nHow many individual differences did her program compute?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nFor each decision her program tried 10,000 trials and hence computed 10,000 differences for a total of 10,000*41, or 410,000 differences.\n\n\n\n“Wait a minute” said Angela. With Amanda’s trials, the total penalty was close to 200. Now it looks like our lowest is close to 70,000. What is going on? Have me done something wrong here? I have heard that AI chatbots can hallucinate. Have wee been hit with a major hallucination?”\nDoom spread in the room. Amanda would be in the room shortly. Have they let her down badly?\nSilent Igor came to their rescue again. “The total penalty in Amanda’s case was computed over 20 trials. Suzie’s total penalties are based on 10,000 trials. Naturally they will be higher. This is an apples to oranges comparison.\nTo make these comparable, we should be comparing the average penalty per trial to remove the direct effect of the number of trials on the total penalty. So we should divide Amanda’s total penalty by 20 and Suzie’s by 10,000 to get the average penalty per trial for each of them. Then we can compare them.”\nDavid immediately did some mental math and came back with “Well, for the numbers we tried with Amanda, the average penalty per trial was approximately 220/20, or 11. For Suzie’s numbers, the plot says that the total penalty for the best decision of 4 is around 70,000. We divide that by 10,000 trials and get a penalty per trial of 7. So we did improve significantly on what we initially came up with.”\nThe group heaved collective sign of relief.\nIgor said “Since Suzie did 10,000 trials for each number, we can be almost certain that 4 is in fact the best solution.\nThe team had finished with 10 minutes to spare. They spent the remaining time preparing a neat presentation for Amanda describing exactly what they had done. They also explained how they had used AI to help in this process.\nWhen she returned, Amanda took one look at the chart and asked a few questions about how they had generated it. She was convinced that these young interns had found the best solution to her kisok staffing problem.\nAmanda said “Thank you all very much. You have done a great job! You accomplished a lot in a short amount of time. Do you know why that happened?\nYou worked as a team. Your ideas built upon each other and you were able to achieve a lot.\nHow do I know even though I was not in the room? Well, I have seen this time and again, and you will too in your working lives. That’s the strength of teams – some say Wisdom of Teams.\nWe will meet next month to review how things went and to refine our decisions.”",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models-story.html#average-or-mean-as-the-decision-in-the-long-run",
    "href": "modules/03-introduction-to-models/path-to-models-story.html#average-or-mean-as-the-decision-in-the-long-run",
    "title": "5  The Simplest Model",
    "section": "5.3 Average or mean as the decision in the long run",
    "text": "5.3 Average or mean as the decision in the long run\nDavid had a thought. “Let us try the decision value of 4 using Amanda’s approach and see what total penalty we get for 20 trials.”\nThey did, and Table 5.3 shows that indeed the total penalty for this is lower than the first two and the average penalty per trial is 166/20. This is higher than the average of 7 with 10,000 trials, but we can chalk it up to using only 20 trials.\n\n\n\nTable 5.3: Decision, random choices, and resulting penalties (Decision = 4)\n\n\n\n\n\nDecision =\n4\n\n\nrandom_choice\ndiff\npenalty\n\n\n\n\n8\n-4\n16\n\n\n2\n2\n4\n\n\n1\n3\n9\n\n\n2\n2\n4\n\n\n4\n0\n0\n\n\n4\n0\n0\n\n\n8\n-4\n16\n\n\n2\n2\n4\n\n\n8\n-4\n16\n\n\n8\n-4\n16\n\n\n4\n0\n0\n\n\n4\n0\n0\n\n\n9\n-5\n25\n\n\n1\n3\n9\n\n\n3\n1\n1\n\n\n9\n-5\n25\n\n\n2\n2\n4\n\n\n2\n2\n4\n\n\n1\n3\n9\n\n\n2\n2\n4\n\n\nTotal penalty =\n166\n\n\n\n\n\n\nSo what is special about 4? Given another set of numbers, do we have to repeat this charting process that Suzie did for each set of numbers?\nWell, 4 happens to be the average of the numbers. It turns out that when we have a large number of trials, we will get the best results, that is the lowest total penalty, when we always guess the average of the numbers. In this case the average is 4.\nSo, we see that in the absence of any other information to help us predict, the best prediction is the mean. Figure 5.3 shows this pictorially.\n\n\n\n\n\n\nFigure 5.3: Many numbers, but just a single prediction!\n\n\n\nWhat we have said above does not mean that we are guaranteed to get the smallest total error if we commit to the average. Clearly, if someone were lucky enough that the random picks all ended up exactly equal to or very close to their decision just by chance, then their total error will be zero! But this is extremely unlikely.\nHowever, those occurrences are extraordinarily unlikely with a large number of trials, as happens in business where a decision is tested by thousands of events. Statistically the best course would be to pick the average.\n\n\n\n\n\n\nQuick Check\n\n\n\nIn what sense is the mean the “best” model?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\n“Best” means minimizing overall prediction error, not being perfect.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nCan a model be “best” and still be inaccurate?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nYes. A model can be the best available option given limited information and still perform poorly in absolute terms.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nDoes it make intuitive sense that the average turns out to be the best model? Explain why or why not.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIt makes sense to guess a number sort of in the middle so that we are generally close to all numbers that could turn up on random choices. See s@ec-simple-model-enrichment-1 if you want to thin k about this more.\n\n\n\n\n\n\n\n\n\nInterpret the word “best model” with caution\n\n\n\nWhen we say the mean is the best model, we do not mean it is always accurate, fair, or appropriate. We mean it is best according to a specific criterion, under specific information constraints.\n\n\nNow you know why I titled the chapter The simplest Model!",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models-story.html#real-world-connection-planning-for-customer-demand",
    "href": "modules/03-introduction-to-models/path-to-models-story.html#real-world-connection-planning-for-customer-demand",
    "title": "5  The Simplest Model",
    "section": "5.4 Real world connection: Planning for Customer Demand",
    "text": "5.4 Real world connection: Planning for Customer Demand\nHow does this small “practice game” relate to the real world? Let us return to the manager’s kiosk problem.\nA company operates two small sales kiosks in the same city—one at the Beach and one at the Mall. The company must decide how many workers to schedule each day.\n\n5.4.1 What the company must do\n\nThe staffing decision must be made in advance\nOnce the schedule is set, it cannot be easily changed without disrupting employees\nIn this first round, the manager insists on one number to use everywhere (the same staffing plan for both kiosks)\n\n\n\n5.4.2 What the company knows\n\nThe company has historical data on daily customer counts—but in an unhelpful format\nThe counts are not labeled by kiosk (Beach vs Mall)\nAt this point, the company does not have any data to explain why customer traffic changes. It simply observes that demand varies\n\nHere are the 20 historical customer counts (mixed together, with no kiosk identification):\n89, 95, 131, 101, 103, 134, 109, 75, 86, 91, 177, 151, 152, 143, 123, 194, 155, 81, 161, 126\n\n\n5.4.3 Why the decision matters\n\nIf too few workers are scheduled:\n\ncustomers wait longer\nservice quality suffers\nsome customers leave without buying\n\nIf too many workers are scheduled:\n\nworkers are idle\nlabor cost rises\nmanagers waste attention adjusting schedules\n\n\n\n\n5.4.4 How the company measures mistakes\n\nBeing slightly wrong is not very costly\nBeing very wrong is much more costly\nThe manager therefore measures the cost of a decision as:\n\nthe square of the difference between planned customers and actual customers\n\n\n\n\n5.4.5 How the decision is evaluated\n\nThe decision is not judged by a single day\nA decision that works well once may simply be lucky\nInstead, the manager looks at performance across many days\nDaily customer counts vary, but the staffing decision stays the same\n\n\n\n5.4.6 The conclusion\n\nThe manager must choose a single number to plan for\nUnder a squared-penalty rule, the number that minimizes total long-run cost is the average of past customer counts (here, about 123.8)\nChoosing a smaller number leads to frequent large shortages\nChoosing a larger number leads to frequent over-staffing\nThe average balances these errors over time\n\n\n\n5.4.7 Why this matters\n\nThis is the first “model” you learn in this book: a model that predicts the same value every time\nIt is simple, but it is not arbitrary—it is the best choice given the information available\nLater chapters improve on this model by adding explanatory information (first categories, then numerical predictors)",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models-story.html#optional-enrichment-topic-how-does-skew-affect-the-situation",
    "href": "modules/03-introduction-to-models/path-to-models-story.html#optional-enrichment-topic-how-does-skew-affect-the-situation",
    "title": "5  The Simplest Model",
    "section": "5.5 Optional enrichment topic: How does skew affect the situation?",
    "text": "5.5 Optional enrichment topic: How does skew affect the situation?\nIn our practice game, we used the numbers:\n4, 2, 6, 1, 8, 1, 3, 9, 2, 4\nin our game. That set has numbers across the entire range. Is it the case that the average performed well because of this? Could it be that the average will not perform well if most of the numbers fall within in a small range and a few extreme cases tend to push the average up?\nFor, example, let us take the following set of numbers:\n2, 1, 2, 1, 3, 3, 2, 1, 10, 9\nThe mean is 3.4. However, this set has eight of its numbers less than or equal to 3. Perhaps a decision below the average will work better?\nLet us play the game 10,000 times and see which decision performs best.\nFigure 5.4 shows the results. We see that the mean is still the best decision.\n\n\n\n\n\n\nFigure 5.4: Average of total_penalty vs various values for decision based on 10,000 plays of the game – shows that the lowest total penalty overall occurs when decision equals the mean of the numbers\n\n\n\nWhy does this happen? Choosing the average definitely leads to small penalty increases most of the time. However, if we went below the average, the relatively rare cases in which the random choice is high, it incurs a very large penalty and that offsets any benefits of going below the average, because we square the difference and that amplifies the error.\nWhat if we did not square the error and instead treated the absolute difference as the penalty. In this case, the median is the best choice. Pretty cool result if you as me! We will not go further into that topic, but it has its applications. In most practical applications, we use the squared-difference as the penalty.\n\n\n\n\n\n\nPause & Think\n\n\n\n\nIf two data sets have the same mean, could the mean be a better model for one than the other?\nWhat additional information might help you judge if it will work better for one than for the other?\n\n\n\n\n\n\n\n\n\nSuggested answers\n\n\n\n\n\n\nYes. The mean works better when observations are tightly clustered.\nVariability, spread, or deviation from the mean would help evaluate model quality.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models-story.html#sec-simple-model-enrichment-1",
    "href": "modules/03-introduction-to-models/path-to-models-story.html#sec-simple-model-enrichment-1",
    "title": "5  The Simplest Model",
    "section": "5.6 Optional enrichment topic: Removing outliers in skewed distributions",
    "text": "5.6 Optional enrichment topic: Removing outliers in skewed distributions\nContinuing from the prior discussion, when faced with a skewed distribution we have a peculiar situation where our intuition tells us to make a decision where most points lie, but cold computation tells us otherwise.\nOne may feel that it makes no sense to choose the mean and accumulate a slightly higher penalty in every event just to balance out the huge penalties that come rarely. But that is what makes sense when we use the quadratic penalty function.\nIf the penalty is quadratic, but we still want to not decide on the average, then we can eliminate the extreme points and reduce the skew. Then we can use the average as before. From a practical viewpoint what that would mean is that we take steps to avoid extreme events by some means.\nInterestingly, if we do not square the differences and simply add the absolute values of the differences to compute the total penalty, then the median (or the middle value among the numbers when they are ordered by magnitude) is the best decision. I find that result very cool!\nCan you think of an experiment to compare removing outliers and not removing them in the context of skewed distributions to see if using the mean in both cases reduces the total penalty significantly?\nYou can delve deeper into this topic if you are interested.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/category-means-story.html",
    "href": "modules/03-introduction-to-models/category-means-story.html",
    "title": "6  Category Means",
    "section": "",
    "text": "6.1 Staffing kiosks at malls and beaches\nAngela, Steve, Dave, Igor, and Suzie – our interns from the previous chapter, sat around the table in the conference room waiting for Amanda and discussed the previous night’s exciting professional basketball game.\nLast chapter, the interns learned an important lesson: when you must commit to one number without any further information, and you measure mistakes more harshly the bigger they are (by using a squared penalty), the best long-run choice is the average.\nAmanda walked in exactly at 9 am. WIth a bright smile, she said “Team, I have some good news. First off, we did very well last month. We had far fewer days of bad mismatches from our staffing levels with the customer counts. Your model improved significantly on what we were doing earlier.\nSecondly, I have better data this time and I am hoping we can improve our decision further.”\n“I have better data this time” Amanda says, pulling up a spreadsheet on her laptop. “The same two kiosks. Same city. Same staffing problem. But …”\nShe points to the new column.\n“This time, the person collecting data included the kiosk.”\nThe interns lean in. The data now tells them where each customer count came from: Mall or Beach.\nThe key difference from the previous chapter is that the decision no longer has to be the same everywhere. Instead, the company can make one decision for the Mall kiosk and a different decision for the Beach kiosk, using the information it has about location. Amanda hoped that this can lead to an even better match between the staffing and the actual customer numbers.\nHere is the data:\nAmanda taps the screen.\n“Last month I asked for, and you gave me, one figure for the number of customers for both kiosks” she says. “Now we have more information and want to do better. Can you use this extra information?”\nIn this scenario, we need to decide a customer level to use for determining staff level – but separately for each kiosk.\nInitially it seems to many of the interns that the new information really does not add any value. David says, “Why not just adopt the overall average again like last month? I mean how is this kiosk information helping us?”\nAngela says “Let us consider an extreme scenario. What if the sales at the Beach kiosk is generally twice that of the sales in the Mall kiosk. In that case, predicting the same number for both does not seem right. Surely we will not have the same staffing level for both. So differentiating the two should make sense.”\n“I agree” Suzie says. “We should first look at the data. Even if the difference in sales is not as dramatic as Angela mentioned, it might still make sense to differentiate. I have computed the averages for the two kiosks. Here they are.” Suzie then shows Table 6.1.\nDavid: “I see your point. These sales do seem quite different with the beach kiosk getting 40% more customers per day on average. Perhaps a different model value does make sense after all. What should the value be though for each kiosk?”\nAgain, Igor speaks last, a little less timid than he was the last time around. “If we treat the data separately for each kiosk then we really have two problems that are identical to our problem from last month.”\nPeter: “Then should we just say the model value for each kiosk’s is its own average sales?”\nThe rest nods in agreement. They cannot see any flaw in this.\nPeter: “For the Mall kiosk, we can use the average of the customers column for only the Mall rows, and similarly for the Beach kiosk.”\nAngela: “Can we verify this with a chart like we did the last time? Except that we will have one chart per kiosk.”\nSuzie: “Since the computer is going to do all the work, we can also compare the average penalty per trial for the two kiosks and compare that with using the overall average.””\nSuzie’s plots for the average penalty for different guesses for each of the kiosks appear in Figure 6.1.\nDave says “As we expected, the best decisions for the two kiosks is still the average, but computed separately for each kiosk. In the plot for the beach kiosk, the lowest average penalty occurs at its average of 146 and at 101 for the mall kiosk.”\nSuzie also reported the average penalties if we used the overall average of number of customers and if we used the specific averages for each kiosk.\nWe can see that the penalty is far lower if we use the kiosk-specific averages rather than one overall average. Why is the average penalty so much higher for the beach kiosk than the mall kiosk? Let us see the variability in the data for these two kiosks. See Figure 6.2.\nkiosk_mall_beach |&gt; \n  point_plot(customers ~ kiosk, annot = \"violin\")\nSince the beach kiosk has a lot more variability than the mall kiosk, the penalty is much more as choosing the average as the decision will be off from the actual value by quite a bit on many days.\nThe interns present their findings to Amanda.\nAmanda studies the plots and the reduction in average penalties and says “This is good. It looks like having more information surely helps us to make better decisions.”\n“She continues, “the lesson is not ‘always use the mean. but’ use the best mean you’re allowed to use, given the information you have.”",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Category Means</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/category-means-story.html#staffing-kiosks-at-malls-and-beaches",
    "href": "modules/03-introduction-to-models/category-means-story.html#staffing-kiosks-at-malls-and-beaches",
    "title": "6  Category Means",
    "section": "",
    "text": "customers\nkiosk\n\n\n\n\n89\nMall\n\n\n95\nMall\n\n\n131\nMall\n\n\n101\nMall\n\n\n103\nMall\n\n\n134\nMall\n\n\n109\nMall\n\n\n75\nMall\n\n\n86\nMall\n\n\n91\nMall\n\n\n177\nBeach\n\n\n151\nBeach\n\n\n152\nBeach\n\n\n143\nBeach\n\n\n123\nBeach\n\n\n194\nBeach\n\n\n155\nBeach\n\n\n81\nBeach\n\n\n161\nBeach\n\n\n126\nBeach\n\n\n\n\n\n\n\nIf we are given only the data in column 1 (the customers counts), then based on the previous chapter, our best choice is the overall average.\nIf we are given the entire table, then we have additional information and can possibly determine a different number for each kiosk.\n\n\n\n\n\n\n\nTable 6.1: Average sales at the two kiosks\n\n\n\n\n\nkiosk\navg_customers\n\n\n\n\nMall\n101\n\n\nBeach\n146\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.1: Plots of guess against average penalty for the two kiosks: As expected, choosing the average (101 for mall, and 146 for beach) generates the lowest average penalty\n\n\n\n\n\n\nAverage penalty per choice if we use the overall average number of customers: 1114\nAverage penalty per choice if we use kiosk-specific average for Mall: 325\nAverage penalty per choice if we use kiosk-specific average for Beach: 880\n\n\n\n\n\n\n\n\n\nFigure 6.2: Comparison of the distributions of the customer traffic for the two kiosks: Beach has a lot more variability",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Category Means</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/intro-to-models.html",
    "href": "modules/03-introduction-to-models/intro-to-models.html",
    "title": "7  What is a model?",
    "section": "",
    "text": "7.1 Mathematical models\nNow that we have looked at the overall mean and category means as good rules for decisions (in the absence of more information), we are ready to learn about models.\nPeople use the word model in many senses. In one of the more common usages, a model is an approximate representation of something for some purpose.\nFigure 7.1 and Figure 7.2 show the image of a house and a miniature model of it. The miniature is a good representation of the house in some respects. For example, it depicts the overall shape and some exterior details quite accurately. If someone were about to build a new house and the architect showed them this model, they would get a good idea about the overall external appearance of the house and the color scheme as well. However, this model differs from the actual house in many ways as well – one cannot actually step inside it! We cannot get a feel for how roomy the interior will feel. We do not know how the house will look in its actual surroundings. It is accurate in some respects and inaccurate in others.\nShould we consider the model in Figure 7.2 good? In the words of statistician George E. P. Box, “All models are wrong. But some are useful for certain purposes.”\nAll models are wrong in the sense that they are only representations. However, as the miniature model of the house in Figure 7.2, they can still be useful for some purpose.\nWe will be building many models in this course and should remember the following:\nHow to apply this philosophy:\nIn this course, we will be building a specific kind of models – statistical models. However, before getting to statistical models, we will consider some simpler mathematical models. The term mathematical model can connote many different things. Let us clarify the sense in which we use the term. Suppose I have a data frame homes with data about many homes. For each home, we have its age, floor-area_sqft, number_of_bedrooms, number_of_bathrooms, and price. Here are the initial rows in the data frame.\nhomes |&gt; \n  head()\nWe might build a model to determine the value of price given the values of one or more of the other variables. In this case, let us use the variable floor_area_sqft. Remember, this is a model and so we do not expect it be be exact. We do not aim for the model to determine the actual price, but only to compute a good estimate of the price. (We have obviously simplified the situation for pedagogical purposes. House prices depend on many other factors. But that need not hold us back. You will still learn the underlying concepts.).\nWhat might the model look like? Equation 7.1 shows the model. For now, do not worry about how we arrived the model.\n\\[\n\\widehat{\\text{price}} = 61{,}399 + 394\\,\\text{floor\\_area\\_sqft}\n\\tag{7.1}\\]\nIn the model, we have placed a hat over price because the model computes not the actual price of a home with the specified floor area and instead only computes an estimate. In statistics, we place a hat over quantities that estimate other quantities.\nEquation 7.1 shows a linear model. We call it linear because all variables are only raised to the fist power. We have not squared or raised a variable to any other power. In this course we build only linear models.\nWe call the equation that Equation 7.1 shows a model function.\nIn our context, we use the term model to denote a mathematical equation of the kind that Equation 7.1 shows. Our models help us to determine the value of a variable in a data frame for any given instance. Specifically, in the house price example that we just considered, the model in Equation 7.1 enables us to estimate the price of a home, given its floor area.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>What is a model?</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/intro-to-models.html#mathematical-models",
    "href": "modules/03-introduction-to-models/intro-to-models.html#mathematical-models",
    "title": "7  What is a model?",
    "section": "",
    "text": "7.1.1 Response and Explanatory variables\nWe call the variable whose value the model estimates as the response variable. The variables based on which the model estimates the response variable are called explanatory variables. We can also refer to the response variable as the dependent variable, and explanatory variables as independent variables.\n\n\n7.1.2 Using the model\nThe house in the fifth row of the data frame has floor area of 1480. Its actual price is $553,000. What price does the model estimate? Well, we can plug the floor area into the model – Equation 7.1 – and find out. If we plug it in, we get $645,999. The model overestimates this price.\nThe house in the fourteenth row of the data frame has floor area of 1293 Its actual price is $561,000. What price does the model estimate? Well, we can plug the floor area into the model – Equation 7.1 – and find out. If we plug it in, we get $572,134. Quite close.\nYou should try out some more cases and wee how the model does. We will learn a lot more about such models, including precisely measuring their quality as we go forward.\nI won’t blame you if you are wondering “We already have the prices in the data frame. Why do we need a model to calculate these?”\nConsider again the home in the fifth row. Why did the model overestimate the price? Well, you should consider that this particular home was not the only one whose floor area was in the vicinity of 1480. The data set has other homes with similar floor areas and their prices vary. If we look at homes that have floor areas between 1430 and 1530, we see that their prices range from $553,000 to $647,000. In fact, many homes below 1480 square feet in floor area have higher prices. How could that be? Well, variables other than floor area play a role in price too. So our model makes an estimate based on the whole data frame.\nThe next section addresses two main uses of models.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>What is a model?</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/intro-to-models.html#purpose-of-a-model",
    "href": "modules/03-introduction-to-models/intro-to-models.html#purpose-of-a-model",
    "title": "7  What is a model?",
    "section": "7.2 Purpose of a model",
    "text": "7.2 Purpose of a model\nWe use models in two main ways:\n\nPrediction: Suppose we have built a model using the data in a data frame to estimate the price of a home, given its floor_area_sqft. We can potentially use this model to estimate the price of any home whose floor area we know and whose price we do not know – even if this home is not in the data set.\nExplanation: Our model in Equation 7.1 tells us that the price of a home is related to its floor area. This makes sense and we could have said this even without a model. However, when people gather data about phenomena that they do not fully understand, and want to understand, they often build models to understand and explain which variables seem to be related and in what way to a variable of interest.\n\nSo, for our purposes in this course, we can define the term model as:\n\nA model is a simple story we tell about data to help us summarize, explain, or predict.\n\nYou might have noted that both of the above purposes actually require us to use a model in contexts that go outside the data that we based the model on. When we start using models outside of the data we used to build the model, we come face to face with the essence of statistics. We will get to that later in the course.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>What is a model?</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/intro-to-models.html#average-is-a-model-too",
    "href": "modules/03-introduction-to-models/intro-to-models.html#average-is-a-model-too",
    "title": "7  What is a model?",
    "section": "7.3 Average is a model too!",
    "text": "7.3 Average is a model too!\nIn Equation 7.1, we had a response variable and an explanatory variable. What if we do not have any explanatory variable? In the context of our home prices example, not having an explanatory variable is equivalent to saying:\n\n“I have a data set of 100 homes”\n“Here are their prices”\n“I am not telling you anything else about these homes”\n“I have selected a random home from my data set”\n“Guess the price of the home I have in mind”\n\nBased on the game we played in Chapter 5, you first compute the average of the prices as $733,440. Your best model to make your guess is:\n\\[\n\\widehat{\\text{price}} = 733{,}440\n\\tag{7.2}\\]\nThat is, like in the game we played, no matter which home I choose, you always state the average as your guess. Equation 7.2 shows the model function when we have no\nThe big difference between the models in Equation 7.1 and Equation 7.2 is that the second one has no explanatory variable. When we are not given any additional helpful information to estimate a value, our best estimate – best model – is the average!",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>What is a model?</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/visualizing-simple-models.html",
    "href": "modules/03-introduction-to-models/visualizing-simple-models.html",
    "title": "8  Visualizing Mean Models",
    "section": "",
    "text": "8.1 Business scenario – fuel efficiency of vehicle fleet\nIn the prior chapters, we have established the overall mean as the best model when we have no other explanatory variable, and the category mean as the simplestbest model* when we have a categorical explanatory variable.\nIn this chapter we use the point_plot function to visualize these models.\nIn Chapter 5, we used a set of numbers to play a game. Now we will extend the idea to actual data frames.\nYou manage a company that operates a fleet of rental cars used primarily for city driving. The fleet includes a wide range of vehicles, from compact sedans to larger SUVs, each with different city fuel efficiency.\nFor planning purposes, the company must commit to one single number to represent the expected city fuel efficiency of a car drawn at random from the fleet. This number is used repeatedly—for estimating fuel costs, setting reimbursement rates, and budgeting operating expenses.\nThe company does not know in advance which specific car will be rented on a given day. Some days the car will be more fuel-efficient than expected, and on other days less fuel-efficient.\nIf the company assumes fuel efficiency that is too high, it underestimates fuel costs. If it assumes fuel efficiency that is too low, it overestimates costs and ties up capital unnecessarily. Larger errors in either direction are more costly than smaller ones.\nThe decision is not evaluated based on a single rental. Instead, it is judged by how well it performs across many rentals over time.\nIn this setting, the question becomes: “What single number should the company commit to so that total long-run cost is as small as possible?”\nThe data about your fleet is in the mpg data. It has many variables, but we focus here on the variable cty showing us the city miles per gallon of each car.\nAssuming (artificially) that the company is only allowed to use the variable cty what single number should they commit to?",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualizing Mean Models</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/visualizing-simple-models.html#business-scenario-fuel-efficiency-of-vehicle-fleet",
    "href": "modules/03-introduction-to-models/visualizing-simple-models.html#business-scenario-fuel-efficiency-of-vehicle-fleet",
    "title": "8  Visualizing Mean Models",
    "section": "",
    "text": "8.1.1 Committing to one number for the entire fleet\nBased on our discussion in Chapter 5, we know that the the company should commit to the average of the variable cty. We can compute the average using the summarize and mean functions as the code below shows.\n\nmpg |&gt; \n  summarize(avg_cty = mean(cty))\n\n\n  \n\n\n\nWe can also visualize it thus:\n\nmpg |&gt; \n  point_plot(cty ~ 1,\n             annot = \"model\",\n             interval = \"none\")\n\n\nThis generates Figure 8.1.\n\n\n\n\n\n\nFigure 8.1: Visualizing mean cty as the model\n\n\n\nThe model shows the scatter plot of the points as well as a line. This line represents the model. You can see that it falls exactly at the average of the cty values.\nTake a look at the code:\nThe tilde expression\ncty ~ 1\nin the code sets cty ans=d the response variable and, by placing 1 on the RHS, shows that we have no explanatory variables.\nThe part\nannot = \"model\nCauses a model to be visualized as well. We know that the mean is our model and this plot visualizes it by drawing a line at the average cty.\nFor now, you can ignore:\ninterval = \"none\"\nSo, if we have to commit to one number as our estimate for the city mileage irrespective of other details about a car like its class, drv or anything else, our best commitment is to the overall average. As we have seen in Chapter 7, this is a model with just a response variable and no explanatory variables. We can write it as Equation 8.1.\n\\[\n\\widehat{\\text{cty}} = 16.9\n\\tag{8.1}\\]\nWe placed a hat on cty because this is just an estimate or a model value and not an actual data point.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualizing Mean Models</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/visualizing-simple-models.html#we-can-do-better-with-more-information",
    "href": "modules/03-introduction-to-models/visualizing-simple-models.html#we-can-do-better-with-more-information",
    "title": "8  Visualizing Mean Models",
    "section": "8.2 We can do better with more information",
    "text": "8.2 We can do better with more information\nInstead of having to commit to one single number for the fuel efficiency for the entire fleet, what if we could use a different number for each class of vehicle? In out mpg data set, we have vehicles of different classes – variable class. This has values like Compact, SUV and Pickup truck.\nBased on Chapter 6, we know that for each class of vehicle, we should commit to the average of cty for that class.\nWe can compute these averages easily:\n\nmpg |&gt; \n  summarize(avg_cty = mean(cty), .by = class)\n\n\n  \n\n\n\nNote how the above code is very similar to what we would use for computing the overall average, but we have added:\n.by = class\nTo show that we do not want just one average for the whole data frame, but we want separate averages for each class of vehicle. As simple as that!\nLet us visualize this model:\n\nmpg |&gt; \n  point_plot(cty ~ class, annot = \"model\",\n             interval = \"none\")\n\nFigure 8.2 shows the model visualization.\n\n\n\n\n\n\nFigure 8.2: Visualizing the more sophisticated model of mean cty by class as the model when we have class as an explanatory variable\n\n\n\nHow do we express this model as an equation like we did with Equation 8.1?\nEquation 8.2 shows the model as an equation.\n\\[\n\\widehat{\\text{cty}} =\n\\begin{cases}\n20.1 & \\text{if } \\text{class} = \\text{\"compact\"} \\\\\n18.8 & \\text{if } \\text{class} = \\text{\"midsize\"} \\\\\n13.5 & \\text{if } \\text{class} = \\text{\"SUV\"} \\\\\n15.4 & \\text{if } \\text{class} = \\text{\"2seater\"} \\\\\n15.8 & \\text{if } \\text{class} = \\text{\"minivan\"} \\\\\n13.0 & \\text{if } \\text{class} = \\text{\"pickup\"} \\\\\n20.4 & \\text{if } \\text{class} = \\text{\"subcompact\"}\n\\end{cases}\n\\tag{8.2}\\]\nInstead of class, we can have drv as the explanatory variable as well. Here are code snippets that will compute the model and plot it.\n\nmpg |&gt; \n  summarize(avg_cty = mean(cty), .by = drv)\n\n\n  \n\n\n\nNote that we only had to change the .by to reflect that we are now computing averages by drv.\n\nmpg |&gt; \n  point_plot(cty ~ drv, annot = \"model\",\n             interval = \"none\")\n\nWe just changed the RHS of the tilde expression to drv instead of class.\nFigure 8.3 shows the visualization.\n\n\n\n\n\n\nFigure 8.3: Visualizing the model of mean cty by drv as the model when we have drv as an explanatory variable\n\n\n\nEquation 8.3 shows the model in equation form.\n\\[\n\\widehat{\\text{cty}} =\n\\begin{cases}\n20.0 & \\text{if } \\text{drv} = \\text{\"f\"} \\\\\n14.3 & \\text{if } \\text{drv} = \\text{\"4\"} \\\\\n14.1 & \\text{if } \\text{drv} = \\text{\"r\"} \\\\\n\\end{cases}\n\\tag{8.3}\\]\nThe mpg data frame has another column fl to represent the fuel type of the vehicle. We now want to use fl as the explanatory variable. Try out the following using the above as examples.\n\nWrite R code to compute the average of cty for each fuel type\nWrite R code to visualize the model with cty as the response variable and fl as the explanatory variable.\nWrite out the model equation for this scenario.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualizing Mean Models</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/04-overview.html",
    "href": "modules/04-more-on-models/04-overview.html",
    "title": "Module 4: Linear Regression Models",
    "section": "",
    "text": "Learning Goals\nThis module fills in the details on top of the intuitive foundation for a linear model that the previous module laid.It introduces the idea of the least squares best fit line and its corresponding model. The module covers R code for building linear models for the case of a single numerical explanatory variable, a single categorical variable and a combination of one numerical and one categorical explanatory variable.It shows how to distill the model function based on the regression output.",
    "crumbs": [
      "More on models -- least squares",
      "Module 4: Linear Regression Models"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/04-overview.html#learning-goals",
    "href": "modules/04-more-on-models/04-overview.html#learning-goals",
    "title": "Module 4: Linear Regression Models",
    "section": "",
    "text": "a\nb\nc",
    "crumbs": [
      "More on models -- least squares",
      "Module 4: Linear Regression Models"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/04-overview.html#structure-of-this-module",
    "href": "modules/04-more-on-models/04-overview.html#structure-of-this-module",
    "title": "Module 4: Linear Regression Models",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nmention that the model gives an estimated value, but defer detailed discussion until later\nR code for building model with no explanatory variable and reconstructing the model function\nR code for building model with one numeric explanatory variable and reconstructing the model function\nR code for building model with one numeric explanatory variable and one categorical explanatory variable and reconstructing the model function",
    "crumbs": [
      "More on models -- least squares",
      "Module 4: Linear Regression Models"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/line-of-best-fit-story.html",
    "href": "modules/04-more-on-models/line-of-best-fit-story.html",
    "title": "9  Line of Best Fit",
    "section": "",
    "text": "9.1 The beach kiosk staffing problem\nIn prior chapters we have looked at the situation when we have a numerical response variable and either no explanatory variable or a single categorical explanatory variable. In this chapter we look at the important case when the response variable and the explanatory variable are numerical.\nIn our running story, this is the third meeting of our interns with Amanda who owns a business that operates kiosks in a city at two locations. Last month, the interns used kiosk labels to help set different staffing levels for the Beach and the Mall.\nNow Amanda brings a new kind of hint.\n“It’s not just location that seems to affect the number of customers we get.” she says. “Today, I need you all to help me with a decision for the beach kiosk. At the Beach kiosk, demand seems to swing with temperature. I want a model that uses what we know today about tomorrow’s temperature (the forecast) to predict customer traffic for tomorrow. This time, I will use the prediction not for staffing, but to decide on stock levels of items. We do not want to under or overstock too much and so good predictions can really help me.”\nShe adds one constraint immediately:\n“Keep it simple. I need something I can explain in one sentence.”\nIn this chapter, we continue our model-building journey. A model is simply a rule that takes information we have and produces a predicted value for the response.\nIn this chapter we consider a single numerical explanatory variable. In this case we need something quite different.\nWe again consider a situation where the company gets a hint before making a decision—but now the hint is numerical, not categorical.\nAmanda is focused on the Beach kiosk. (The Mall kiosk is steadier, but the Beach kiosk shas more variance in daily customer numbers and so just using the average can be costly.\nManagement has noticed that customer traffic at the Beach kiosk varies strongly with the day’s high temperature.\nOver many past days, the company has recorded:\nLet us assume that data are available for 25 different days. (As before, for understanding the ideas, the exact number of days is not important.)\nTable 9.1 shows the historical data.\nBased this, the company must decide how many customers will show up tomorrow. They will u8se that to determine how much of each item to stock for sale. It can use an extremely reliable temperature forecast for its city.\nThe problem now is this:",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line of Best Fit</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/line-of-best-fit-story.html#the-beach-kiosk-staffing-problem",
    "href": "modules/04-more-on-models/line-of-best-fit-story.html#the-beach-kiosk-staffing-problem",
    "title": "9  Line of Best Fit",
    "section": "",
    "text": "the day’s high temperature (in °F), and\nthe number of customers who visited the beach kiosk that day.\n\n\n\n\n\n\nTable 9.1: Daily temperature and customer counts\n\n\n\n\n\nday\ntemperature\ncustomers\n\n\n\n\nD01\n58\n98\n\n\nD02\n60\n104\n\n\nD03\n62\n122\n\n\nD04\n64\n113\n\n\nD05\n66\n117\n\n\nD06\n68\n133\n\n\nD07\n70\n126\n\n\nD08\n72\n115\n\n\nD09\n74\n123\n\n\nD10\n76\n128\n\n\nD11\n78\n145\n\n\nD12\n80\n141\n\n\nD13\n58\n106\n\n\nD14\n60\n107\n\n\nD15\n62\n105\n\n\nD16\n64\n127\n\n\nD17\n66\n120\n\n\nD18\n68\n103\n\n\nD19\n70\n128\n\n\nD20\n72\n121\n\n\nD21\n74\n120\n\n\nD22\n76\n130\n\n\nD23\n78\n127\n\n\nD24\n80\n132\n\n\nD25\n58\n98\n\n\n\n\n\n\n\n\n\nGiven the forecast high temperature for a day, how many customers should the company plan for?\n\n\n9.1.1 How does this differ from earlier examples we have studied?\nIn Chapter 5, we used no hints at all, and the mean turned out to be the best model.\nIn Chapter 6, our hint was categorical, and the mean within each category turned out to be the best model.\nThis time, the situation is fundamentally different. Our hint, temperature, is numerical, and so none of the earlier “mean-based” approaches applies directly.\nOne possibility would be to convert temperature into categories such as Cold, Warm, and Hot, and then apply the category-means approach. While this would work, it is somewhat artificial. Two days that differ by only one degree could end up in different categories. For example, calling 50°F Cold and 51°F Warm introduces an arbitrary cutoff that has no real business justification.\nTemperature does not naturally fall into clear bins. Can we do better?\n\n\n9.1.2 Toward a rule-based model\nRather than assigning a few separate numbers, Amanda now wants a rule that:\n\ntakes the day’s temperature as input and produces an estimated number of customers as output.\n\nMany such rules are possible. The company wants a simple rule that takes a temperature as input and produces a predicted number of customers as output. Let us assume that the same general pattern will continue for temperatures. It is not going to be exactly the same, but the general range and the distribution will remain the same for the next month. Therefore, for any given temperature the model’s or rule’s output should reasonably match the actual number of customers in the data set.\nFigure 9.1 shows what Amanda is looking for at a high level.\n\n\n\n\n\n\nFigure 9.1: Model that takes a temperature as input and produces a predicted number of customers for the beackh kiosk – the model should be based on the data in Table 9.1\n\n\n\nWe know that with higher temperatures more people will hit the beach.\nPeter has an idea. “Since Amanda is looking for a simple rule, should we consider consider a model which looks something like this?” He draws a picture. Figure 9.2). “Of course, I just made up the numbers 15 and 2. With this model, if we input a temperature of 70, the model will predict 155 customers.”\n\n\n\n\n\n\nFigure 9.2: Caption\n\n\n\nPeter further clarifies “I did not mean the specific numbers 15 and 2 literally in Figure 9.2, I am really saying that we could look for a model that has the general shape in this figure” and he quickly sketches Figure 9.3\n“We just need to find good values for a and b.\n\n\n\n\n\n\nFigure 9.3: Peter’s second explanatory sketch showing that he wants a model of this general form, but wants to find good values for a and b so that the model generates good predictions\n\n\n\nPeter then writes Equation 9.1 to explain the same idea and adds “I placed a hat on top of customers because the equation only calculates an estimate and this could be different from an actual value.”\n\\[\n\\widehat{\\text{customers}} = a +  b\\,\\text{temperature}\n\\tag{9.1}\\]\nAngela says “That’s a great idea. Amanda wanted something simple. Of course we gave her very simple models earlier too, but now we have to incorporate temperature and I cannot imagine something that is adjusts its predictions to the temperature and is simpler than what Peter has shown.”\n“If we fix the values of a and b, eq-kiosk-cust-temp-model produces a predicted number of customers for any temperature. For example, if we fix a value of 15 for a and 2 for b as Peter did before, then Equation 9.2 shows our model to determine the number of customers for any temperature. If we plug in a temperature of 70, the model will give us 155 as the estimated number of customers.”\n\\[\n\\widehat{\\text{customers}} = 15 +  2\\,\\text{temperature}\n\\tag{9.2}\\]\nIgor says “I like this idea too. Its prediction changes smoothly as temperature changes, and it avoids arbitrary cutoffs.”\nDave sounds a cautionary note “This is all great guys, but how do we find the best values for a and b? And what do we mean by good choices?”\nSuzie: “I have an idea about how we might define good. Suppose we have a model – let us say for the time being that we use Equation 9.2. That is, given any temperature, we plug it into Equation 9.2 and it spits out the estimated number of customers.\nSuppose we plug in 66 for temperature. Equation 9.2 says the number of customers is 15 + 2*66, or 147. In our data, the number of customers for 66 degrees is 117 for one of the rows where the temperature is 66.”\nDavid’s eyes light up “Oh, we can treat this just like we did earlier and get the penalty by squaring!. This penalty will become 30*30, or 900 for that one row.”\nIgor then observes “For any given value of a and b, we can compute the penalty corresponding to each row of the data. With a = 15 and b = 2, we can compute the penalties for each row.” He then worked for a few minutes to come up with penalties for all the rows of Table 9.1 and came up with Table 9.2.\n\n\n\nTable 9.2: Igor’s computations: For each row he plugged in the temperature into Equation 9.2 and computed est_customers (estimated customers by Equation 9.2) and then computed the squared difference between the actual value of customers and est_customers\n\n\n\n\n\nday\ntemperature\ncustomers\nest_customers\ndiff\npenalty\n\n\n\n\nD01\n58\n98\n131\n-33\n1089\n\n\nD02\n60\n104\n135\n-31\n961\n\n\nD03\n62\n122\n139\n-17\n289\n\n\nD04\n64\n113\n143\n-30\n900\n\n\nD05\n66\n117\n147\n-30\n900\n\n\nD06\n68\n133\n151\n-18\n324\n\n\nD07\n70\n126\n155\n-29\n841\n\n\nD08\n72\n115\n159\n-44\n1936\n\n\nD09\n74\n123\n163\n-40\n1600\n\n\nD10\n76\n128\n167\n-39\n1521\n\n\nD11\n78\n145\n171\n-26\n676\n\n\nD12\n80\n141\n175\n-34\n1156\n\n\nD13\n58\n106\n131\n-25\n625\n\n\nD14\n60\n107\n135\n-28\n784\n\n\nD15\n62\n105\n139\n-34\n1156\n\n\nD16\n64\n127\n143\n-16\n256\n\n\nD17\n66\n120\n147\n-27\n729\n\n\nD18\n68\n103\n151\n-48\n2304\n\n\nD19\n70\n128\n155\n-27\n729\n\n\nD20\n72\n121\n159\n-38\n1444\n\n\nD21\n74\n120\n163\n-43\n1849\n\n\nD22\n76\n130\n167\n-37\n1369\n\n\nD23\n78\n127\n171\n-44\n1936\n\n\nD24\n80\n132\n175\n-43\n1849\n\n\nD25\n58\n98\n131\n-33\n1089\n\n\n\n\n\n\nAngela said: “Well the total penalty is 28,312. How do we know if different values for a and b can give us a lower penalty?. In earlier weeks we only had to try different values for one thing. Here we have two things a and b to think about. What do we do?”\nSuzie says: “But we have made progress. We have at least clearly defined the problem. We need to find the values for a and b that will give us the lowest possible total penalty along the lines of Igor’s Table 9.2. That is, we try many different values for &a and b and compute Igor’s table (Table 9.2) for that combination. We then select the combination taht produces the lowest total penalty. We want the lowest total penalty because that will mean that we have got the model predictions as close as we can to the real values in the data.”\nPeter says “Hey, this reminds me of something I saw my dad doing for his work. He always says that visualizing makes things easier. WHy don’t we try that first?””",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line of Best Fit</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/line-of-best-fit-story.html#visualizing-the-model",
    "href": "modules/04-more-on-models/line-of-best-fit-story.html#visualizing-the-model",
    "title": "9  Line of Best Fit",
    "section": "9.2 Visualizing the model",
    "text": "9.2 Visualizing the model\nPeter continued: “Graphically, Equation 9.2 represents a straight line. Let us generate a scatter plot of the data first.” He wrote come code and generated Figure 9.4 which showed the relationship between temperature and customers.\n\nkiosk |&gt; \n  point_plot(customers ~ temperature)\n\n\n\n\n\n\n\nFigure 9.4: Scatterplot of temperature against number of customers from the kiosk data frame\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nLooking at the scatter plot of temperature versus customers:\n\nDoes the relationship appear perfectly linear?\nDoes that prevent us from using a linear model?\n\nExplain briefly.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nThe relationship is not perfectly linear; there is noticeable scatter around what seems to be a somewhat linear pattern.\nThis does not prevent us from using a linear model. A linear model is a simplification that captures the overall trend, not a claim that all points lie exactly on a line.\n\n\n\n\nPeter then said: “We have been using Equation 9.2 as our tentative model to explore. In my pre-calculus class, i learned that this is actually the equation of a straight line. I can add it to the earlier plot.”\nHe then generated Figure 9.5 which has both the earlier scatter plot and the line from Equation 9.2. (You need not worry about how he added the line yet. We will learn that shortly.)\n\n\n\n\n\n\nFigure 9.5: Adding a line showing the kiosk model with a = 15 and b = 2\n\n\n\nIgor says: “Wow this model is way off the mark! We can reduce the total penalty by a lot.”\nPeter smiles: “Please explain to us lesser mortals Igor!”\nTry to answer by yourself before reading what Igor has to say.\nIgor says: “Well, the line represents the model predictions. If the predictions are good, they will be near the actual points. But our line,representing the prediction for any temperature is way above the general region where the points actually lie! For example, we see from Figure 9.5 that two days in our historical data have had a temperature of 64 degrees. The number of customers for those two days have been 113 and 127. But the tentative model from Equation 9.2 predicts 143. In fact, we can take any actual row of our data and see that the model predictions are way higher than the general area of the points. So, we clearly did not make great initial choices for a and b! No offense Peter, I know you were just illustrating the form, and not trying to be exact.”\nAngela says: “OK, if we want the model predictions to reflect actual data, we would want the line to be within the point cloud and perhaps generally go through the middle of the cloud. That would make the predictions at least fall in the general region of the actual values. Let us try again with different choices for a and b. How should we change the values? Should we increase or decrease each one?”\nIgor says: “In Equation 9.2 a represents where the line intercepts the y-axis at x = 0. I learned this in my pre-calc class. In that class they called a the intercept.”\nSuzie jumped up “I know why. they call it the intercept because that is where the line intersects the y-axis. But in this plot, the line does not intersect the y-axis at all.”\nIgor: “It does intersect the y-axis, if only we zoom out and show the origin of the plot where x=0 and y=0). Currently the x-axis only starts from around 55.”\nPeter quickly modified the plot to show the origin. Figure 9.6 shows his new plot. This is effectively the same plot as Figure 9.5, only zoomed out so that we can see more.\n\n\n\n\n\n\nFigure 9.6: Model with a = 15 and b = 2 plotted to reveal the origin of the plot so as to see the intercept\n\n\n\nEven though a temperature of 0°F is well outside our data range, visualizing the intercept helps us understand what changing a actually does to the line.\nSuzie: “From the plot, we can see that the line correctly intersects the y-axis at 15 since a = 15. What can we do to improve the line so that it goes through the points instead of way above them?”\nDavid opines “To make the line go through the point cloud, it looks like we need to make the line less steep, almost like grabbing the top right of the line and pulling it down. How do we achieve that?”\nPeter’s pre-calculus comes to the rescue again. “The value for b determines the steepness of the line. So, we can decrease it to get the line to go through the cloud of points. Perhaps we should decrease it from 2 to 1.5 and see what happens.”\nPeter modifies his plot again. Figure 9.7 shows his new plot.\n\n\n\n\n\n\nFigure 9.7: Second attempt: model with a = 15 and b = 1.5\n\n\n\nAngela says: “Wow! that looks so much better! But can we do even better?”\nIgor produced the table for this model and announced: “We have smashed the total penalty from 28312 to 1469! But I still don’t know if that is the lowest possible. Maybe there are other values of a and b that can reduce it even more.”\nPeter, tired of generating more and more plots, says “We can keep on trying – we have an infinite number of choices for a and b. How do we find the best line? What does best even mean?",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line of Best Fit</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/line-of-best-fit-story.html#what-do-we-mean-by-the-best-model",
    "href": "modules/04-more-on-models/line-of-best-fit-story.html#what-do-we-mean-by-the-best-model",
    "title": "9  Line of Best Fit",
    "section": "9.3 What do we mean by the best model?",
    "text": "9.3 What do we mean by the best model?\nIn the previous section, the team tried a few values for a and b and our second attempt proved to be much better than the first. However, we want the best line. But what does that mean?\nDavid says “Before visualizing the models, we had already identified what we mean by best. We had already established that the best model or line, is the ones that generates the lowest total penalty as we computed in Table 9.2. That is, we choose the values for a and b and mimic the process we used for @btl-igor-computations-for-15-2 and choose the values that produce the smallest total penalty. But how do we do that when we have an infinite number of possibilities.”\nAngela says: “Can we not repeat what Suzie did and generate a plot for various values and visually see what the best values are?”\nPeter says: “Guys, before we think about that, I can help to visualize the sort of computations we did in Table 9.2 but for the line in Figure 9.7.” and comes up with Figure 9.8.\nHe goes on to explain: “We are after a model that produces good predictions. Therefore the difference between the actual value and the model prediction (the column diff in Table 9.2) matters. The lower that difference is the better.” He then generates Figure 9.8 to visualize the previous model (a = 15 and b = 1.5) with the differences between the actual values and model predictions explicitly marked for three chosen points. These differences are usually called errors or even better, residuals.\n\n\n\n\n\n\nFigure 9.8: Model with a = 15 and b = 1.5 visualized along with the residual or error for three chosen points\n\n\n\n\n\n\n\n\n\nCommon Misconception\n\n\n\nThe error or residual for a prediction is not the shortest distance to the line. It is the vertical difference between the observed value and the predicted value. This is because our model predicts the response (customers) and we compare that to the actual temperature for any point.\n\n\nDavid points to Figure 9.8 and says: “For the point at temperature = 62, the actual data had customers = 105, whereas our model predicts 108. So the model is off by -3 (if we compute the residual using this equation.” and he writes out Equation 9.3.\n\\[\n\\text{residual} = \\text{actual value} - \\text{model prediction}\n\\tag{9.3}\\]\nPeter continues: ““For the data point with temperature = 68, the model predicts 116, whereas the actual value was 132 for a residual of +16. Finally, for the point where the temperature is 76, the model prediction is 129, whereas the actual value was 130. The model did well on this point. In this way, we can compute the residual for every point and square it. We are not particularly concerned about the actual sign of the residual since we are going to square it anyway. This is exactly what Igor showed us in Table 9.2.”\n\n\n\n\n\n\nQuick Check\n\n\n\nIf a model predicts 120 customers for a given temperature, and the actual number is 132:\n\nWhat is the prediction error?\nIs it positive or negative?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe prediction error is 132 − 120 = 12.\nThe error is positive because the actual value is higher than the predicted value.\n\n\n\nAngela says: “If we draw any line through these points, each line will have low absolute residuals for some points and higher ones for others. But we have many points and so we want an overall measure of how close the line is to all the points by considering all the absolute residuals. Even if a line is spot-on for a few points, it is not very useful if it is wildly off for many others. That overall measure is in fact the total penalty from Table 9.2. It is very nice that we have seen the computations and also seen its visualization to make sure we understand all this. I sure am glad we are doing this as a team!”\nIgor says, “Yes, now I too see the connection between my table (Table 9.2) and Peter’s visualization of the concept. So, we will square the residuals from all the points and add them all up. That is how much total penalty the line/model has when we consider all data points. As Amanda wanted, squaring punishes bigger residuals much more than smaller ones. I have heard that this is a standard theme in statistics.”\n\n\n\n\n\n\nWhat’s the deal with squaring?\n\n\n\nWe square errors for the same reason we used squared deviations when defining variance:\n\npositive and negative errors should not cancel out\nlarger errors should count more than smaller ones\n\nLeast squares regression that we study in this course is built on the same logic as variance. In a later chapter, we will see this idea pop when we look at the explanatory power of a model. Hang tight till then!\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhy don’t we choose the line that minimizes the sum of errors instead of the sum of squared errors?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPositive and negative errors would cancel out if we summed errors directly.\nSquaring errors ensures that all errors contribute positively and that larger errors are penalized more heavily.\n\n\n\nClearly, we want a line that is as close as possible to all the points overall. Honestly, the line in Figure 9.8 seems to be pretty good, at least visually.\nDavid says: “For this model (a = 15, b = 1.5), the total of all the squared residuals is: 1,469. Is there a line that can give an even lower value? What are the values for a and b for which we get the lowest possible total of all the squared residuals?”\nThe team now understands exactly what they mean by the best model. They are looking for values for a and b that will result in the lowest total penalty. But they are stuck on to how to actually get it.\nCatching the thread about trying to generate a plot to identify the best values just like Suzie did in the first week, Angela returns to her idea. “Suzie, can you do you pull off your plotting magic again, please?”\nSuzie: “Well angela, that would be a three-dimensional plot because we will have a, b, and the total penalty all on the same plot. I can do it, but I am afraid it will not be much help.”\nIgor: “Wht not just print out a large table of all combinations of a and b and the total penalty for each combination? We can then look through the table and select the best value – or even ask the program to look through the table and find the values for us?”\nSuzie says “We can Igor, but I have two issues. Firstly, when I try the various combinations of a and b, I can only do it in some intervals like 0.1 or something like that. What if the best solution was between two of my values? We will then not get an exact solution, but we will get something very close. A bigger problem is what if Amanda comes up with two more variables to use in our predictions? Then trying all combinations can generate a very large number of rows, perhaps millions. There has to be a better way.”\nSteve, who has been silent for a long time has actually been having a nagging feeling that he has done something similar in one of his classes. He now says “I think we learned something like this in my econ class. Rather than making Suzie write a whole new program, I think there is a function in R, based in calculus that can readily do this for us! Let me try.”\nAnd he runs the code below: which generates the best values for a and b that guarantee that the sum of squared residuals is the smallest possible.\n\nkiosk |&gt; \n  model_train(customers ~ temperature) |&gt; \n  coef()\n\n(Intercept) temperature \n      22.06        1.42 \n\n\nSteve then explains: “Ah, now I remember what the output means. We see two values in the output: intercept and temperature. The first is a and the second is b! These are exactly the values of a and b that minimize the sum of squared residuals.”\nFrom the output, Steve then writes the model equation Equation 9.4, and says:\n“This equation shows the best model for our problem, given that we want a straight line and want to minimize the sum of squared residuals.”\n“Our econ teacher called this the least-squares regression line.”\n\\[\n\\widehat{\\text{customers}} = 22.06 +  1.42\\,\\text{temperature}\n\\tag{9.4}\\]\n\n\n\n\n\n\nKey Idea\n\n\n\nSince there is an infinite number of possible choices for a and b, we need a systematic way to find the values that minimize the total of the squared residuals. Mathematically, this problem can be solved using calculus. Practically, we sit back and let software do this for us.\n\n\n\n\n\n\n\n\nPause & Think\n\n\n\nComplete the sentence:\n“A least squares regression line is the line that _____.”\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nA least squares regression line is the line that minimizes the sum of squared vertical prediction errors, or the sum of squared residuals (same thing).\n\n\n\nIgor is at it again. He announces “The total penalty for this is 1388. This beats the 1469 we got earlier. And, according to Steve, this is the best. I am quite impressed that we had eyeballed a solution very close to the best!”\nThe team found a line that minimizes the sum of squared residuals. Technically this is called the least-squares criterion and plays a very important role in statistics and data analysis.\nThey explained the model to Amanda. After a lot of questioning to understand their decision criteria, Amanda agreed to use this model for the next month.\nAmanda was thrilled with their work. “Great work team. Talk to me when you graduate. I want to assemble a business analytics team for my businesses and I sure can do with a team like yours.”\n\n\n\n\n\n\nPause & Think\n\n\n\nSuppose tomorrow’s high temperature is 73°F. Your model predicts 124 customers.\n\nDoes this mean exactly 124 customers will arrive?\nWhat other factors (besides temperature) might cause the actual number to be higher or lower?\n\nWhat does this tell you about what a regression model can—and cannot—do?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\n\nNo. 124 is only a model prediction. In reality the number of customers may be 124 or higher or lower.\nThe model acts as if only the temperature determines the number of customers. In reality, many other factors play a role too. Perhaps there is a football game in town and that draws people away. Perhaps the humidity is unusually high and people stay away. Many factors play a role in this outcode. Our model uses a simple rule that can potentially perform decently without being accurate.\n\nIn later chapters we will see that we can include more variables in a model and improve its performance.",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line of Best Fit</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/line-of-best-fit-story.html#optional-enrichment-topic-beyond-straight-lines",
    "href": "modules/04-more-on-models/line-of-best-fit-story.html#optional-enrichment-topic-beyond-straight-lines",
    "title": "9  Line of Best Fit",
    "section": "9.4 Optional enrichment topic: Beyond straight lines",
    "text": "9.4 Optional enrichment topic: Beyond straight lines\nIn this chapter, we have assumed that a straight line is a reasonable model for the relationship between temperature and customer traffic. In many business settings, this assumption works well: simple models are easy to interpret, easy to communicate, and often good enough for decision-making.\nHowever, not all relationships are well captured by a straight line.\n\n9.4.1 When a straight line may not be appropriate\nIn some situations, a scatter plot may reveal patterns such as:\n\ncurvature (the relationship bends),\ndiminishing returns (the effect of increases slows down), or\nthreshold effects (behavior changes after a certain point).\n\nFor example, at very high temperatures, customer traffic at a beach kiosk might level off or even decline as conditions become uncomfortable. A straight line cannot capture this kind of behavior well, no matter how its slope and intercept are chosen.\nIn such cases, forcing a linear model may lead to systematic residual patterns, indicating that the model is missing something important.\n\n\n9.4.2 A flexible alternative: LOWESS smoothing\nOne way to explore nonlinear relationships is through a technique called LOWESS (short for locally weighted scatter plot smoothing).\nFigure 9.9 shows an example of a LOWESS model using the mpg data frame. In this data frame the displacement of an engine is related to the highway mileage hwy, but not in a linear way. The LOWESS model better describes it than a straight line would. Like the earlier line model, this too can take in a value for displ and provide an estimated value for hwy.\n\n\n\n\n\n\nFigure 9.9: LOWESS model for displacement vs cty in the mpg data frame showing how it captures the curvature in the relationship better than a lien can\n\n\n\nRather than fitting a single global rule like a straight line, LOWESS works by:\n\nfocusing on a small neighborhood of points around each value of the explanatory variable, and\nfitting simple local models that adapt to the data in that neighborhood.\n\nThe result is a smooth curve that follows the overall pattern of the data without requiring us to specify a particular functional form in advance.\nLOWESS is especially useful for:\n\nexploratory analysis,\nvisualizing trends, and\ndiagnosing whether a straight-line model is reasonable.\n\nIt is not intended to replace regression models in all cases, but rather to help us understand the structure of the data.\n\n\n9.4.3 Models as choices, not defaults\nThe key takeaway is not that linear models are “wrong,” but that models are choices.\n\nA straight line is a good choice when the relationship is approximately linear and interpretability matters.\nA nonlinear model may be a better choice when the data clearly suggest curvature or changing behavior.\nFlexible tools like LOWESS help us see what the data are trying to tell us before we commit to a specific model.\n\nIn all cases, the central modeling question remains the same:\n\nDoes this model capture the important structure in the data well enough to support the decisions we want to make?\n\nThis mindset—treating models as purposeful approximations rather than automatic formulas—will guide us throughout the rest of this book.",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line of Best Fit</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/visualizing-models.html",
    "href": "modules/04-more-on-models/visualizing-models.html",
    "title": "10  Visualizing models",
    "section": "",
    "text": "10.1 Recap of models\nIn this chapter, we will first revisit how to build visualize the two mean models we saw earlier in Chapter 5 and Chapter 6. To be sure, we already covered these in Chapter 8, but repeat it with different examples to provide reinforcement.\nWe will then learn how to visualize line models that we learned about in Chapter 9. This will set the stage for us to learn how to build these models and construct the models as equations.\nFrom prior chapters, we have learned that a model takes as input the values of the explanatory variable(s) (if any) and outputs an estimated value of the outcome variable.\nWe started off in Chapter 5 by showing that, in the absence of an explanatory variable, the mean is the best model. For example, our mpg data frame contains data about several cars. Let us suppose that our outcome variable is hwy, the highway mileage of a car.\nIf we are given only the data from that single column hwy, and nothing else, the best model we can come up with for estimating the highway mileage of a car is to always provide the average highway mileage from the data set.\nWe then considered, in Chapter 6, the case where we had a single explanatory variable, a categorical one and saw that here too the same idea applies, except that the best model for a category is the average for that category.\nWe then studied, in Chapter 9 the case of a single explanatory variable again, but this time the explanatory variable was numerical. We learned there about the least squares criterion to find the best model.\nChapter 8 showed us how to visualize the models we built around the idea of the mean as a model in Chapter 5 and @#sec-category-means. In this chapter, we use new examples to briefly review how to visualize those models and also compute and write out the model functions. We then move on to visualizing our line models from Chapter 9 and how to build and reconstruct the model function as an equation.\nWe will first learn how to visualize the models. We then look at how to generate the model function in equation form.",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing models</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/visualizing-models.html#visualizing-simple-mean-models-no-explanatory-variable",
    "href": "modules/04-more-on-models/visualizing-models.html#visualizing-simple-mean-models-no-explanatory-variable",
    "title": "10  Visualizing models",
    "section": "10.2 Visualizing simple mean models (no explanatory variable)",
    "text": "10.2 Visualizing simple mean models (no explanatory variable)\nWe take a different dataset this time. The data frame txhousing contains real estate data for the state of Texas for several years. We first follow the good practice of understanding the dataset before jumping in and building models. Table 10.1 shows the first 10 rows .\n\ntxhousing |&gt;\n    head(10) \n\n\n\n\nTable 10.1: First 10 rows from the txhousing data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nyear\nmonth\nsales\nvolume\nmedian\nlistings\ninventory\ndate\n\n\n\n\nAbilene\n2000\n1\n72\n5380000\n71400\n701\n6.3\n2000\n\n\nAbilene\n2000\n2\n98\n6505000\n58700\n746\n6.6\n2000\n\n\nAbilene\n2000\n3\n130\n9285000\n58100\n784\n6.8\n2000\n\n\nAbilene\n2000\n4\n98\n9730000\n68600\n785\n6.9\n2000\n\n\nAbilene\n2000\n5\n141\n10590000\n67300\n794\n6.8\n2000\n\n\nAbilene\n2000\n6\n156\n13910000\n66900\n780\n6.6\n2000\n\n\nAbilene\n2000\n7\n152\n12635000\n73500\n742\n6.2\n2000\n\n\nAbilene\n2000\n8\n131\n10710000\n75000\n765\n6.4\n2001\n\n\nAbilene\n2000\n9\n104\n7615000\n64500\n771\n6.5\n2001\n\n\nAbilene\n2000\n10\n101\n7040000\n59300\n764\n6.6\n2001\n\n\n\n\n\n\nWe see that for the first 10 rows, city and year are the same, but there is variation in the other variables. The dataset has 8602 observations or rows. Use the command ?txhousing in RStudio to answer the following questions.\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat is the unit of observation int his dataset? Unfortunately, the codebook does not give sufficient details. Se if you can use ChatGPT or other chatbot to find the answer.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nFor every city in Texas, the dataset has its real estate activity for one month. The data spans 12 months each from 2000 to 2014 and for 2015 7 months from Jan to July. There are 46 cities. For each city we have data for 180 + 7 = 187 months. Therefore the unit of observation is the real estate activity for a single city for a single month.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nExplain precisely what the first row tells us.\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe first row tells us that in Jan 2000, the city of Abilene had 72 sales, 701 listings, and so on.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhy does the data frame have 8602 rows?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\n187 months times 46 cities = 8602.\n\n\n\nIf you had access to only the column listings and we had to build a model to predict listings based on no other information, what would you predict?\n\ntxhousing|&gt;\n    point_plot(listings ~ 1, annot = \"model\", interval = \"none\")\n\nIn this example, our outcome variable is listings and we have no explanatory variable. Therefore we use “1” on the RHS of the tilde expression.\nThis code shows you how to add a model as an annotation. We had used “annot=” earlier for generating violin plots. Here we use:\nannot = “model”, interval = “none”\nThe first part asks the function to annotate the scatter plot with a model. For now, we will just use the second part “interval = …” without explanation. Later in the course, you will see why we use this.\n\n\n\n\n\n\nFigure 10.1: Scatter plot of listings from the txhousing data frame annotated with a model: the model is the blue horizontal line at the average value of listings (3217)\n\n\n\nLet us see one more example of visualizing a simple mean model.\nThe data frame Boston_marathon contains data about the results of the Boston Marathon over the years.\nLike we did with the txhousing data frame, we should first understand the data frame before processing it. Take a look at the codebook for the Boston_marathon data frame and answer the following questions. Try to recall how we got the codebook for txhousing and use the same approach here. The codebook is not too informative, but just viewing it using View(Boston_marathon) might tell you what you need to know.\n\n\n\n\n\n\nQuick Check\n\n\n\nHow many variables does the data frame have?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nSix\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat is the unit of observation? That is, what does a row tell us about?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nEach row tells us about the result of a year’s race for a gender. The unit of observation therefore is the result of a race in a year – with the races for males and females being considered as different races.\nWe have two rows for years starting from 1972 when women started participating. Prior to that, from 1897 to 1971, we have only one row per year.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nWhich column shows the finishing time?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe variable time shows the finishing time in seconds and the colvariableumn minutes shows it in minutes.\n\n\n\n\n\n\n\n\n\nQuick Check\n\n\n\nHow can you verify the suggested answer from the previous question?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nMultiply minutes by 60 and that should equal time. However, we cannot use the variable time as it is not numerical. (Take a close look.)\n\n\n\nLet us visualize a model for the variable minutes. As before, we generate the scatter plot and then annotate with a model. We changed the data frame and the tilde expression. Figure 10.2 shows the plot. The model line is at minutes = 144.\n\nBoston_marathon |&gt; \n  point_plot(minutes ~ 1, annot = \"model\",\n             interval = \"none\")\n\n\n\n\n\n\n\nFigure 10.2: Scatter plot of minutes from the Boston_marathon data frame annotated with a model",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing models</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/visualizing-models.html#visualizing-category-mean-models-single-explanatory-variable-that-is-categorical",
    "href": "modules/04-more-on-models/visualizing-models.html#visualizing-category-mean-models-single-explanatory-variable-that-is-categorical",
    "title": "10  Visualizing models",
    "section": "10.3 Visualizing category mean models (single explanatory variable that is categorical)",
    "text": "10.3 Visualizing category mean models (single explanatory variable that is categorical)\nWe built a model for minutes from the Boston_marathon data frame assuming no explanatory variable. What if we used sex as the explanatory variable? Figure 10.3 shows the model. This time we have a separate mean model for each sex.\n\nBoston_marathon |&gt; \n  point_plot(minutes ~ sex, annot = \"model\",\n             interval = \"none\")\n\n\n\n\n\n\n\nFigure 10.3: Boston marathon model for minutes with *sex8 as the explanatory variabl\n\n\n\nFor another example, we now use the mpg data frame and plot a model for the city. mileage (variable cty) with the kind of drive (variable drv) as the explanatory variable. Figure 10.4 shows the plot. This time we see that the model is the mean for each kind of drive.\n\nmpg |&gt; \n  point_plot(cty ~ drv, annot = \"model\", \n             interval = \"none\")\n\n\n\n\n\n\n\nFigure 10.4: Model of category means for cty ~ drv for the mpg data frame",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing models</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/visualizing-models.html#visualizing-least-squares-line-models-single-explanatory-variable-that-is-numeric",
    "href": "modules/04-more-on-models/visualizing-models.html#visualizing-least-squares-line-models-single-explanatory-variable-that-is-numeric",
    "title": "10  Visualizing models",
    "section": "10.4 Visualizing least-squares line models (single explanatory variable that is numeric)",
    "text": "10.4 Visualizing least-squares line models (single explanatory variable that is numeric)\nWe use the Anthro_F data frame. Take a look at its codebook and answer the following question.\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat is the unit observation?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nEach row gives us information about a single college woman in the age range 18-25. SO the unit of observation is a single college woman in the age range 18-25.\n\n\n\nWe would like to visualize a model that predicts the body fat (variable BFat) with Wrist as the explanatory variable. Figure 10.5 shows the scatter plot and the least squares line.\n\n Anthro_F |&gt; \n  point_plot(BFat ~ Wrist, annot = \"model\",\n             interval = \"none\")\n\n\n\n\n\n\n\nFigure 10.5: Least squares line model for the Anthro_f data frame: BFat as the response variable with Wrist as the explanatory variable\n\n\n\nAs we saw from Chapter 5, the best model would be to predict the average. Do you recall how to compute the average?\n\ntxhousing |&gt;\n    summarize(avg_listings = mean(listings))\n\n\n  \n\n\n\nWhy is the result not a number? Why did we get NA?\nYou might recall from Section 1.8 that NA means missing value. Why is the mean a missing value? The txhousing data frame has some missing values in the column listings. As a result, it is unable to compute the average. Suppose I asked you what is 2 + 3, you can say “5.” since you have both the numbers that you want to add up.\nHowever, if I asked you what is NA + 5? You cannot say what it is because you do not know the values of the two numbers you are trying to sum. In general, whenever NA is involved in an operation, the result is always NA.\nThis is very inconvenient. Wht does the system simply not ignore the NAs and compute the average of the remaining numbers? By default it does not do that, but we can make the system do that in the following way.\n\ntxhousing |&gt;\n    summarize(avg_listings = mean(listings, na.rm = TRUE))\n\n\n  \n\n\n\nWhen you add “na.rm = TRUE”, R leaves out the missing values and computes the average of the rest. Remember, R is case sensitive and so you have to type na.rm in lowercase and TRUE in uppercase. Otherwise you will get an error.\nSo we saw that the mean of listings is 3217 and that is our model value.\nWe can write the model function as Equation 11.1\n\\[\n\\widehat{\\text{listings}} = 3217\n\\tag{10.1}\\]\nWe had built the mean model by explicitly writing code to compute the average.\nIn the earlier chapters, we have built This can get cumbersome as we will need to build the model differently depending on the type of explanatory variable. Fortunately, the LSTbook package provides a single function – the model_train function through which we can build all the models for this course.\nLet us see how to use it.",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing models</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/finding-the-model-function.html",
    "href": "modules/04-more-on-models/finding-the-model-function.html",
    "title": "11  Finding the Model Function",
    "section": "",
    "text": "11.1 Finding the model function for models with no explanatory variables\nOur journey so far has introduced the idea of models and shown us the intuition, and rationale behind models. We have also seen how to visualize the models. However, we have not lingered too much on systematically building models and reconstructing the model function from what R shows us about the models.\nThis chapter closes the loop.\nWhen we have a numerical outcome variable and no explanatory variable, Chapter 5 showed us that the best model would be to predict the mean or average. We look at two ways of building such models:",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Finding the Model Function</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/finding-the-model-function.html#finding-the-model-function-for-models-with-no-explanatory-variables",
    "href": "modules/04-more-on-models/finding-the-model-function.html#finding-the-model-function-for-models-with-no-explanatory-variables",
    "title": "11  Finding the Model Function",
    "section": "",
    "text": "building the model by explicitly computing the mean\nusing the model_train function (our go to method from now on)\n\n\n11.1.1 Building mean models by explicitly computing the mean\nDo you remember how to compute the mean oif a variable?\n\n\n\n\n\n\nQuick Check\n\n\n\nGiven a hypothetical data frame named dat do you recall how to compute the mean of one of its variables named col using R?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\ndat |&gt; summarize(avg_col = mean(col))\n\n\n\nUsing the txhousing data frame, we can build a model for the number of listings as follows:\n\ntxhousing |&gt;\n    summarize(avg_listings = mean(listings))\n\n\n  \n\n\n\nWhy is the result not a number? Why did we get NA?\nYou might recall from Section 1.8 that NA means missing value. Why is the mean of listings a missing value? The txhousing data frame has some missing values in the column listings. As a result, our code is unable to compute the average. Suppose I asked you what is 2 + 3, you can say “5.” since you have both the numbers that you want to add up.\nHowever, if I asked you what is NA + 5? You cannot say what it is because you do not know the values of the two numbers you are trying to sum. In general, whenever NA is involved in an operation, the result is always NA.\nThis is very inconvenient. Wht does the system simply not ignore the NAs and compute the average of the remaining numbers? By default it does not do that, but we can make the system do that in the following way.\n\ntxhousing |&gt;\n    summarize(avg_listings = mean(listings, na.rm = TRUE))\n\n\n  \n\n\n\nWhen you add “na.rm = TRUE”, R leaves out the missing values and computes the average of the rest. Remember, R is case sensitive and so you have to type “na.rm” in lowercase and “TRUE” in uppercase. Otherwise you will get an error.\nSo we saw that the mean of listings is 3217 and that is our model value.\nWe can write the model function as Equation 11.1\n\\[\n\\widehat{\\text{listings}} = 3217\n\\tag{11.1}\\]\nWe have built the mean model by explicitly writing code to compute the average.\n\n\n\n\n\n\nQuick Check\n\n\n\nIn Chapter 5, we saw a situation where the interns had to find a single fixed number to represent the number of customers who will show up at a kiosk on any day. What could be a business analogue of that in the listings example from the txhousing data frame?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe analogue would be: We need to build a model for the number of listings in a typical month, and we were allowed to give just a single fixed number for it. Also, like before, bigger errors are penalized more strongly through squaring prediction errors.\n\n\n\n\n\n11.1.2 Building the mean model using model_train\nThe model_train function greatly simplifies the process of building models irrespective of the form of the model. We do not need to worry about what types of explanatory variables and how many of them are involved.\nLet us see how to use it to build the model for listings with no explanatory variables.\n\ntxhousing |&gt;\n    model_train(listings ~ 1)\n\nA trained model relating the response variable \"listings\"\nto explanatory variable \"\".\n\nTo see relevant details, use model_eval(), conf_interval(),\nR2(), regression_summary(), anova_summary(), or model_plot(),\nor the native R model-reporting functions.\n\n\nWe have built the model, but the output does not give us any details about the model to enable us to reconstruct the model function.\nWe can fix that easily by calling another function – coef – to get the model coefficients.\n\ntxhousing |&gt;\n    model_train(listings ~ 1) |&gt;\n    coef()\n\n(Intercept) \n       3217 \n\n\nThe output says that the intercept is 3217 (rounded). The model output does not show any other coefficients because we do not have any explanatory variables. From this, we can reconstruct the model function as Equation 11.2 (same as Figure 10.1). We explain below exactly how we went from the coefficients in the output to Equation 11.2.\n\\[\n\\widehat{\\text{listings}} = 3217\n\\tag{11.2}\\]\nYou night recall from Chapter 9 that in general when we have a single explanatory variable, we are trying to find the best values for a and b in an equation of the from that Equation 11.3 provides to compute an estimated value for the outcome variable (hence the hat)\n\\[\n\\widehat{\\text{outcome}} = a + b\\,\\,\\text{explanatory\\_var}\n\\tag{11.3}\\]\nThe intercept in the model coefficients output by the coef function corresponds to a. If there is another coefficient in the output that corresponds to b. In the current example we have only the intercept and no other coefficient and so we use only a in reconstructing the model function as Equation 11.2.\nLet us consider another example using the mpg data frame. We will now build a model for cty with no explanatory variables.\n\nmpg |&gt; \n  model_train(cty ~ 1) |&gt; \n  coef()\n\n(Intercept) \n       16.9 \n\n\nWe see that the intercept is 16.86 (rounded). This is, of course, the mean of cty. Equation 11.4 shows the model function.\n\\[\n\\widehat{\\text{cty}} = 16.9\n\\tag{11.4}\\]\n\n\n\n\n\n\nQuick Check\n\n\n\nCan you think of a business scenario where this sort of model might be used?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIf a company had a fleet of cars and needed to estimate something based on the city mileage of the fleet as a whole and we had to settle on juse a single representative number for the whole fleet, then the average would be the best – assuming again that bigger errors are more heavily penalized.\n\n\n\nLet us close out with another example from the Anthro_F data frame to predict body fat (variable *BFat) with no explanatory variables.\n\nAnthro_F |&gt; \n  model_train(BFat ~ 1) |&gt; \n  coef()\n\n(Intercept) \n       21.8 \n\n\nThis shows an intercept of 21.8 (rounded). Equation 11.5 shows the model function.\n\\[\n\\widehat{\\text{BFat}} = 21.8\n\\tag{11.5}\\]\n\n\n\n\n\n\nQuick Check\n\n\n\nAs before, what might be a usage scenario for this model?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIf, for some reason we needed a representative number – a single number – for the body fat of a group of college age females in the age range 18-25 (as the data set on which we based the model), then this model would work.\n\n\n\n\n\n\n\n\n\nSteps to build the model function for models with no explanatory variable\n\n\n\n\nDetermine the tilde expression for your model (like listings ~ 1, or some such)\nPipe the data frame to the model_train function and pass the tilde expression as well\nPipe the output of the model_train function to the coef() function\nLook at the results and map the intercept to a\nConstruct the model function by substituting the name of the outcome variable and the value of a in the equation with the intercept: \\[\n\\widehat{\\text{outcome\\_var}} = \\text{a}\n\\]",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Finding the Model Function</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/finding-the-model-function.html#finding-the-model-function-for-models-with-one-explanatory-variable-alone",
    "href": "modules/04-more-on-models/finding-the-model-function.html#finding-the-model-function-for-models-with-one-explanatory-variable-alone",
    "title": "11  Finding the Model Function",
    "section": "11.2 Finding the model function for models with one explanatory variable alone",
    "text": "11.2 Finding the model function for models with one explanatory variable alone\nMirroring the process from the previous section, we have two ways of building these models:\n\nbuilding the model by explicitly computing the category means\nusing the model_train function (our go to method from now on)\n\n\n11.2.1 Building category mean models by explicitly computing category means\nLet us use the Boston_marathon data frame to build a model for minutes with sex as the explanatory variable.\n\nBoston_marathon |&gt; \n  summarize(avg_minutes = mean(minutes), .by = sex)\n\n\n  \n\n\n\nIn the above code, if we had not used “.by = sex”, we would have got the overall mean. But using “.by = sex” computes a separate mean for each value of sex.\nWe now have the mean of minutes for each sex and can build the model function as:\n\\[\n\\widehat{\\text{minutes}} =\n\\begin{cases}\n142 & \\text{if } \\text{sex} = \\text{\"male\"} \\\\\n150 & \\text{if } \\text{sex} = \\text{\"female\"}\n\\end{cases}\n\\tag{11.6}\\]\nAs before, this is needlessly cumbersome. We can do this using the model_train function like before.\n\nBoston_marathon |&gt; \n  model_train(minutes ~ sex) |&gt; \n  coef()\n\n(Intercept)     sexmale \n     149.87       -8.33 \n\n\nThe output looks a bit different now. We have the intercept as before. But we have a single coefficient named sexmale.\nWhen we have categorical explanatory variables, the model_train function reports coefficients a bit differently than what we did earlier.\nOur explanatory variable has two possible values, female and male. The intercept represents the model value for one of these – typically, the alphabetically lowest one – in this case female as f comes alphabetically before m.\nSo the coefficient 149.86, which we wil round to 10 is the model value for female as our earlier computations also showed. This is the base value. The coefficient for sexmale shows the coefficient value for male relative to that for the base. This means that the coefficient for male is less (because it is negative) than that for female by 8.33. The coefficient for male is approximately 142.\nSo we can construct the model function as:\n\\[\n\\widehat{\\text{minutes}} =\n\\begin{cases}\n142 & \\text{if } \\text{sex} = \\text{\"female\"} \\\\\n142 & - 8\\,\\, \\text{if } \\text{sex} = \\text{\"male\"}\n\\end{cases}\n\\tag{11.7}\\]\nEquation 11.7 is effectively the same as Equation 11.6, but written slightly differently.\nYou should read it as, if sex is “female” then the model value is 150. If sex is “male” then the model value is 8 minutes lower (negative sign).\nDoes this make sense? It does, because we know that males run slightly faster than females and so their finishing time will be lower.\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat would be the use case scenario for this model?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nIf the race organizers are planning to do something based on the typical race completion time and can do something different for male and female participants, then the average finishing times for each gender would be a good model.\n\n\n\nLet us consider one more example using the acct_type_balance data frame with balance as the outcome variable and bank_account_type as the explanatory variable.\n\nacct_type_balance |&gt; \n  model_train(balance ~ bank_account_type) |&gt; \n  coef()\n\n             (Intercept) bank_account_typeSavings \n                    4901                     1016 \n\n\nThe output shows the intercept as 4901. Our explanatory variable bank_account_type has two possible values Savings and Checking. From the previous example, we know that the coef() function will treat one of these as the base.\n\n\n\n\n\n\nQuick Check\n\n\n\nWHich account tyoe will it treat as the base? Checking or Savings?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\n*Checking** because “C” is alphabetically before “S”.\n\n\n\nChecking is the base and its average is reported as the intercept and the model value for Savings is reported relative to the model value for Checking.\nThe model function therefore becomes:\n\\[\n\\widehat{\\text{balance}} =\n\\begin{cases}\n4901 & \\text{if } \\text{bank\\_account\\_type} = \\text{\"Checking\"} \\\\\n4901 & + 1016\\,\\, \\text{if } \\text{bank\\_account\\_type} = \\text{\"Savings\"}\n\\end{cases}\n\\tag{11.8}\\]\nThe model says that savings accounts generally have a higher account balance than checking accounts. Makes sense.\n\n\n\n\n\n\nSteps to build the model function for models with one categorical explanatory variable\n\n\n\n\nDetermine the tilde expression for your model (like balance ~ bank_account_type, or some such)\nPipe the data frame to the model_train function and pass the tilde expression as well\nPipe the output of the model_train function to the coef() function\nFind which category has been used as the base and note the intercept\nNote the coefficients for each of the other categories\nConstruct the model function by substituting the name of the outcome variable, and the value of the intercept in the following equation.\nThen add conditions for each additional category suitably substituting the appropriate category names. \\[\n\\widehat{\\text{outcome}} =\n\\begin{cases}\nintercept & \\text{if } \\text{explanatory variable} = \\text{base category} \\\\\nintercept & + \\,\\text{coef1} \\, \\text{if } \\text{explanatory variable} = \\text{\"category 1\"} \\\\\nintercept & +\\, \\text{coef2} \\, \\text{if } \\text{explanatory variable} = \\text{\"category 2\"} \\\\\nintercept & +\\, \\text{coef3} \\, \\text{if } \\text{explanatory variable} = \\text{\"category 3\"}\n\\end{cases}\n\\]",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Finding the Model Function</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/finding-the-model-function.html#finding-the-model-function-for-models-with-a-single-numerical-explanatory-variable",
    "href": "modules/04-more-on-models/finding-the-model-function.html#finding-the-model-function-for-models-with-a-single-numerical-explanatory-variable",
    "title": "11  Finding the Model Function",
    "section": "11.3 Finding the model function for models with a single numerical explanatory variable",
    "text": "11.3 Finding the model function for models with a single numerical explanatory variable\nUnlike the two mean models, we have no direct computation approach. We will just use the model_train function.\nWe will build a model using the mpg data frame with the highway mileage (variable hwy) as the outcome variable and the engine displacement (displ) as the explanatory variable.\n\nmpg |&gt; \n  model_train(hwy ~ displ) |&gt; \n  coef()\n\n(Intercept)       displ \n      35.70       -3.53 \n\n\nAs before, we substitute the “intercept* for a and in the case of a numerical explanatory variable, we substitute the other coefficient for b.\nEquation 11.9 shows the model function.\n\\[\n\\widehat{\\text{hwy}} = 36 - 3.5\\, \\text{displ}\n\\tag{11.9}\\]\nLet us see another example. We use the Hill_racing data frame to build a model with time (finishing time measured in seconds) as the outcome variable, and)distance (measured in km) as the explanatory variable.\n\nHill_racing |&gt; \n  model_train(time ~ distance) |&gt; \n  coef()\n\n(Intercept)    distance \n       -211         381 \n\n\nWe see that the intercept is -211. The coefficient for distance is 381.\nLet us build the model function before talking about the crazy-seeming negative intercept.\n\\[\n\\widehat{\\text{time}} = -211 + 381\\, \\text{distance}\n\\tag{11.10}\\]\nDoes it make sense that the coefficient for distance is positive? Per Equation 11.10, as distance increases, the estimated time will increase. This makes sense.\nWhat about the intercept of -211?\nFrom Equation 11.10, we see that if there is a hypothetical race with distance of 0 km, then the model says that people will finish the race 211 seconds before the race starts!\nObviously that makes no sense as time works in the normal world. SO what is going on?\nModels are approximate and we can only expect reasonable results within the range of data on which they were built. We need to be wary of interpreting models very outside of their scope. In our dataset, the shortest race is 1.1 km and the average is 10.7 km.\nFor the 1.,1 km race, the model predicts a time of 208 seconds or around 3.5 minutes, which seems reasonable – given that these are “hill” races.\n\n\n\n\n\n\nQuick Check\n\n\n\nWhat would be a scenario where this model can help?\n\n\n\n\n\n\n\n\nSuggested answer\n\n\n\n\n\nThe organizers are getting ready to conduct the races again. Now, they want to give an estimated completion time (one single number) for each race. They can use this model to find this time based on the distance of each race.\n\n\n\n\n\n\n\n\n\nSteps to build the model function for models with one numerical explanatory variable\n\n\n\n\nDetermine the tilde expression for your model (like hwy ~ displ, or some such)\nPipe the data frame to the model_train function and pass the tilde expression as well\nPipe the output of the model_train function to the coef() function\nNote the intercept and the coefficient for the explanatory variable (expl_coeff)\nConstruct the model function by substituting the name of the outcome variable, and the value of the intercept in the following equation.\nThen add conditions for each additional category suitably substituting the appropriate category names. \\[\n\\widehat{\\text{outcome}} = \\text{intercept} + \\text{expl\\_var\\_coeff}\\,\\,\\text{explanatory variable}\n\\]",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Finding the Model Function</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/model-with-cat-and-num-explanatories-story.html",
    "href": "modules/04-more-on-models/model-with-cat-and-num-explanatories-story.html",
    "title": "12  Amanda Strikes Again",
    "section": "",
    "text": "In prior chapters we have looked at three different situations in the specific context of Amanda’s kiosks. Table 12.1 shows the scenarios and the models we found for each.\n\n\n\nTable 12.1: Model types we have looked at so far for the kiosk scenarios\n\n\n\n\n\n\n\n\n\n\nResponse variable\nExplanatory var\nBest model\n\n\n\n\nNum customers, one for both kiosks\nNone\nMean customers\n\n\nNum customers, one per kiosk\nKiosk location\nMean for each kiosk\n\n\nNumber of customers\nTemperature\nLeast squares line\n\n\n\n\n\n\nTable 12.2 summarizes the situations in more generic terms than the specific kiosk context and the best model for each. (Recall that in this book we consider only numerical response variables.)\n\n\n\nTable 12.2: Model types we have looked at so far\n\n\n\n\n\n\n\n\n\n\nResponse variable type\nExplanatory variable type\nBest model\n\n\n\n\nNumerical\nNone\nMean\n\n\nNumerical\nCategorical\nCategory means\n\n\nNumerical\nNumerical\nLeast squares line\n\n\n\n\n\n\nIn our running story, this is the fourth meeting of our interns with Amanda. Amanda owns a business that operates kiosks in a city at two locations.\n“Team, our beach kiosk performed really well last month. Using the temperature to estimate the number of customers really helped us stay efficient. We hit our lowest food waste thus far, and run out of stock by a lot either, at least compared to past months. Your models are helping.”\nThe interns feel very happy to hear this.\nAmanda continues. “I now have more data and have reason to believe that the number of customers who turn up at the mall kiosk also seems to be related to the temperature, but not quite in the same way as the beach kiosk. I am not able to put my finger on it, but something tells me that we can do better with a model that considers both temperature and the kiosk location.”\nShe then shared the data frame kiosk_beach_mall_temp with the interns.\nShe reiterates her earlier constraint:\n“Keep it simple. I need something I can explain in one sentence.”\nShe emails the team the data and heads out, wondering if the team will pull off something good once more. While exuding confidence externally, internally she has a nagging doubt that she might be stretching this young team of undergrads too much.\nHer confidence energizes the team and they all think through this in silebnce for a few minutes.\nSuzie is the first to speak up. “When we gave one number per kiosk, our model looked like this.” and she wrote out Equation 12.1\n\\[\n\\widehat{\\text{num customers}} =\n\\begin{cases}\n101 & \\text{if } \\text{kiosk} = \\text{\"Mall\"} \\\\\n146 & \\text{if } \\text{kiosk} = \\text{\"Beach\"}\n\\end{cases}\n\\tag{12.1}\\]\nWhen we predicted the number of customers based on the temperature for the beach kiosk alone our model became this” and she wrote out Equation 12.2. “How do we continue from here?”\n\\[\n\\widehat{\\text{customers}} = 22.06 +  1.42\\,\\text{temperature}\n\\tag{12.2}\\]\nEveryone sat back, looking intently at the two simple equations.\nThen David spoke up. “Can we think of some combination of the two? In the case of the temperature model, we had a general model like this in which we found the best values for a and b.” and he writes out Equation 12.3.\n\\[\n\\widehat{\\text{customers}} = a +  b\\,\\text{temperature}\n\\tag{12.3}\\]\nDavid then continues: “Why not just add another variable c to this?” And he wrote out Equation 12.4\n\\[\n\\widehat{\\text{customers}} = a +  b\\,\\text{temperature} + c\\,\\text{kiosk\\_location}\n\\tag{12.4}\\]\n“Now we just have to find the best values for a, b, and c. That’s all!”\n“Terrific!” Angela says. “Looks like we are done!” People nod in agreement.\n“Not so fast Angela.” Igor says, and everyone seems mildly irritated as if he is just being picky.\nBut, Igor continues “Kiosk location is either beach or mall. How do we multiply that by the value of c?”\nThey fall into a deep silence again. They felt that Amanda sure gave them a tough nut to crack this time.\nSuzie once again tries to connect the old model to the new one. She says “Igor is right in that we cannot multiply c by the kiosk location. But can we not something like we did earlier with if the kiosk is beach then something, otherwise something sort of approach?” and she points to Equation 12.1.\nLong silence once again. Then Igor speaks out. “Perhaps our model should look something like this.”\n\\[\n\\widehat{\\text{customers}}\n= a + b\\,\\text{temperature}\n+ \\begin{cases}\nc_{\\text{Mall}} & \\text{if } \\text{kiosk}=\\text{\"Mall\"} \\\\\nc_{\\text{Beach}} & \\text{if } \\text{kiosk}=\\text{\"Beach\"}\n\\end{cases}\n\\]\nTime was running out. Amanda would be back any time now.\nDavid spoke up finally “Peter, last time you wrote some code to get the model. Why not just try that again? Perhaps it can already do what we want.”\nJust then Amanda comes back. The team was quite embarrassed that they did not have a solution for her. However, she senses the mood in the room and says “It looks like you have worked hard, but have not hit upon any solution yet.”\nPeter replies “We have made some progress, but are not quite there yet. Can you give us a little more time?” He then laid out exactly where they stood.\nAmanda senses their disappointment and immediately says: “You seem to be on a good path here. I have a sense that you can pull this off. Let us meet in a few days. Good luck!”\nOver the next day, some read up on statistics. Others were all over chatbots and Igor consulted his professor.\nThey met briefly in the university cafeteria over lunch and agreed to meet as a team later that day to see where they stood.\nAt the meeting, Peter started “We were on the right track.”\nIgor agreed. I spoke to Dr. Berlisky from the statistics department and he said that this problem actually involves a somewhat more advanced concept called interactions effects, However, he suggested a simple idea. Just separate the data for thw two kiosks and build separate models for each! I wonder why we did not think of that. It might be more complex than Amanda would want. But from what the prof said, I think we have no choice.”\nThe team agreed.\nPeter wrote up the code once again to first visualize both models.\n\nkiosk_beach_mall_temp |&gt; \n  point_plot(customers ~ temperature + kiosk,\n             annot = \"model\",\n             interval = \"none\")\n\n\n\n\n\n\n\nFigure 12.1: Separate least-square lines for the two kiosks showing that the beach kiosk is more sensitive to temperature\n\n\n\nDavid says “Amanda’s instinct that the two kiosks relate duqite differently seems correct. The beach kiosk seems much more sensitive to temperature.”\nSteve says “Yeah. At lower temperatures it has higher sales, but when it gets warmer, more people seem to want to hist the beach. Makes perfect sense to me!”\nThe team feels happy that they can explain the common sense behind the model.\nPeter then writes the code to get separate models for the two kiosks. First for the Mall kiosk.\n\nkiosk_beach_mall_temp |&gt; \n  filter(kiosk == \"Mall\") |&gt; \n  model_train(customers ~ temperature) |&gt; \n  coef()\n\nFrom the output, they reconstructed the model for the Mall kiosk as:\n\\[\n\\widehat{\\text{customers}} = 72 +  0.7\\,\\text{temperature}\n\\tag{12.5}\\]\n\nkiosk_beach_mall_temp |&gt; \n  filter(kiosk == \"Beach\") |&gt; \n  model_train(customers ~ temperature) |&gt; \n  coef()\n\nFrom the output, they reconstructed the model for the Mall kiosk as:\n\\[\n\\widehat{\\text{customers}} = 22 +  1.4\\,\\text{temperature}\n\\tag{12.6}\\]\nFrom Figure 12.1, it seemed clear that the two kiosks were quite different in how they related tgo temperature and therefore two separate models seemed to make sense.\nLater that week, they met with Amanda again, a little fearful that they did not stick to her “simple” requirement and were giving two different models. They told her honestly that there was a more advanced approach that would fit both into a single model, but were not sufficiently knowledgeable to present that approach, although the results would be the same as with two separate models.\nFortunately, Amanda was happy with their model. She says “Although I had asked for something simple, I guess when we want the model to be more and more specific, we have to tolerate some complexity after all. I am fine with this.”\nThe team resolved to learn more about the interaction effects that Dr. Berlinsky had spoken about and present that to Amanda when possible.",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Amanda Strikes Again</span>"
    ]
  },
  {
    "objectID": "modules/06-variance-analysis/06-overview.html",
    "href": "modules/06-variance-analysis/06-overview.html",
    "title": "Module 6: Explanatory Power of a Model",
    "section": "",
    "text": "Learning Goals\nWe turn our attention now to measuring the quality of a model based on the notion of a model’s ability to explain the variability in the response variable.",
    "crumbs": [
      "Explanatory Power of a Model",
      "Module 6: Explanatory Power of a Model"
    ]
  },
  {
    "objectID": "modules/06-variance-analysis/06-overview.html#learning-goals",
    "href": "modules/06-variance-analysis/06-overview.html#learning-goals",
    "title": "Module 6: Explanatory Power of a Model",
    "section": "",
    "text": "a\nb\nc",
    "crumbs": [
      "Explanatory Power of a Model",
      "Module 6: Explanatory Power of a Model"
    ]
  },
  {
    "objectID": "modules/06-variance-analysis/06-overview.html#structure-of-this-module",
    "href": "modules/06-variance-analysis/06-overview.html#structure-of-this-module",
    "title": "Module 6: Explanatory Power of a Model",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nWhat does it mean for a model to explain variability in the response variable?\nComputing the model values and residuals\nthe variance equation\nR-squared",
    "crumbs": [
      "Explanatory Power of a Model",
      "Module 6: Explanatory Power of a Model"
    ]
  },
  {
    "objectID": "modules/07-populations-samples/07-overview.html",
    "href": "modules/07-populations-samples/07-overview.html",
    "title": "Module 7: Population, Samples and Inference",
    "section": "",
    "text": "Learning Goals\nAt this point, we have learned about models in general, and linear regression models in particular. In addition, we can also now use R to build linear models and to construct the model function based on the output of the R code. We have also looked at the quality of a model based on its R-squared.\nUntil now we have looked at a mdoel as a mathematical function into which we can plug in the value of explanatory variable(s) and get an estimated value for the response variable.\nWe now transition from mathematical functions to statistical models. We first define the terms between population and sample and reveal that much of statistics deals with inference, that is, learning something about the population based on just a sample.",
    "crumbs": [
      "Populations, Samples and Inference",
      "Module 7: Population, Samples and Inference"
    ]
  },
  {
    "objectID": "modules/07-populations-samples/07-overview.html#learning-goals",
    "href": "modules/07-populations-samples/07-overview.html#learning-goals",
    "title": "Module 7: Population, Samples and Inference",
    "section": "",
    "text": "a\nb\nc",
    "crumbs": [
      "Populations, Samples and Inference",
      "Module 7: Population, Samples and Inference"
    ]
  },
  {
    "objectID": "modules/07-populations-samples/07-overview.html#structure-of-this-module",
    "href": "modules/07-populations-samples/07-overview.html#structure-of-this-module",
    "title": "Module 7: Population, Samples and Inference",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nPopulation and sample\nStatistc",
    "crumbs": [
      "Populations, Samples and Inference",
      "Module 7: Population, Samples and Inference"
    ]
  },
  {
    "objectID": "modules/07-populations-samples/pop-vs-sample.html",
    "href": "modules/07-populations-samples/pop-vs-sample.html",
    "title": "13  Population vs Sample",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Populations, Samples and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Population vs Sample</span>"
    ]
  },
  {
    "objectID": "modules/07-populations-samples/sampling-variability.html",
    "href": "modules/07-populations-samples/sampling-variability.html",
    "title": "14  Sampling Variability",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Populations, Samples and Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling Variability</span>"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/08-overview.html",
    "href": "modules/08-probability-variation/08-overview.html",
    "title": "Module 8: Probability as Variation",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Probability",
      "Module 8: Probability as Variation"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/08-overview.html#learning-goals",
    "href": "modules/08-probability-variation/08-overview.html#learning-goals",
    "title": "Module 8: Probability as Variation",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Probability",
      "Module 8: Probability as Variation"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/08-overview.html#structure-of-this-module",
    "href": "modules/08-probability-variation/08-overview.html#structure-of-this-module",
    "title": "Module 8: Probability as Variation",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Probability",
      "Module 8: Probability as Variation"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/randomness.html",
    "href": "modules/08-probability-variation/randomness.html",
    "title": "15  Randomness",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Randomness</span>"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/variability.html",
    "href": "modules/08-probability-variation/variability.html",
    "title": "16  Variability",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Variability</span>"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/distributions.html",
    "href": "modules/08-probability-variation/distributions.html",
    "title": "17  Distributions",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "modules/09-sampling-variation/09-overview.html",
    "href": "modules/09-sampling-variation/09-overview.html",
    "title": "Module 9: Sampling Variation and Estimators",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Sampling Variation and Estimators",
      "Module 9: Sampling Variation and Estimators"
    ]
  },
  {
    "objectID": "modules/09-sampling-variation/09-overview.html#learning-goals",
    "href": "modules/09-sampling-variation/09-overview.html#learning-goals",
    "title": "Module 9: Sampling Variation and Estimators",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Sampling Variation and Estimators",
      "Module 9: Sampling Variation and Estimators"
    ]
  },
  {
    "objectID": "modules/09-sampling-variation/09-overview.html#structure-of-this-module",
    "href": "modules/09-sampling-variation/09-overview.html#structure-of-this-module",
    "title": "Module 9: Sampling Variation and Estimators",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Sampling Variation and Estimators",
      "Module 9: Sampling Variation and Estimators"
    ]
  },
  {
    "objectID": "modules/09-sampling-variation/se-estimators.html",
    "href": "modules/09-sampling-variation/se-estimators.html",
    "title": "18  SE Estimators",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Sampling Variation and Estimators",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>SE Estimators</span>"
    ]
  },
  {
    "objectID": "modules/09-sampling-variation/sampling-dist-slopes.html",
    "href": "modules/09-sampling-variation/sampling-dist-slopes.html",
    "title": "19  Sampling Distributions Slopes",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Sampling Variation and Estimators",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling Distributions Slopes</span>"
    ]
  },
  {
    "objectID": "modules/10-confidence-bands/10-overview.html",
    "href": "modules/10-confidence-bands/10-overview.html",
    "title": "Module 10: Confidence Intervals and Bands",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Confidence Intervals and Bands",
      "Module 10: Confidence Intervals and Bands"
    ]
  },
  {
    "objectID": "modules/10-confidence-bands/10-overview.html#learning-goals",
    "href": "modules/10-confidence-bands/10-overview.html#learning-goals",
    "title": "Module 10: Confidence Intervals and Bands",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Confidence Intervals and Bands",
      "Module 10: Confidence Intervals and Bands"
    ]
  },
  {
    "objectID": "modules/10-confidence-bands/10-overview.html#structure-of-this-module",
    "href": "modules/10-confidence-bands/10-overview.html#structure-of-this-module",
    "title": "Module 10: Confidence Intervals and Bands",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Confidence Intervals and Bands",
      "Module 10: Confidence Intervals and Bands"
    ]
  },
  {
    "objectID": "modules/10-confidence-bands/ci-slope.html",
    "href": "modules/10-confidence-bands/ci-slope.html",
    "title": "20  CI Slope",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Confidence Intervals and Bands",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>CI Slope</span>"
    ]
  },
  {
    "objectID": "modules/10-confidence-bands/ci-bands.html",
    "href": "modules/10-confidence-bands/ci-bands.html",
    "title": "21  CI Bands",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Confidence Intervals and Bands",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>CI Bands</span>"
    ]
  },
  {
    "objectID": "modules/11-signal-noise/11-overview.html",
    "href": "modules/11-signal-noise/11-overview.html",
    "title": "Module 11: Signal and Noise",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Signal, Noise, and R-Squared",
      "Module 11: Signal and Noise"
    ]
  },
  {
    "objectID": "modules/11-signal-noise/11-overview.html#learning-goals",
    "href": "modules/11-signal-noise/11-overview.html#learning-goals",
    "title": "Module 11: Signal and Noise",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Signal, Noise, and R-Squared",
      "Module 11: Signal and Noise"
    ]
  },
  {
    "objectID": "modules/11-signal-noise/11-overview.html#structure-of-this-module",
    "href": "modules/11-signal-noise/11-overview.html#structure-of-this-module",
    "title": "Module 11: Signal and Noise",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Signal, Noise, and R-Squared",
      "Module 11: Signal and Noise"
    ]
  },
  {
    "objectID": "modules/11-signal-noise/variance-decomposition.html",
    "href": "modules/11-signal-noise/variance-decomposition.html",
    "title": "22  Variance Decomposition",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Signal, Noise, and R-Squared",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Variance Decomposition</span>"
    ]
  },
  {
    "objectID": "modules/11-signal-noise/r-squared.html",
    "href": "modules/11-signal-noise/r-squared.html",
    "title": "23  R-Squared",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Signal, Noise, and R-Squared",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>R-Squared</span>"
    ]
  },
  {
    "objectID": "modules/12-hypothesis-testing/12-overview.html",
    "href": "modules/12-hypothesis-testing/12-overview.html",
    "title": "Module 12: Hypothesis Testing",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Hypothesis Testing via Regression",
      "Module 12: Hypothesis Testing"
    ]
  },
  {
    "objectID": "modules/12-hypothesis-testing/12-overview.html#learning-goals",
    "href": "modules/12-hypothesis-testing/12-overview.html#learning-goals",
    "title": "Module 12: Hypothesis Testing",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Hypothesis Testing via Regression",
      "Module 12: Hypothesis Testing"
    ]
  },
  {
    "objectID": "modules/12-hypothesis-testing/12-overview.html#structure-of-this-module",
    "href": "modules/12-hypothesis-testing/12-overview.html#structure-of-this-module",
    "title": "Module 12: Hypothesis Testing",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Hypothesis Testing via Regression",
      "Module 12: Hypothesis Testing"
    ]
  },
  {
    "objectID": "modules/12-hypothesis-testing/t-tests-regression.html",
    "href": "modules/12-hypothesis-testing/t-tests-regression.html",
    "title": "24  t-tests for Regression",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Hypothesis Testing via Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>t-tests for Regression</span>"
    ]
  },
  {
    "objectID": "modules/12-hypothesis-testing/interpretation.html",
    "href": "modules/12-hypothesis-testing/interpretation.html",
    "title": "25  Interpretation",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Hypothesis Testing via Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interpretation</span>"
    ]
  },
  {
    "objectID": "modules/13-business-applications/13-overview.html",
    "href": "modules/13-business-applications/13-overview.html",
    "title": "Module 13: Business Applications",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Business Applications of Regression",
      "Module 13: Business Applications"
    ]
  },
  {
    "objectID": "modules/13-business-applications/13-overview.html#learning-goals",
    "href": "modules/13-business-applications/13-overview.html#learning-goals",
    "title": "Module 13: Business Applications",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Business Applications of Regression",
      "Module 13: Business Applications"
    ]
  },
  {
    "objectID": "modules/13-business-applications/13-overview.html#structure-of-this-module",
    "href": "modules/13-business-applications/13-overview.html#structure-of-this-module",
    "title": "Module 13: Business Applications",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Business Applications of Regression",
      "Module 13: Business Applications"
    ]
  },
  {
    "objectID": "modules/13-business-applications/marketing.html",
    "href": "modules/13-business-applications/marketing.html",
    "title": "26  Marketing",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Business Applications of Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Marketing</span>"
    ]
  },
  {
    "objectID": "modules/13-business-applications/finance.html",
    "href": "modules/13-business-applications/finance.html",
    "title": "27  Finance",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Business Applications of Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Finance</span>"
    ]
  },
  {
    "objectID": "modules/13-business-applications/hr.html",
    "href": "modules/13-business-applications/hr.html",
    "title": "28  Human Resources",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Business Applications of Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Human Resources</span>"
    ]
  },
  {
    "objectID": "modules/13-business-applications/operations.html",
    "href": "modules/13-business-applications/operations.html",
    "title": "29  Operations",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Business Applications of Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Operations</span>"
    ]
  },
  {
    "objectID": "modules/14-projects/14-overview.html",
    "href": "modules/14-projects/14-overview.html",
    "title": "Module 14: Projects",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Projects and Datasets",
      "Module 14: Projects"
    ]
  },
  {
    "objectID": "modules/14-projects/14-overview.html#learning-goals",
    "href": "modules/14-projects/14-overview.html#learning-goals",
    "title": "Module 14: Projects",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Projects and Datasets",
      "Module 14: Projects"
    ]
  },
  {
    "objectID": "modules/14-projects/14-overview.html#structure-of-this-module",
    "href": "modules/14-projects/14-overview.html#structure-of-this-module",
    "title": "Module 14: Projects",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Projects and Datasets",
      "Module 14: Projects"
    ]
  },
  {
    "objectID": "modules/14-projects/project-guidelines.html",
    "href": "modules/14-projects/project-guidelines.html",
    "title": "30  Project Guidelines",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Projects and Datasets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Project Guidelines</span>"
    ]
  },
  {
    "objectID": "modules/14-projects/datasets.html",
    "href": "modules/14-projects/datasets.html",
    "title": "31  Datasets",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Projects and Datasets",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "modules/Appendices/01-overview.html",
    "href": "modules/Appendices/01-overview.html",
    "title": "32  Appendices",
    "section": "",
    "text": "32.1 Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "modules/Appendices/01-overview.html#learning-goals",
    "href": "modules/Appendices/01-overview.html#learning-goals",
    "title": "32  Appendices",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "modules/Appendices/01-overview.html#structure-of-this-module",
    "href": "modules/Appendices/01-overview.html#structure-of-this-module",
    "title": "32  Appendices",
    "section": "32.2 Structure of This Module",
    "text": "32.2 Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Appendices</span>"
    ]
  }
]