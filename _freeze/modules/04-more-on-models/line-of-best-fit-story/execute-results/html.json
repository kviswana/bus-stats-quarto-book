{
  "hash": "a4f41b86040bdd0ce0903962d17fa481",
  "result": {
    "engine": "knitr",
    "markdown": "# Line of Best Fit {#sec-best-fit-line}\n\nIn prior chapters we have looked at the situation when we have a numerical response variable and either no explanatory variable or a single categorical explanatory variable. In this chapter we look at the important case when the response variable and the explanatory variable are numerical.\n\nIn our running story, this is the third meeting of our interns with Amanda who owns a business that operates kiosks in a city at two locations. Last month, the interns used kiosk labels to help set different staffing levels for the Beach and the Mall.\n\nNow Amanda brings a new kind of hint.\n\n“It’s not just location that seems to affect the number of customers we get.” she says. “Today, I need you all to help me with a decision for the beach kiosk. At the Beach kiosk, demand seems to swing with temperature. I want a model that uses what we know *today* about tomorrow's temperature (the forecast) to predict customer traffic for tomorrow. This time, I will use the prediction not for staffing, but to decide on stock levels of items. We do not want to under or overstock too much and so good predictions can really help me.”\n\nShe adds one constraint immediately:\n\n“Keep it simple. I need something I can explain in one sentence.”\n\nIn this chapter, we continue our model-building journey. A model is simply a rule that takes information we have and produces a predicted value for the response.\n\n-   With no explanatory variable, we learned in @sec-mean-model, that our model is “always predict the mean.”\n\n-   With a single categorical explanatory variable, @sec-category-means showed us that our model should be “predict the mean of the category.”\n\n::: {.callout-note title=\"Quick Check\"}\nWhen we use the mean as a model, what value do we predict for every observation?\n\nWhy is this considered a model rather than just a summary?\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nThe model predicts the same value—the mean—for every observation.\n\nIt is a model because it provides a rule for generating predicted values, not just a numerical description of the data.\n:::\n\n::: {.callout-note title=\"Quick Check\"}\nHow does a category-mean model improve on the overall mean model?\n\nWhat additional information does it use?\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nA category-mean model allows different predictions for different groups.\n\nIt improves on the overall mean model by using information about group membership instead of treating all observations as identical.\n:::\n\nIn this chapter we consider a single numerical explanatory variable. In this case we need something quite different.\n\n## The beach kiosk staffing problem\n\nWe again consider a situation where the company gets a hint before making a decision—but now the hint is numerical, not categorical.\n\nAmanda is focused on the Beach kiosk. (The Mall kiosk is steadier, but the Beach kiosk shas more variance in daily customer numbers and so just using the average can be costly.\n\nManagement has noticed that customer traffic at the Beach kiosk varies strongly with the day’s high temperature.\n\nOver many past days, the company has recorded:\n\n-   the day’s high temperature (in °F), and\n\n-   the number of customers who visited the beach kiosk that day.\n\nLet us assume that data are available for 25 different days. (As before, for understanding the ideas, the exact number of days is not important.)\n\n@tbl-temp-customers shows the historical data.\n\n\n| day | temperature | customers |\n|:----|------------:|----------:|\n| D01 |          58 |        98 |\n| D02 |          60 |       104 |\n| D03 |          62 |       122 |\n| D04 |          64 |       113 |\n| D05 |          66 |       117 |\n| D06 |          68 |       133 |\n| D07 |          70 |       126 |\n| D08 |          72 |       115 |\n| D09 |          74 |       123 |\n| D10 |          76 |       128 |\n| D11 |          78 |       145 |\n| D12 |          80 |       141 |\n| D13 |          58 |       106 |\n| D14 |          60 |       107 |\n| D15 |          62 |       105 |\n| D16 |          64 |       127 |\n| D17 |          66 |       120 |\n| D18 |          68 |       103 |\n| D19 |          70 |       128 |\n| D20 |          72 |       121 |\n| D21 |          74 |       120 |\n| D22 |          76 |       130 |\n| D23 |          78 |       127 |\n| D24 |          80 |       132 |\n| D25 |          58 |        98 |\n\n: Daily temperature and customer counts {#tbl-temp-customers}\n\nBased this, the company must decide how many customers will show up tomorrow. They will u8se that to determine how much of each item to stock for sale. It can use an extremely reliable temperature forecast for its city.\n\nThe problem now is this:\n\n> Given the forecast high temperature for a day, how many customers should the company plan for?\n\n### How does this differ from earlier examples we have studied?\n\nIn @sec-mean-model, we used no hints at all, and the mean turned out to be the best model.\n\nIn @sec-category-means, our hint was categorical, and the mean within each category turned out to be the best model.\n\nThis time, the situation is fundamentally different. Our hint, temperature, is numerical, and so none of the earlier “mean-based” approaches applies directly.\n\nOne possibility would be to convert temperature into categories such as *Cold*, *Warm*, and *Hot*, and then apply the category-means approach. While this would work, it is somewhat artificial. Two days that differ by only one degree could end up in different categories. For example, calling 50°F Cold and 51°F Warm introduces an arbitrary cutoff that has no real business justification.\n\nTemperature does not naturally fall into clear bins. Can we do better?\n\n### Toward a rule-based model\n\nRather than assigning a few separate numbers, Amanda now wants a rule that:\n\n> takes the day’s temperature as input and produces an estimated number of customers as output.\n\nMany such rules are possible. The company wants a simple rule that takes a temperature as input and produces a predicted number of customers as output. Let us assume that the same general pattern will continue for temperatures. It is not going to be exactly the same, but the general range and the distribution will remain the same for the next month. Therefore, for any given temperature the model's or rule's output should reasonably match the actual number of customers in the data set.\n\n@fig-beach-kiosk-model-box-diagram shows what Amanda is looking for at a high level.\n\n![Model that takes a temperature as input and produces a predicted number of customers for the beackh kiosk -- the model should be based on the data in @tbl-temp-customers](../../images/fig-beach-kiosk-model-box-diagram.png){#fig-beach-kiosk-model-box-diagram width=\"70%\" fig-align=\"center\"}\n\nWe know that with higher temperatures more people will hit the beach. \n\nPeter has an idea. \"Since Amanda is looking for a simple rule, should we consider consider a model which looks something like this?\" He draws a picture. @fig-specific-beach-kiosk-model-box). \"Of course, I just made up the numbers 15 and 2. With this model, if we input a temperature of 70, the model will predict 155 customers.\" \n\n![Caption](../../images/fig-specific-beach-kiosk-model-box.png){#fig-specific-beach-kiosk-model-box width=70% fig-align=center}\n\nPeter further clarifies \"I did not mean the specific numbers 15 and 2 literally in @fig-specific-beach-kiosk-model-box, I am really saying that we could look for a model that has the general shape in this figure\" and he quickly sketches @fig-beach-kiosk-temp-generic-model \n\n\"We just need to find *good* values for *a* and *b*.\n\n![Peter's second explanatory sketch showing that he wants a model of this general form, but wants to find good values for *a* and *b* so that the model generates good predictions](../../images/fig-beach-kiosk-temp-generic-model.png){#fig-beach-kiosk-temp-generic-model width=70% fig-align=center}\n\n\nPeter then writes @eq-kiosk-cust-temp-model to explain the same idea and adds \"I placed a *hat* on top of *customers* because the equation only calculates an *estimate* and this could be different from an actual value.\" \n\n\n$$\n\\widehat{\\text{customers}} = a +  b\\,\\text{temperature}\n$$ {#eq-kiosk-cust-temp-model}\n\n\nAngela says \"That's a great idea. Amanda wanted something simple. Of course we gave her very simple models earlier too, but now we have to incorporate *temperature* and I cannot imagine something that is adjusts its predictions to the temperature and is simpler than what Peter has shown.\"\n\n\"If we fix the values of *a* and *b*, eq-kiosk-cust-temp-model produces a predicted number of customers for any temperature. For example, if we fix a value of 15 for *a* and 2 for *b* as Peter did before,  then @eq-kiosk-cust-temp-model-specific shows our model to determine the number of customers for any temperature. If we plug in a temperature of 70, the model will give us 155 as the estimated number of customers.\"\n\n$$\n\\widehat{\\text{customers}} = 15 +  2\\,\\text{temperature}\n$$ {#eq-kiosk-cust-temp-model-specific}\n\nIgor says \"I like this idea too. Its prediction changes smoothly as temperature changes, and it avoids arbitrary cutoffs.\"\n\nDave sounds a cautionary note \"This is all great guys, but how do we find the *best* values for *a* and *b*? And what do we mean by *good* choices?\"\n\nSuzie: \"I have an idea about how we might define *good*. Suppose we have a model -- let us say for the time being that we use @eq-kiosk-cust-temp-model-specific. That is, given any temperature, we plug it into @eq-kiosk-cust-temp-model-specific and it spits out the estimated number of customers.\n\nSuppose we plug in 66 for temperature. @eq-kiosk-cust-temp-model-specific says the number of customers is 15 + 2*66, or 147. In our data, the number of customers for 66 degrees is 117 for one of the rows where the temperature is 66.\"\n\nDavid's eyes light up \"Oh, we can treat this just like we did earlier and get the penalty by squaring!. This penalty will become 30*30, or 900 for that one row.\"\n\nIgor then observes \"For any given value of *a* and *b*, we can compute the penalty corresponding to each row of the data. With *a* = 15 and *b* = 2, we can compute the penalties for each row.\" He then worked for a few minutes to come up with penalties for all the rows of @tbl-temp-customers and came up with @tbl-igor-computations-for-15-2.\n\n|day | temperature| customers| est_customers| diff| penalty|\n|:---|-----------:|---------:|-------------:|----:|-------:|\n|D01 |          58|        98|           131|  -33|    1089|\n|D02 |          60|       104|           135|  -31|     961|\n|D03 |          62|       122|           139|  -17|     289|\n|D04 |          64|       113|           143|  -30|     900|\n|D05 |          66|       117|           147|  -30|     900|\n|D06 |          68|       133|           151|  -18|     324|\n|D07 |          70|       126|           155|  -29|     841|\n|D08 |          72|       115|           159|  -44|    1936|\n|D09 |          74|       123|           163|  -40|    1600|\n|D10 |          76|       128|           167|  -39|    1521|\n|D11 |          78|       145|           171|  -26|     676|\n|D12 |          80|       141|           175|  -34|    1156|\n|D13 |          58|       106|           131|  -25|     625|\n|D14 |          60|       107|           135|  -28|     784|\n|D15 |          62|       105|           139|  -34|    1156|\n|D16 |          64|       127|           143|  -16|     256|\n|D17 |          66|       120|           147|  -27|     729|\n|D18 |          68|       103|           151|  -48|    2304|\n|D19 |          70|       128|           155|  -27|     729|\n|D20 |          72|       121|           159|  -38|    1444|\n|D21 |          74|       120|           163|  -43|    1849|\n|D22 |          76|       130|           167|  -37|    1369|\n|D23 |          78|       127|           171|  -44|    1936|\n|D24 |          80|       132|           175|  -43|    1849|\n|D25 |          58|        98|           131|  -33|    1089|\n\n: Igor's computations: For each row he plugged in the temperature into @eq-kiosk-cust-temp-model-specific and computed est_customers (estimated customers by @eq-kiosk-cust-temp-model-specific) and then computed the squared difference between the actual value of *customers* and *est_customers* {#tbl-igor-computations-for-15-2}\n\nAngela said: \"Well the total penalty is 28,312. How do we know if different values for *a* and *b* can give us a lower penalty?. In earlier weeks we only had to try different values for one thing. Here we have two things *a* and *b* to think about. What do we do?\"\n\nSuzie says: \"But we have made progress. We have at least clearly defined the problem. We need to find the values for *a* and *b* that will give us the lowest possible total penalty along the lines of Igor's @tbl-igor-computations-for-15-2. That is, we try many different values for &*a* and *b* and compute Igor's table (@tbl-igor-computations-for-15-2) for that combination. We then select the combination taht produces the lowest total penalty. We want the lowest total penalty because that will mean that we have got the model predictions as close as we can to the real values in the data.\"\n\nPeter says \"Hey, this reminds me of something I saw my dad doing for his work. He always says that visualizing makes things easier. WHy don't we try that first?\"\"\n\n\n## Visualizing the model\n\nPeter continued: \"Graphically, @eq-kiosk-cust-temp-model-specific represents a straight line. Let us generate a scatter plot of the data first.\" He wrote come code and generated @fig-kiosk-temp-cust-scatter which showed the relationship between *temperature* and *customers*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkiosk |> \n  point_plot(customers ~ temperature)\n```\n:::\n\n\n\n![Scatterplot of *temperature* against number of *customers* from the *kiosk* data frame](../../images/fig-kiosk-temp-cust-scatter.png){#fig-kiosk-temp-cust-scatter width=\"70%\" fig-align=\"center\"}\n\n\n::: {.callout-note title=\"Quick Check\"}\nLooking at the scatter plot of temperature versus customers:\n\n-   Does the relationship appear perfectly linear?\n-   Does that prevent us from using a linear model?\n\nExplain briefly.\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\n- The relationship is not perfectly linear; there is noticeable scatter around what seems to be a somewhat linear pattern.\n\n- This does not prevent us from using a linear model. A linear model is a simplification that captures the overall trend, not a claim that all points lie exactly on a line.\n:::\n\nPeter then said: \"We have been using @eq-kiosk-cust-temp-model-specific as our tentative model to explore. In my pre-calculus class, i learned that this is actually the equation of a straight line. I can add it to the earlier plot.\" \n\nHe then generated  @fig-kiosk-model-initial-attempt which has both the earlier scatter plot and the line from @eq-kiosk-cust-temp-model-specific. (You need not worry about how he added the line yet. We will learn that shortly.)\n\n![Adding a line showing the kiosk model with *a* = 15 and *b* = 2](../../images/fig-kiosk-model-initial-attempt.png){#fig-kiosk-model-initial-attempt width=\"70%\" fig-align=\"center\"}\n\n\nIgor says: \"Wow this model is way off the mark! We can reduce the total penalty by a lot.\"\n\nPeter smiles: \"Please explain to us lesser mortals Igor!\"\n\nTry to answer by yourself before reading what Igor has to say.\n\nIgor says: \"Well, the line represents the model predictions. If the predictions are good, they will be near the actual points. But our line,representing the prediction for any temperature is way above  the general region where the points actually lie! For example, we see from @fig-kiosk-model-initial-attempt that two days in our historical data have had a temperature of 64 degrees. The number of customers for those two days have been 113 and 127. But the tentative model from @eq-kiosk-cust-temp-model-specific predicts 143. In fact, we can take any actual row of our data and see that the model predictions are way higher than the general area of the points. So, we clearly did not make great initial choices for *a* and *b*! No offense Peter, I know you were just illustrating the form, and not trying to be exact.\"\n\nAngela says: \"OK, if we want the model predictions to reflect actual data, we would want the line to be within the point cloud and perhaps generally go through the middle of the cloud. That would make the predictions at least fall in the general region of the actual values. Let us try again with different choices for *a* and *b*. How should we change the values? Should we increase or decrease each one?\"\n\nIgor says: \"In @eq-kiosk-cust-temp-model-specific *a* represents where the line intercepts the y-axis at x = 0. I learned this in my pre-calc class. In that class they called *a* the *intercept*.\"\n\nSuzie jumped up \"I know why. they call it the intercept because that is where the line *intersects* the y-axis. But in this plot, the line does not intersect the y-axis at all.\"\n\nIgor: \"It does intersect the y-axis, if only we zoom out and show the origin of the plot where x=0 and  y=0). Currently the x-axis only starts from around 55.\" \n\nPeter quickly modified the plot to show the origin. @fig-initial-model-with-intercept-visible shows his new plot. This is effectively the same plot as @fig-kiosk-model-initial-attempt, only zoomed out so that we can see more.\n\n![Model with *a* = 15 and *b* = 2 plotted to reveal the origin of the plot so as to see the intercept](../../images/fig-initial-model-with-intercept-visible.png){#fig-initial-model-with-intercept-visible width=\"70%\" fig-align=\"center\"}\n\nEven though a temperature of 0°F is well outside our data range, visualizing the intercept helps us understand what changing *a* actually does to the line.\n\nSuzie: \"From the plot, we can see that the line correctly intersects the y-axis at 15 since *a = 15*. What can we do to improve the line so that it goes through the points instead of way above them?\"\n\nDavid opines  \"To make the line go through the point cloud, it looks like we need to make the line less steep, almost like grabbing the top right of the line and pulling it down. How do we achieve that?\" \n\nPeter's pre-calculus comes to the rescue again. \"The value for *b* determines the steepness of the line. So, we can decrease it to get the line to go through the cloud of points. Perhaps we should decrease it from 2 to 1.5 and see what happens.\"\n\nPeter modifies his plot again. @fig-kiosk-model-second-guess shows his new plot.\n\n![Second attempt: model with *a* = 15 and *b* = 1.5](../../images/fig-kiosk-model-second-guess.png){#fig-kiosk-model-second-guess width=\"70%\" fig-align=\"center\"}\n\nAngela says: \"Wow! that looks so much better! But can we do even better?\" \n\nIgor produced the table for this model and announced: \"We have smashed the total penalty from 28312 to 1469! But I still don't know if that is the lowest possible. Maybe there are other values of *a* and *b* that can reduce it even more.\"\n\nPeter, tired of generating more and more plots, says \"We can keep on trying -- we have an infinite number of choices for *a* and *b*. How do we find the *best* line? What does *best* even mean?\n\n## What do we mean by the *best* model?\n\nIn the previous section, the team tried a few values for *a* and *b* and our second attempt proved to be much better than the first. However, we want the *best* line. But what does that mean?\n\nDavid says \"Before visualizing the models, we had already identified what we mean by *best*. We had already established that the *best* model or *line*, is the ones that generates the lowest total penalty as we computed in @tbl-igor-computations-for-15-2. That is, we choose the values for *a* and *b* and mimic the process we used for @btl-igor-computations-for-15-2 and choose the values that produce the smallest total penalty. But how do we do that when we have an infinite number of possibilities.\"\n\nAngela says: \"Can we not repeat what Suzie did and generate a plot for various values and visually see what the best values are?\"\n\nPeter says: \"Guys, before we think about that, I can help to visualize the sort of computations we did in @tbl-igor-computations-for-15-2 but for the line in @fig-kiosk-model-second-guess.\" and comes up with \n@fig-tentative-model-with-residuals-visualized.\n\nHe goes on to explain: \"We are after a model that produces *good* predictions. Therefore the difference between the actual value and the model prediction (the column *diff* in @tbl-igor-computations-for-15-2) matters. The lower that difference is the better.\" He then generates @fig-tentative-model-with-residuals-visualized to visualize the previous model (*a* = 15 and *b* = 1.5) with the differences between the actual values and model predictions explicitly marked for three chosen points. These differences are usually called *errors* or even better, *residuals*.\n\n![Model with *a* = 15 and *b* = 1.5 visualized along with the *residual* or error for three chosen points](../../images/fig-tentative-model-with-residuals-visualized.png){#fig-tentative-model-with-residuals-visualized width=\"70%\" fig-align=\"center\"}\n\n::: {.callout-warning title=\"Common Misconception\"}\nThe error or *residual* for a prediction is **not** the shortest distance to the line. It is the **vertical difference** between the observed value and the predicted value. This is because our model predicts the response (customers) and we compare that to the actual temperature for any point.\n:::\n\nDavid points to @fig-tentative-model-with-residuals-visualized and says: \"For the point at *temperature* = 62, the actual data had *customers* = 105, whereas our model predicts 108. So the model is off by -3 (if we compute the residual using this equation.\" and he writes out @eq-residual-1.\n\n$$\n\\text{residual} = \\text{actual value} - \\text{model prediction}\n$$ {#eq-residual-1}\n\nPeter continues: \"\"For the data point with *temperature* = 68, the model predicts 116, whereas the actual value was 132 for a residual of +16. Finally, for the point where the *temperature* is 76, the model prediction is 129, whereas the actual value was 130. The model did well on this point. In this way, we can compute the *residual* for every point and square it. We are not particularly concerned about the actual sign of the residual since we are going to square it anyway. This is exactly what Igor showed us in @tbl-igor-computations-for-15-2.\"\n\n::: {.callout-note title=\"Quick Check\"}\nIf a model predicts 120 customers for a given temperature, and the actual number is 132:\n\n-   What is the prediction error?\n-   Is it positive or negative?\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nThe prediction error is 132 − 120 = 12.\n\nThe error is positive because the actual value is higher than the predicted value.\n:::\n\nAngela says: \"If we draw any line through these points, each line will have low *absolute residuals* for some points and higher ones for others. But we have many points and so we want an *overall measure of how close the line is to all the points* by considering all the *absolute residuals*. Even if a line is spot-on for a few points, it is not very useful if it is wildly off for many others. That overall measure is in fact the total penalty from @tbl-igor-computations-for-15-2. It is very nice that we have seen the computations and also seen its visualization to make sure we understand all this. I sure am glad we are doing this as a team!\"\n\nIgor says, \"Yes, now I too see the connection between my table (@tbl-igor-computations-for-15-2) and Peter's visualization of the concept. So, we will square the residuals from all the points and add them all up. That is how much total penalty the line/model has when we consider *all* data points. As Amanda wanted, squaring punishes bigger residuals much more than smaller ones. I have heard that this is a standard theme in statistics.\"\n\n::: {.callout-warning title=\"What's the deal with squaring?\"}\nWe square errors for the same reason we used squared deviations when defining variance:\n\n-   positive and negative errors should not cancel out\n-   larger errors should count more than smaller ones\n\nLeast squares regression that we study in this course is built on the same logic as variance. In a later chapter, we will see this idea pop when we look at the explanatory power of a model. Hang tight till then!\n:::\n\n::: {.callout-note title=\"Quick Check\"}\nWhy don’t we choose the line that minimizes the **sum of errors** instead of the **sum of squared errors**?\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nPositive and negative errors would cancel out if we summed errors directly.\n\nSquaring errors ensures that all errors contribute positively and that larger errors are penalized more heavily.\n:::\n\nClearly, we want a line that is as close as possible to all the points overall. Honestly, the line in @fig-tentative-model-with-residuals-visualized seems to be pretty good, at least visually.\n\nDavid says: \"For this model (a = 15, b = 1.5), the total of all the squared residuals is: 1,469. Is there a line that can give an even lower value? What are the values for *a* and *b* for which we get the *lowest* possible total of all the squared residuals?\"\n\nThe team now understands exactly what they mean by the *best* model. They are looking for values for *a* and *b* that will result in the lowest total penalty. But they are stuck on to how to actually get it. \n\nCatching the thread about trying to generate a plot to identify the best values just like Suzie did in the first week, Angela returns to her idea. \"Suzie, can you do you pull off your plotting magic again, please?\"\n\nSuzie: \"Well angela, that would be a three-dimensional plot because we will have *a*, *b*, and the *total penalty* all on the same plot. I can do it, but I am afraid it will not be much help.\"\n\nIgor: \"Wht not just print out a large table of all combinations of *a* and *b* and the total penalty for each combination? We can then look through the table and select the best value -- or even ask the program to look through the table and find the values for us?\"\n\nSuzie says \"We can Igor, but I have two issues. Firstly, when I try the various combinations of *a* and *b*, I can only do it in some intervals like 0.1 or something like that. What if the best solution was between two of my values? We will then not get an exact solution, but we will get something very close. A bigger problem is what if Amanda comes up with two more variables to use in our predictions? Then trying all combinations can generate a very large number of rows, perhaps millions. There has to be a better way.\"\n\nSteve, who has been silent for a long time has actually been having a nagging feeling that he has done something similar in one of his classes. He now says \"I think we learned something like this in my econ class. Rather than making Suzie write a whole new program, I think there is a function in R, based in calculus that can readily do this for us! Let me try.\" \n\nAnd he runs the code below: which generates the *best* values for *a* and *b* that guarantee that the *sum of squared residuals* is the smallest possible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkiosk |> \n  model_train(customers ~ temperature) |> \n  coef()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) temperature \n      22.06        1.42 \n```\n\n\n:::\n:::\n\n\nSteve then explains: \"Ah, now I remember what the output means. We see two values in the output: *intercept* and *temperature*. The first is *a* and the second is *b*! These are exactly the values of *a* and *b* that minimize the sum of squared residuals.\" \n\nFrom the output, Steve then writes the model equation @eq-kiosk-cust-temp-model-best, and says:\n\n\"This equation shows the *best model* for our problem, given that we want a straight line and want to minimize the sum of squared residuals.\"\n\n\"Our econ teacher called this the least-squares regression line.\"\n\n$$\n\\widehat{\\text{customers}} = 22.06 +  1.42\\,\\text{temperature}\n$$ {#eq-kiosk-cust-temp-model-best}\n\n::: {.callout-note title=\"Key Idea\"}\nSince there is an infinite number of possible choices for *a* and *b*, we need a systematic way to find the values that minimize the total of the squared residuals. Mathematically, this problem can be solved using calculus. Practically, we sit back and let software do this for us.\n:::\n\n::: {.callout-note title=\"Pause & Think\"}\nComplete the sentence:\n\n“A least squares regression line is the line that \\_\\_\\_\\_\\_.”\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nA least squares regression line is the line that minimizes the sum of squared vertical prediction errors, or the sum of squared residuals (same thing).\n:::\n\nIgor is at it again. He announces \"The total penalty for this is 1388. This beats the 1469 we got earlier. And, according to Steve, this is the *best*. I am quite impressed that we had eyeballed a solution very close to the best!\"\n\nThe team found a line that *minimizes the sum of squared residuals*. Technically this is called the *least-squares criterion* and plays a very important role in statistics and data analysis.\n\nThey explained the model to Amanda. After a lot of questioning to understand their decision criteria, Amanda agreed to use this model for the next month. \n\nAmanda was thrilled with their work. \"Great work team. Talk to me when you graduate. I want to assemble a business analytics team for my businesses and I sure can do with a team like yours.\"\n\n::: {.callout-note title=\"Pause & Think\"}\nSuppose tomorrow’s high temperature is 73°F. Your model predicts 124 customers.\n\n-   Does this mean exactly 124 customers will arrive?\n-   What other factors (besides temperature) might cause the actual number to be higher or lower?\n\nWhat does this tell you about what a regression model can—and cannot—do?\n:::\n\n::: {.callout-note icon=\"false\" collapse=\"true\"}\n## Suggested answer\n\n-   No. 124 is only a model prediction. In reality the number of customers may be 124 or higher or lower.\n\n-   The model acts as if only the temperature determines the number of customers. In reality, many other factors play a role too. Perhaps there is a football game in town and that draws people away. Perhaps the humidity is unusually high and people stay away. Many factors play a role in this outcode. Our model uses a simple rule that can potentially perform decently without being accurate.\n\nIn later chapters we will see that we can include more variables in a model and improve its performance.\n:::\n\n## Optional enrichment topic: Beyond straight lines\n\nIn this chapter, we have assumed that a straight line is a reasonable model for the relationship between temperature and customer traffic. In many business settings, this assumption works well: simple models are easy to interpret, easy to communicate, and often good enough for decision-making.\n\nHowever, not all relationships are well captured by a straight line.\n\n### When a straight line may not be appropriate\n\nIn some situations, a scatter plot may reveal patterns such as:\n\n-   curvature (the relationship bends),\n\n-   diminishing returns (the effect of increases slows down), or\n\n-   threshold effects (behavior changes after a certain point).\n\nFor example, at very high temperatures, customer traffic at a beach kiosk might level off or even decline as conditions become uncomfortable. A straight line cannot capture this kind of behavior well, no matter how its slope and intercept are chosen.\n\nIn such cases, forcing a linear model may lead to systematic residual patterns, indicating that the model is missing something important.\n\n### A flexible alternative: LOWESS smoothing\n\nOne way to explore nonlinear relationships is through a technique called LOWESS (short for locally weighted scatter plot smoothing).\n\n@fig-lowess-example shows an example of a LOWESS model using the *mpg* data frame. In this data frame the *displacement* of an engine is related to the highway mileage *hwy*, but not in a linear way. The LOWESS model better describes it than a straight line would. Like the earlier line model, this too can take in a value for *displ* and provide an estimated value for *hwy*.\n\n![LOWESS model for *displacement* vs *cty* in the *mpg* data frame showing how it captures the curvature in the relationship better than a lien can](../../images/fig-lowess-example.png){#fig-lowess-example width=\"70%\" fig-align=\"center\"}\n\nRather than fitting a single global rule like a straight line, LOWESS works by:\n\n-   focusing on a small neighborhood of points around each value of the explanatory variable, and\n\n-   fitting simple local models that adapt to the data in that neighborhood.\n\nThe result is a smooth curve that follows the overall pattern of the data without requiring us to specify a particular functional form in advance.\n\nLOWESS is especially useful for:\n\n-   exploratory analysis,\n\n-   visualizing trends, and\n\n-   diagnosing whether a straight-line model is reasonable.\n\nIt is not intended to replace regression models in all cases, but rather to help us understand the structure of the data.\n\n### Models as choices, not defaults\n\nThe key takeaway is not that linear models are “wrong,” but that models are choices.\n\n-   A straight line is a good choice when the relationship is approximately linear and interpretability matters.\n\n-   A nonlinear model may be a better choice when the data clearly suggest curvature or changing behavior.\n\n-   Flexible tools like LOWESS help us see what the data are trying to tell us before we commit to a specific model.\n\nIn all cases, the central modeling question remains the same:\n\n> Does this model capture the important structure in the data well enough to support the decisions we want to make?\n\nThis mindset—treating models as purposeful approximations rather than automatic formulas—will guide us throughout the rest of this book.\n",
    "supporting": [
      "line-of-best-fit-story_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}