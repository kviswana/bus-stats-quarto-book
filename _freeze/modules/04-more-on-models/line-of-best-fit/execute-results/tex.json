{
  "hash": "1069fe2f19cbc4c0432bba9016a44d61",
  "result": {
    "engine": "knitr",
    "markdown": "# Line of best fit {#sec-best-fit-line}\n\nIn prior chapters we have looked at the situation when we have a numerical response variable and either no explanatory variable or a single categorical explanatory variable. In this chapter we look at the important case when the response variable and the explanatory variable are numerical.\n\nIn this chapter, we continue our model-building journey. A model is simply a rule that takes information we have and produces a predicted value for the response.\n\n-   With no explanatory variable, we learned in @sec-mean-model, that our model is “always predict the mean.”\n\n-   With a single categorical explanatory variable, @sec-category-means showed us that our model should be “predict the mean of the category.”\n\n::: {.callout-note title=\"Quick Check\"}\nWhen we use the sample mean as a model, what value do we predict for every observation?\n\nWhy is this considered a model rather than just a summary?\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nThe model predicts the same value—the sample mean—for every observation.\n\nIt is a model because it provides a rule for generating predicted values, not just a numerical description of the data.\n:::\n\n::: {.callout-note title=\"Quick Check\"}\nHow does a category-mean model improve on the overall mean model?\n\nWhat additional information does it use?\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nA category-mean model allows different predictions for different groups.\n\nIt improves on the overall mean model by using information about group membership instead of treating all observations as identical.\n:::\n\nIn this chapter we consider a single numerical explanatory variable. In this case we need something quite different.\n\n## The beach kiosk staffing problem\n\nWe again consider a situation where the company gets a hint before making a decision—but now the hint is numerical, not categorical.\n\nThe same company operates sales kiosks at the Beach location. Management has noticed that customer traffic at the beach kiosk varies strongly with the day’s high temperature.\n\nOver many past days, the company has recorded:\n\n-   the day’s high temperature (in °F), and\n\n-   the number of customers who visited the beach kiosk that day.\n\nLet us assume that data are available for 25 different days. (As before, for understanding the ideas, the exact number of days is not important.)\n\n@tbl-temp-customers shows the historical data.\n\n| day | temperature | customers |\n|:----|------------:|----------:|\n| D01 |          58 |        98 |\n| D02 |          60 |       104 |\n| D03 |          62 |       122 |\n| D04 |          64 |       113 |\n| D05 |          66 |       117 |\n| D06 |          68 |       133 |\n| D07 |          70 |       126 |\n| D08 |          72 |       115 |\n| D09 |          74 |       123 |\n| D10 |          76 |       128 |\n| D11 |          78 |       145 |\n| D12 |          80 |       141 |\n| D13 |          58 |       106 |\n| D14 |          60 |       107 |\n| D15 |          62 |       105 |\n| D16 |          64 |       127 |\n| D17 |          66 |       120 |\n| D18 |          68 |       103 |\n| D19 |          70 |       128 |\n| D20 |          72 |       121 |\n| D21 |          74 |       120 |\n| D22 |          76 |       130 |\n| D23 |          78 |       127 |\n| D24 |          80 |       132 |\n| D25 |          58 |        98 |\n\n: Daily temperature and customer counts {#tbl-temp-customers}\n\nBased this, the company must decide how many workers to schedule for tomorrow. It can use an extremely reliable temperature forecast for its city.\n\nThe problem now is this:\n\n> Given the high temperature for a day, how many customers should the company plan for?\n\n### How does this differ from earlier examples we have studied?\n\nIn @sec-mean-model, we used no hints at all, and the mean turned out to be the best model.\n\nIn @sec-category-means, our hint was categorical, and the mean within each category turned out to be the best model.\n\nThis time, the situation is fundamentally different. Our hint—temperature—is numerical, and so none of the earlier “mean-based” approaches applies directly.\n\nOne possibility would be to convert temperature into categories such as *Cold*, *Warm*, and *Hot*, and then apply the category-means approach. While this would work mechanically, it is somewhat artificial. Two days that differ by only one degree could end up in different categories. For example, calling 50°F Cold and 51°F Warm introduces an arbitrary cutoff that has no real business justification.\n\nTemperature does not naturally fall into clear bins. Can we do better?\n\n### Toward a rule-based model\n\nRather than assigning a few separate numbers, the company now wants a rule that:\n\n> takes the day’s temperature as input and produces a staffing recommendation as output.\n\nMany such rules are possible. The company wants a simple rule that, takes a temperature as input and produces a predicted number of customers as output. Its output should match the actual number of customers in the data set as well as possible.\n\n@fig-beach-kiosk-model-box-diagram shows what we are looking for at a high level.\n\n![Model that takes a temperature as input and produces a predicted number of customers for the beackh kiosk -- the model should be based on the data in @tbl-temp-customers](../../images/fig-beach-kiosk-model-box-diagram.png){#fig-beach-kiosk-model-box-diagram width=\"70%\" fig-align=\"center\"}\n\nWe know that with higher temperatures more people will hit the beach.\n\nThis naturally leads us to consider a model which looks something like @fig-specific-beach-kiosk-model-box. We provide the temperature as input and the model uses a *very simple formula* to compute the predicted number of customers for that temperature. Do not take the model in \\@@fig-specific-beach-kiosk-model-box too literally -- we just made up the numbers 15 and 2 in the model to illustrate the idea.\n\n![The general shape we want for the beach model -- we have just arbitrarily made up the numbers 15 and 2 in the model to illustrate](../../images/fig-specific-beach-kiosk-model-box.png){#fig-specific-beach-kiosk-model-box width=\"70%\" fig-align=\"center\"}\n\nIn reality, we seek a model like @fig-generic-box-model-beach-kiosk. Here the form of the model is fixed. All that remains is to find values for *a* and *b*. The *hat* on top of *cust* in the figure shows that the model computes an *estimated* value. We use the *hat* to distinguish between an actual value in the data set and an *estimated* value.\n\n![Actual form of the model we seek -- in this we need to find good values for b and b](../../images/fig-generic-box-model-beach-kiosk.png){#fig-generic-box-model-beach-kiosk width=\"70%\" fig-align=\"center\"}.\n\nWe can express it as a mathematical equation as @eq-kiosk-cust-temp-model shows. We just need to replace *a* and *b* with actual numbers to get a usable model.\n\n$$\n\\widehat{\\text{customers}} = a +  b\\,\\text{temperature}\n$$ {#eq-kiosk-cust-temp-model}\n\nIf we fix the values of *a* and *b*, this model produces a predicted number of customers for any temperature. For example, if we fix a value of 15 for *a* and 2 for *b*, then @eq-kiosk-cust-temp-model-specific shows our model to determine the number of customers for any temperature. If we plug in a temperature of 70, the model will give us 155 as the estimated number of customers.\n\n$$\n\\widehat{\\text{customers}} = 15 +  2\\,\\text{temperature}\n$$ {#eq-kiosk-cust-temp-model-specific}\n\nIts prediction changes smoothly as temperature changes, and it avoids arbitrary cutoffs. We just arbitrarily fixed *a* and *b* without any knowledge of how well those choices would perform. What do we mean by *how well those choices would perform?*\n\nIf we use the model for many days, then for each day the model gives a prediction and for each day there is an actual number of customers who turn up. The difference between the actual value and the model’s prediction is called a *residual* -- you could call it the error too.\n\nSome residuals are positive and others are negative. If we simply added them up, large positive and negative errors could cancel each other out. Squaring the *residuals* ensures that all errors contribute positively and that larger errors are penalized more heavily. There is also a different reason why we square residuals, but we will not get into that.\n\nIf the model's predictions are close to the actual values then the residuals will be low. Therefore, we want the total of all the squared residuals to be as low as we can make it, by choosing good values for *a* and *b*.\n\nWe can understand things better through a visualization.\n\n## Visualizing the model\n\nGraphically, @eq-kiosk-cust-temp-model-specific represents a straight line. Let us generate a scatter plot of the data first. @fig-kiosk-temp-cust-scatter shows the plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkiosk |> \n  point_plot(customers ~ temperature)\n```\n:::\n\n\n![Scatterplot of *temperature* against number of *customers* from the *kiosk* data frame](../../images/fig-kiosk-temp-cust-scatter.png){#fig-kiosk-temp-cust-scatter width=\"70%\" fig-align=\"center\"}\n\n::: {.callout-note title=\"Quick Check\"}\nLooking at the scatterplot of temperature versus customers:\n\n-   Does the relationship appear perfectly linear?\n-   Does that prevent us from using a linear model?\n\nExplain briefly.\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nThe relationship is not perfectly linear; there is noticeable scatter.\n\nThis does not prevent us from using a linear model. A linear model is a simplification that captures the overall trend, not a claim that all points lie exactly on a line.\n:::\n\nLet us then add the model line from @eq-kiosk-cust-temp-model-specific too on top of it. Right now, we need not concern ourselves with the code. @fig-kiosk-model-initial-attempt shows the scatter plot with the tentative model line.\n\n![Adding a line showing the kiosk model with *a* = 15 and *b* = 1](../../images/fig-kiosk-model-initial-attempt.png){#fig-kiosk-model-initial-attempt width=\"70%\" fig-align=\"center\"}\n\nThe model from @eq-kiosk-cust-temp-model-specific when visualized in @fig-kiosk-model-initial-attempt seems way off the mark. Can you see why?\n\nWell, the line represents the model predictions and they are way off from the general region where the points actually lie! For example, we see from @fig-kiosk-model-initial-attempt that two days in our historical data have had a temperature of 64 degrees. The number of customers for those two days have been 113 and 127. But the tentative model from @eq-kiosk-cust-temp-model-specific predicts 143. That does not make sense. So, we clearly did not make great choices for *a* and *b*.\n\nIf we want the model predictions to reflect actual data, we would want the line to be within the point cloud. That would make the predictions fall in the general region of the actual values.\n\nLet us try again with different choices for *a* and *b*. How should we change the values? Should we increase or decrease each one?\n\nIn @eq-kiosk-cust-temp-model-specific *a* represents where the line intercepts the y-axis at x = 0. You might recall this from your prior mathematics knowledge, but it is fine if you do not. *a* is generally called the *intercept*.\n\nIn @fig-kiosk-model-initial-attempt, we cannot see where the line actually intersects the y-axis at x = 0, since the x-axis starts only from around 55. Let us generate a plot that helps us to looks at both the axes starting from 0. @fig-initial-model-with-intercept-visible shows the plot. This is effectively the same plot as @fig-kiosk-model-initial-attempt, only zoomed out so that we can see more.\n\n![Model with *a* = 15 and *b* = 2 plotted to reveal the origin of the plot so as to see the intercept](../../images/fig-initial-model-with-intercept-visible.png){#fig-initial-model-with-intercept-visible width=\"70%\" fig-align=\"center\"}\n\nEven though a temperature of 0°F is well outside our data range, visualizing the intercept helps us understand what changing *a* actually does to the line.\n\nFrom the plot, we can see that the line correctly intersects the y-axis at 15 since *a = 15*. To make the line go through the point cloud, it looks like we need to make the line less steep. The value for *b* determines the steepness of the line. So, let us decrease it from 2 to 1.5 and see what happens.\n\n![Second attempt: model with *a* = 15 and *b* = 1.5](../../images/fig-kiosk-model-second-guess.png){#fig-kiosk-model-second-guess width=\"70%\" fig-align=\"center\"}\n\nMuch better! But can we do even better? We can keep on trying -- we have an infinite number of choices. Can we find the *best* line? What does that even mean?\n\n## What do we mean by the *best* model?\n\nIn the previous section, we tried a few values for *a* and *b* and our second attempt proved to be much better than the first. However, we want the *best* line. But what does that even mean?\n\nWe are after a model that produces *good* predictions. Therefore the difference between the actual value and the model prediction -- called the *residual* -- matters. The lower that difference is the better. @fig-tentative-model-with-residuals-visualized visualizes our previous model (*a* = 15 and *b* = 1.5) with the error or *residual* explicitly marked for three chosen points.\n\n![Model with *a* = 15 and *b* = 1.5 visualized along with the *residual* or error for three chosen points](../../images/fig-tentative-model-with-residuals-visualized.png){#fig-tentative-model-with-residuals-visualized width=\"70%\" fig-align=\"center\"}\n\n::: {.callout-warning title=\"Common Misconception\"}\nThe error or *residual* for a prediction is **not** the shortest distance to the line. It is the **vertical difference** between the observed value and the predicted value. This is because our model predicts the response (customers) and we compare that to the actual temperature for any point.\n:::\n\nIn @fig-tentative-model-with-residuals-visualized, we see that for the point at *temperature* = 62, the actual data had *customers* = 105, whereas our model predicts 108. So the model is off by -3 (if we compute the residual as in @eq-residual-1.\n\n$$\n\\text{residual} = \\text{actual value} - \\text{model prediction}\n$$ {#eq-residual-1}\n\nFor the data point with *temperature* = 68, the model predicts 116, whereas the actual value was 132 for a residual of +16. Finally, for the point where the *temperature* is 76, the model prediction is 129, whereas the actual value was 130. The model did well on this point. In this way, we can compute the *residual* for every point. We are not particularly concerned about the actual sign of the residual. If it is off from the actual, it is off -- does not matter if it is higher or lower. So we remove the sign and talk about the *absolute* residual.\n\n::: {.callout-note title=\"Quick Check\"}\nIf a model predicts 120 customers for a given temperature, and the actual number is 132:\n\n-   What is the prediction error?\n-   Is it positive or negative?\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nThe prediction error is 132 − 120 = 12.\n\nThe error is positive because the actual value is higher than the predicted value.\n:::\n\nIf we draw any line through these points, each line will have low *absolute residuals* for some points and higher ones for others.\n\nWe have many points and so we want an *overall measure of how close the line is to all the points* by considering all the *absolute residuals*. Even if a line is spot on for a few points, it is not very useful if it is wildly off for many others.\n\nAs we did before, we will square the residuals from all the points and add them all up. That is how much error the line/model has when we consider all data points. WHat squaring does is to punish bigger residuals much more than smaller ones. This is a standard theme in statistics.\n\n::: {.callout-warning title=\"What's the deal with squaring?\"}\nWe square errors for the same reason we used squared deviations when defining variance:\n\n-   positive and negative errors should not cancel out\n-   larger errors should count more than smaller ones\n\nLeast squares regression that we study in this course is built on the same logic as variance. In a later chapter, we will see this idea pop when we look at the explanatory power of a model. Hang tight till then!\n:::\n\n::: {.callout-note title=\"Quick Check\"}\nWhy don’t we choose the line that minimizes the **sum of errors** instead of the **sum of squared errors**?\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nPositive and negative errors would cancel out if we summed errors directly.\n\nSquaring errors ensures that all errors contribute positively and that larger errors are penalized more heavily.\n:::\n\nClearly, we want a line that is as close as possible to all the points overall. Honestly, the line in @fig-tentative-model-with-residuals-visualized seems to be pretty good, at least visually.\n\nFor this model (a = 15, b = 1.5), the total of all the squared residuals is: 1,469.\n\nIs there a line that can give an even lower value? What are the values for *a* and *b* for which we get the *lowest* possible total of all the squared residuals?\n\n::: {.callout-note title=\"Key Idea\"}\nSince there is an infinite number of possible choices for *a* and *b*, we need a systematic way to find the values that minimize the total of the squared residuals. Mathematically, this problem can be solved using calculus. Practically, we sit back and let software do this for us.\n:::\n\n::: {.callout-note title=\"Pause & Think\"}\nComplete the sentence:\n\n“A least squares regression line is the line that \\_\\_\\_\\_\\_.”\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nA least squares regression line is the line that minimizes the sum of squared vertical prediction errors.\n:::\n\nThe code below generates the *best* values for *a* and *b* that guarantee that the *sum of squared residuals* is the smallest possible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkiosk |> \n  model_train(customers ~ temperature) |> \n  coef()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) temperature \n      22.06        1.42 \n```\n\n\n:::\n:::\n\n\nWe see two values in the output: *intercept* and *temperature*. The first is *a* and the second is *b*! These are exactly the values of *a* and *b* that minimize the sum of squared residuals.\n\n@eq-kiosk-cust-temp-model-best shows the *best model* for our problem, given that we want a straight line and want to minimize the sum of squared residuals is:\n\n$$\n\\widehat{\\text{customers}} = 22.06 +  1.42\\,\\text{temperature}\n$$ {#eq-kiosk-cust-temp-model-best}\n\nThe optional enrichment topic at the end of the chapter talks about possibilities other than straight lines.\n\nLet us now see if the sum of squared residuals is smaller than what we obtained for our previous choices for *a* and *b*: 1,469.\n\nThe value is: 1,387. Lower, as we expected. But not by much. We had eyeballed very good values for *a* and *b*!\n\nWe found a line that *minimizes the sum of squared residuals*. Technically this is called the *least-squares criterion* and plays a very important role in statistics and data analysis.\n\n::: {.callout-note title=\"Pause & Think\"}\nSuppose tomorrow’s high temperature is 73°F. Your model predicts 124 customers.\n\n-   Does this mean exactly 124 customers will arrive?\n-   What other factors (besides temperature) might cause the actual number to be higher or lower?\n\nWhat does this tell you about what a regression model can—and cannot—do?\n:::\n\n::: {.callout-note icon=\"false\" collapse=\"true\"}\n## Suggested answer\n\n-   No. 124 is only a model prediction. In reality the number of customers may be 124 or higher or lower.\n\n-   The model acts as if only the temperature determines the number of customers. In reality, many other factors play a role too. Perhaps there is a football game in town and that draws people away. Perhaps the humidity is unusually high and people stay away. Many factors play a role in this outcode. Our model uses a simple rule that can potentially perform decently without being accurate.\n\nIn later chapters we will see that we can include more variables in a model and improve its performance.\n:::\n\n## Optional enrichment topic: Beyond straight lines\n\nIn this chapter, we have assumed that a straight line is a reasonable model for the relationship between temperature and customer traffic. In many business settings, this assumption works well: simple models are easy to interpret, easy to communicate, and often good enough for decision-making.\n\nHowever, not all relationships are well captured by a straight line.\n\n### When a straight line may not be appropriate\n\nIn some situations, a scatter plot may reveal patterns such as:\n\n-   curvature (the relationship bends),\n\n-   diminishing returns (the effect of increases slows down), or\n\n-   threshold effects (behavior changes after a certain point).\n\nFor example, at very high temperatures, customer traffic at a beach kiosk might level off or even decline as conditions become uncomfortable. A straight line cannot capture this kind of behavior well, no matter how its slope and intercept are chosen.\n\nIn such cases, forcing a linear model may lead to systematic residual patterns, indicating that the model is missing something important.\n\n### A flexible alternative: LOWESS smoothing\n\nOne way to explore nonlinear relationships is through a technique called LOWESS (short for locally weighted scatter plot smoothing).\n\n@fig-lowess-example shows an example of a LOWESS model using the *mpg* data frame. In this data frame the *displacement* of an engine is related to the highway mileage *hwy*, but not in a linear way. The LOWESS model better describes it than a straight line would. Like the earlier line model, this too can take in a value for *displ* and provide an estimated value for *hwy*.\n\n![LOWESS model for *displacement* vs *cty* in the *mpg* data frame showing how it captures the curvature in the relationship better than a lien can](../../images/fig-lowess-example.png){#fig-lowess-example width=\"70%\" fig-align=\"center\"}\n\nRather than fitting a single global rule like a straight line, LOWESS works by:\n\n-   focusing on a small neighborhood of points around each value of the explanatory variable, and\n\n-   fitting simple local models that adapt to the data in that neighborhood.\n\nThe result is a smooth curve that follows the overall pattern of the data without requiring us to specify a particular functional form in advance.\n\nLOWESS is especially useful for:\n\n-   exploratory analysis,\n\n-   visualizing trends, and\n\n-   diagnosing whether a straight-line model is reasonable.\n\nIt is not intended to replace regression models in all cases, but rather to help us understand the structure of the data.\n\n### Models as choices, not defaults\n\nThe key takeaway is not that linear models are “wrong,” but that models are choices.\n\n-   A straight line is a good choice when the relationship is approximately linear and interpretability matters.\n\n-   A nonlinear model may be a better choice when the data clearly suggest curvature or changing behavior.\n\n-   Flexible tools like LOWESS help us see what the data are trying to tell us before we commit to a specific model.\n\nIn all cases, the central modeling question remains the same:\n\n> Does this model capture the important structure in the data well enough to support the decisions we want to make?\n\nThis mindset—treating models as purposeful approximations rather than automatic formulas—will guide us throughout the rest of this book.\n",
    "supporting": [
      "line-of-best-fit_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}