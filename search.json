[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Business Statistics: A Modeling Approach",
    "section": "",
    "text": "Introduction to Business Statistics\nA Modeling and Regression-Based Approach\nThis text presents a modren, modeling-first approach to business statistics. Rather than a traditional probability-first sequence, we begin with data, relationships, patterns, models, and regression — the tools most relevant to business decision-making in marketing, finance, economics, HR, and analytics.\nYou will learn:\n\nhow to visualize data\n\nhow to describe relationships\n\nhow to build regression models\n\nhow to interpret uncertainty\n\nhow to make data-driven decisions\n\nLet’s get started.",
    "crumbs": [
      "Introduction to Business Statistics"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "This book accompanies the course Introduction to Business Statistics (BQUA2811) at Stillman School of Business. It takes inspiration from Lessons in Statistical Thinking by Daniel Kaplan (LST). LST takes a modeling centric approach and changes the ordering of topics significantly from traditional introductory statistics courses. LST uses almost no matheatics and only provides intuitive explanation for most concepts – which serves a first statistics course for undergraduate business students very well.\nAfter teaching the course for two semesters broadly using the LST approach, I found some need for changes. The following main drivers shaped the changes:\n\nStudents benefit from understanding structure before encountering uncertainty\nA modeling-centric approach helps to provide an overarching course focus\nCourses that depend on BQUA2811 primiraly expect students to be competent in linear regression and related statistics concepts.\n\nI wanted to provide more of a business focus and use more business-related datasets than LST does. More imoortantly, I felt that LST introduces statistical models very early.Even the very first plots – lines and point estimates – that students encounter show confidence bands. Despite enphasizing the difference between mathematical models and statistical models and providing many examples and illustrations from daily life, I found that students had a very hard time understanding the idea of a statistical model.\nI surmised that teaching the idea of models and conceopts relating to uncertainty together made it very chalenging for students. Therefore I decided to make a big change and restrict the initial discussion of models to just the data set used for modeling without bringing up the idea of inference from the model. Once students grasp the idea of a least squares model independent of uncertainty, learn wnat it means to explain variation, and eventually grasp R-squared, we can then layer on uncertainty that arises when we use a model in broader contexts.\nI also learned that even in developing the notion of a model, it is pedagogically very useful to ramp it up gradually starting from mean as the simpest model (when we have no explanatory variables), category means as models when we have a categorical explanatory variable, and finally extending to full-blown least-square models when the explanatory variable is also numerical. Once students grasp the idea of model, it then becomes easier to introduce uncertainty, probability and confidence intervals.\nBecause of this change, I needed the initial plots to only plot lines instead of confidence bandds. However the plotting function from the LSTbook package that accompanies LST only provides bands. I therefore created a package named LSTextras to extend the functionality of LSTbook package in a very small way to enable plotting of point estimates and linear models without confidence bands. I also included several business-related datasets in the package. Combined with the datasets already available in LSTbook and the other packages like ggplot2 that it depends on, students using LSTextras will ave no dearth of data to play with and experiment with the concepts that they learn.\nThe book is organized into 14 modules reflecting this storyline.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/01-overview.html",
    "href": "modules/01-data-visualization/01-overview.html",
    "title": "Module 1: Data, Visualization & Patterns",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, point plots and relationships. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "Module 1: Data, Visualization & Patterns"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/01-overview.html#learning-goals",
    "href": "modules/01-data-visualization/01-overview.html#learning-goals",
    "title": "Module 1: Data, Visualization & Patterns",
    "section": "",
    "text": "Describe data frames, variables, and instances\n\nPerform simple manipulations of data frames using R\nCreate and interpret scatterplots using the point_plot function\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "Module 1: Data, Visualization & Patterns"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/01-overview.html#structure-of-this-module",
    "href": "modules/01-data-visualization/01-overview.html#structure-of-this-module",
    "title": "Module 1: Data, Visualization & Patterns",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nScatterplots\nRelationships between variables",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "Module 1: Data, Visualization & Patterns"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html",
    "href": "modules/01-data-visualization/data-frames.html",
    "title": "1  Data Frames and Variables",
    "section": "",
    "text": "Learning outcomes\nWe store data in an R object called a Data frame. This chapter introduces data frames, explains its key components and teaches you how to get some basic information from them. We will learn more in later chapters.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#learning-outcomes",
    "href": "modules/01-data-visualization/data-frames.html#learning-outcomes",
    "title": "1  Data Frames and Variables",
    "section": "",
    "text": "After working though this chapter, you will be able to:\n\nExplain the term data frame\nExplain what a column and row of a data frame represent\nName the two types of variables that we will deal with in this course\nDistinguish between numerical and categorical variables and provide examples of each\nExplain what the term level of a categorical variable represents, and provide examples\nExplain why a column is also called a variable\nExplain why a row is called an instance or specimen\nDemonstrate the following skills related to R:\n\nload a data frame from a loaded package\nexplain what the pipe operator does\nlook at the first few rows in a data frame\nlook at the last few rows in a data frame\nretrieve only the rows that satisfy certain conditions\nretrieve a subset of the columns of a data frame\nfind the number of rows and columns in a data frame\ncompute the average of a numeric column in a data frame\nuse the count function to compute the number of rows in a data frame for each level of a categorical variable",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#the-r-software-package-for-statistics",
    "href": "modules/01-data-visualization/data-frames.html#the-r-software-package-for-statistics",
    "title": "1  Data Frames and Variables",
    "section": "1.1 The R Software Package for Statistics",
    "text": "1.1 The R Software Package for Statistics\nIn this course, we will be using the R software package for all computations and visualizations. While R offers a great deal of functionality, we will be using a limited subset that suffices for the course topics. In particular, we will be using a specific R package that contains all the functions and data we need for the course. You should install R and RStudio using the instructions provided in Chapter 31. That section also shows you how to use RStudio and how to load a package – specificaly the LSTbook package – into R.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#r-data-frames",
    "href": "modules/01-data-visualization/data-frames.html#r-data-frames",
    "title": "1  Data Frames and Variables",
    "section": "1.2 R Data Frames",
    "text": "1.2 R Data Frames\nIn R, we generally represent data in a simple tabular structure named data frame – a table with rows and columns. Table 1.1 shows an example of some initial rows from the Boston_marathon data frame contained in the LSTbook package.\n\n\n\n\nTable 1.1: Boston Marathon data (first 10 rows).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nname\ncountry\ntime\nsex\nminutes\n\n\n\n\n2022\nEvans Chebet\nKenya\n02:06:51\nmale\n127\n\n\n2021\nBenson Kipruto\nKenya\n02:09:51\nmale\n130\n\n\n2019\nLawrence Cherono\nKenya\n02:07:57\nmale\n128\n\n\n2018\nYuki Kawauchi\nJapan\n02:15:58\nmale\n136\n\n\n2017\nGeoffrey Kirui\nKenya\n02:09:37\nmale\n130\n\n\n2016\nLemi Berhanu\nEthiopia\n02:12:45\nmale\n133\n\n\n2015\nLelisa Desisa\nEthiopia\n02:09:17\nmale\n129\n\n\n2014\nMebrahtom “Meb” Keflezighi\nUnited States\n02:08:37\nmale\n129\n\n\n2013\nLelisa Desisa\nEthiopia\n02:10:22\nmale\n130\n\n\n2012\nWesley Korir\nKenya\n02:12:40\nmale\n133\n\n\n\n\n\n\n\n\nWe first define the term data frame and then explain some terms that the definitoion uses.\n\nData frame: A structured dataset organized as rows and columns, where rows correspond to individual observations and columns correspond to variables, with each column containing values of the same type.\n\nA data frame contains data about some topic. Each row represents an occurence of the topic. For example, Table 1.1 contains data about the results of the Boston Marathon over the years. Each row represents the result for one year, for one gender. We also refer to a row as an observation, or as an instance, or in a medical or biology context, as a specimen.\nEach column represents some attribute of interest about instances. In Table 1.1, we have columns to represent the year of the race, the name, county and sex of the winner and the finishing time.\nIn statistics, we generally refer to columns of a data frame as variables. We explain why in Section 1.3.\nIn the 1880’s, Francis Galton was developing ways to quantify the heritability of traits. As part of this work, he collected data on the heights of adult children and their parents. Table 1.2 shows the initial few rows of that data.\n\n\n\n\nTable 1.2: Galton data (first 10 rows).\n\n\n\n\n\n\nfamily\nfather\nmother\nsex\nheight\nnkids\n\n\n\n\n1\n78.5\n67.0\nM\n73.2\n4\n\n\n1\n78.5\n67.0\nF\n69.2\n4\n\n\n1\n78.5\n67.0\nF\n69.0\n4\n\n\n1\n78.5\n67.0\nF\n69.0\n4\n\n\n2\n75.5\n66.5\nM\n73.5\n4\n\n\n2\n75.5\n66.5\nM\n72.5\n4\n\n\n2\n75.5\n66.5\nF\n65.5\n4\n\n\n2\n75.5\n66.5\nF\n65.5\n4\n\n\n3\n75.0\n64.0\nM\n71.0\n2\n\n\n3\n75.0\n64.0\nF\n68.0\n2\n\n\n\n\n\n\n\n\nIn Table 1.2, each row represents an adult child. The variables in this data frame represent the father’s height (father), mother’s height (mother), sex of the adult child (sex), height of the adult child (height), and the number of children (nkinds) in the family. Each family also has a unique number for each family (family).",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#sec-variables",
    "href": "modules/01-data-visualization/data-frames.html#sec-variables",
    "title": "1  Data Frames and Variables",
    "section": "1.3 Variables",
    "text": "1.3 Variables\nIf we look at a column of a data frame, say the column time in Table 1.1, we see that each row can potentially have a different value in this column. More generally, the values in a column of a data frame can vary from row to row – no wonder we call columns of a data frame variables. Going forward, we will use the term variable to refer to a column of a data frame.\nIn this course, you will be working with many data frames. You should always be sure to understand what a row of a data frame contains information about. In the case of Table 1.1, each row rewpresents the results of one race. In the case of Table 1.2, each row represents one adult child. We use the term unit of measurement to refer to the object or concept about which each row stores information.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#numerical-and-categorical-variables",
    "href": "modules/01-data-visualization/data-frames.html#numerical-and-categorical-variables",
    "title": "1  Data Frames and Variables",
    "section": "1.4 Numerical and Categorical Variables",
    "text": "1.4 Numerical and Categorical Variables\nIn Table 1.1, the variable minutes has values that are numbers. We can such variables numerical variables. On the other hand, the variable sex can take on only the values male and female which are not numeric. Similarly, the variable country can also take on non-numetic values. We call such variables – those that take on non-numerical values categorical. The individual values of a particular categorical variable are called levels. Thus the values mal and female are the levels of the variable sex and the country names like Kenya, Japan and Ethiopia are the levels of the variable country.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#data-frames-in-r",
    "href": "modules/01-data-visualization/data-frames.html#data-frames-in-r",
    "title": "1  Data Frames and Variables",
    "section": "1.5 Data Frames in R",
    "text": "1.5 Data Frames in R\nIn this section we look at the practical side of data frames – using them in R. We will only look at using data frames that are built in to the LSTbook package and mecome available as soon as we load the package LSTbook. If you have not already done so, you should install R and RStudio and learn how o use RStudio using the instructions from Chapter 31.\n\n1.5.1 Viewing the first few rows of a data frame\nIn an RStudio command prompt you can enter the name of the data frame to get a preview of its contents.\n\nBoston_marathon\n\n\n  \n\n\n\nLet us understand the output above. The first row says:\n# A tibble: 175 × 6\nThis says that the data frame (also called tibble) has 175 rows and 6 columns.\nThe second line mentiones the column (variable) names. The third line has sone specific notation within angle brackets below each variable. This is telling us what type of value is stored in each column. -  means that the value in a column is a double, that is, number that could potentially have a fractional part. -  means that the column has character or non-numeric values -  tells us that the column stores a time - and so on.\nIf the data frame has many columns and all of them will not fit in our output width, it will only display the columns that will fit and provide information below about the additional columns. In the present case all columns do fit and so there is not additional information provided.\n\n\n1.5.2 Functions\nYou are likely familiar with the term functions as used in mathematics – you rpovide an input to the function and it performs some computation and provides an output. For example, we might have a function that takes a number as input and produces its square as output. Given the number 5 as input, it will produce the number 25 as its output. Given the number 1.5 as input, it will produce its square 2.25 as its output.\nPictorially we might represent a function as a box that has one or more inputs and produces a single output. (We can also have functions that take no inputs and produce an output, but we will not worry about them now.) See Figure 1.1.\n\n\n\n\n\n\nFigure 1.1: A function that squares its input\n\n\n\nWe might have functions that take more than one input. For example, a function might take two inputs x and y, and produce 23 + 3x + 4y as output. Figure 1.2 shows a function with two inputs.\n\n\n\n\n\n\nFigure 1.2: A function that computes 23+3x+4y where x and y are its inputs\n\n\n\nIn computing, we commonly refer to a function’s inputs as its arguments. Figure 1.3 illustrates this.\n\n\n\n\n\n\nFigure 1.3: We generally refer to a function’s inputs as its arguments – this function has two arguments, x and y\n\n\n\nThe functions we considered in our examples thus far in Figure 1.1, Figure 1.2, and Figure 1.3 have been so simple that we were able to show the actual computation in the box representing the function. Most functions we use in real-life are more complex and we give them evocative names. In such cases, it makes more sense to place the name of the function in the box. Figure 1.4 shows a generirc repesentation of a function named cost with three arguments named arg1, arg2, and arg3.\n\n\n\n\n\n\nFigure 1.4: Generic representation of a function named cost that has three arguments\n\n\n\nWhen we use a function to perform some computation, we are said to invoke the function. We generally invoke a function by using its name and by supplying the argument values in parentheses, as Figure 1.5 shows. We also use the term passing arguments to functions to refer to the act of supplying the inputs to a function. We also refer to the result of a function as its return value. We will also speak of a function as returning something. Figure 1.5 also shows that a function argumnent can be numeric or non-numeric.\n\n\n\n\n\n\nFigure 1.5: Invoking a named function and storing the function’s return value in a variable named total\n\n\n\nIn the next section, we look at using functions to perform computing in R. At that time we will look at a different way of passing arguments to functions.\nAlmost all forms of computing, including statistical computing, make significant use of functions. In this course, many of the functions we use take data frame as input and produce a data frame as output. Let us now look at how we use functions in R to work with data and to perform statistical computations.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#tidy-data",
    "href": "modules/01-data-visualization/data-frames.html#tidy-data",
    "title": "1  Data Frames and Variables",
    "section": "1.6 Tidy data",
    "text": "1.6 Tidy data\nWe encounter tabular data everywhere, each one formatted for the specific purpose it serves. Not all of them lend themselves easily to analysis, especially with software tools. To be suitable for analysis by software toold, they need to follow some minimal set of conventions. Data analysis packages can work very well with tidy data. In a tidy dataset:\n\n** Rows represent single instances:** each row represents exactly one instance of the unit of observation. It should not happen that one row represents a person and another row represents a university\n** All values in a column are of the same type:** each column represents a variable and every row has the same type of value for that variable. That is, if we have a variable named salary then every row represents salary in the same way, for example in dollars and as a number. It cannot be the case that one row represents salary in dollars and another in euros. Or that one row represents salary in dollars and another in thousands of dollars\nCell values are atomic: each cell contains an atomic value – that is a value in a cell cannot be further decomposed while still having some specific meaning for the context. This means that, in a university context,we cannot have a single cell that contains the names or numbers of several courses. We say this because the list of several course numbers can be split up into individual course numbers and these still have specific meaning in a context. What about a cell that contains the name of a product (like ““soap”). That can be split up into the individual letters “s”, “o”, “a”, and, “p”. However, this does not count, since, unlike the course number example, the individual letters do not have any specific context-related meaning.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#using-r-functions",
    "href": "modules/01-data-visualization/data-frames.html#using-r-functions",
    "title": "1  Data Frames and Variables",
    "section": "1.7 Using R functions",
    "text": "1.7 Using R functions\nWe will now look at some common R functions that we will use repeatedly in this course.\n\n1.7.1 nrow\nWe will consider the nrow() function that allows us to find out how many rows a data frame contains. We pass a data frame as argument to the nrow() function and it returns a number specifying how many rows the argument data frame has. For example, to find out how many rows the Boston_marathon data frame has, we can use the following code.\n\nnrow(Boston_marathon)\n\n[1] 175\n\n\nIn the code above, we used the style of function invocation that Figure 1.5 introduced.\n\n\n1.7.2 The pipe operator\nIn this course we also use another method of passing an argument to a function – via pipes. Let us first see the code and then learn how it works. We will invoke the nrow() function again, but using a different approach. The code below does exactly what the earlier code accomplished – finding the number of rows in a data frame.\n\nBoston_marathon |&gt; nrow()\n\n[1] 175\n\n\nIn the code above, we are still passing the Boston_marathon data frame as an argument to the nrow() function, but we are passing it along a pipe which appears in the code as “|&gt;”.\nFigure 1.6 clarifies code.\n\n\n\n\n\n\nFigure 1.6: Using the pipe operator to supply the Boston_marathon data frame as an argument to the norw function\n\n\n\nThe pipe operator “|&gt;” has the effect of injecting whatever appears on its left hand side as an argument to the function on the right hand side. Figure 1.7 makes this very explicit by literally depicting the pipe operator “|&gt;” as a physical pipe.\n\n\n\n\n\n\nFigure 1.7: Seeing the pipeoprtator literally as a pipe though which the argument flows into a function\n\n\n\nGoing forward, we will use this pipe approach extensively, but also mix in the earlier approach in places. You will catch on to our patters pretty easily.\n\n\n1.7.3 ncol\nWe can use the ncol() function to find the number of columns in a data frame.\n\nBoston_marathon |&gt; ncol()\n\n[1] 6\n\n\nThe above shows us that the Boston_marathon data frame has 6 columns or variables.\n\n\n1.7.4 head\nYou can use the head() function to preview the first several rows of a data frame. This time we will use the Births2022 data frame from the LSTbook package.\n\nBirths2022 |&gt; head()\n\n\n  \n\n\n\nThe above previewed the first six rows of Births2022 by default because we did not specify how many rows we wanted. We can specify that explicitly. The following previews the first 10 rows.\n\nBirths2022 |&gt; head(10)\n\n\n  \n\n\n\n\n\n1.7.5 names\nWe use this to find the names of the columns in a data frame.\n\nBirths2022 |&gt; names()\n\n [1] \"month\"           \"dow\"            \n [3] \"place\"           \"paternity\"      \n [5] \"meduc\"           \"feduc\"          \n [7] \"married\"         \"fage\"           \n [9] \"mage\"            \"total_kids\"     \n[11] \"interval\"        \"prenatal_start\" \n[13] \"prenatal_visits\" \"mheight\"        \n[15] \"wt_pre\"          \"wt_delivery\"    \n[17] \"diabetes_pre\"    \"diabetes_gest\"  \n[19] \"hbp_pre\"         \"hbp_gest\"       \n[21] \"eclampsia\"       \"induction\"      \n[23] \"augmentation\"    \"anesthesia\"     \n[25] \"presentation\"    \"method\"         \n[27] \"trial_at_labor\"  \"attendant\"      \n[29] \"payment\"         \"apgar5\"         \n[31] \"apgar10\"         \"plurality\"      \n[33] \"sex\"             \"duration\"       \n[35] \"menses\"          \"weight\"         \n[37] \"living\"          \"breastfed\"      \n\n\n\n\n1.7.6 summarize\nWe often calculate summaries (like the average of a variable) of the data contained in a data frame. We will discuss other examples of summaries – like variance, standard deviation, covariance, and correlation coefficient – at appropriate points in the course.\nThe code below shows how to compute the average of the time variable in the Boston_marathon data frame.\n\nBoston_marathon |&gt; \n  summarize(avg_time = mean(time))\n\n\n  \n\n\n\nIn the above code, we passed the Boston_marathon data frame via a pipe to the summarize function. Within the summarize function, we have used the R function mean to compute the average by passing the variable time as an argument to the mean function.\nWe chose to name the return value from the mean function (the average time) as avg_time. You do not necessarily need to name the summaries we compute in the summarize function, but we strongly recommend it.\nFor most of the summaries we compute we will use the general summarize function. WIthin the summarize function, we use specific functions depending on the kind of summary we are computing. In the previous example, we used the mean function for the average. Later we will see other functions to use within the summarize function.\n\n\n1.7.7 count\nThe count() function comes in handy when we want to count how many occurrences of each distinct value in a column are present in a variable. For example, suppose we want to find how many rows are there for each sex we can do the following.\n\nBoston_marathon |&gt; count(sex)\n\n\n  \n\n\n\nSimilarly, if we wanted to find out how many times each country occurs, we can do this.\n\nBoston_marathon |&gt; count(country)\n\n\n  \n\n\n\nAs you can see, the count() function comes in most handy when a variable is categorical. It also works for numerical variables, but will apply much more rarely in these cases.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#missing-values-nas",
    "href": "modules/01-data-visualization/data-frames.html#missing-values-nas",
    "title": "1  Data Frames and Variables",
    "section": "1.8 Missing values (NAs)",
    "text": "1.8 Missing values (NAs)\nSuppose the HR department of a company has a data frame of information about employees with variables (columns) employee_id, first_name, last_name, base_salary, phone_no, and base_office. It is not essential that we have data on every variabkle for every row. For example, it could be the case that the company does not have a phone_no for some employees and does not have a base_office for some. These are called missing values. A missing value denotes somnething whose value we do not know. A missing value is not the same as a blank character value or a zero numeric value. Blanks and zero are valid and known values for variables. A missing value is different – we do not know the value.\nTable 1.3 shows a data frame with no missing values. In each row, we see values for every variable.\n\n\n\n\nTable 1.3: A data frame with no missing values – in every row, every variable has a value\n\n\n\n\n\n\nchannel\nadvertising_spend_k\nweekly_sales_k\n\n\n\n\nSearch Ads\n28.1\n152\n\n\nSearch Ads\n30.6\n152\n\n\nSearch Ads\n93.4\n369\n\n\nSearch Ads\n51.3\n289\n\n\nRetail Promo\n54.6\n175\n\n\nSearch Ads\n86.6\n342\n\n\nRetail Promo\n46.5\n169\n\n\nRetail Promo\n41.4\n142\n\n\nSearch Ads\n78.1\n322\n\n\nRetail Promo\n41.1\n182\n\n\n\n\n\n\n\n\nOn the other hand Table 1.4 shows a small data frame with missing values.\n\n\n\n\nTable 1.4: A data frame with missing values – the NAs in columns phone_no and base_office represent missing values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nemployee_id\nfirst_name\nlast_name\nbase_salary\nphone_no\nbase_office\n\n\n\n\n10210\nBetty\nChu\n67000\n+1 (777) 555-1212\nNew Jersey\n\n\n24532\nJason\nFingleton\n85000\n+1 (732) 555-1212\nNA\n\n\n56437\nShim\nSung\n76000\nNA\nNew York\n\n\n20320\nRavi\nShankar\n100000\n+91 81487 03210\nShankar\n\n\n67582\nEmily\nWeitz\n87000\nNA\nWeitz",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/data-frames.html#codebook",
    "href": "modules/01-data-visualization/data-frames.html#codebook",
    "title": "1  Data Frames and Variables",
    "section": "1.9 Codebook",
    "text": "1.9 Codebook\nBefore using a data frame, we need to understand it well. That means, at the very least, that we know the unit of observation, the meaning of each variable and the units of measurement where applicable. This information is in the codebook for a data frame. You can use the ? operator for this purpose as the example code below shows.\n\n?Boston_marathon\n\nThis will show the codebook in the bottom right pane of RStudio.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Frames and Variables</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html",
    "href": "modules/01-data-visualization/point-plots.html",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "",
    "text": "Learning outcomes\nThis chapter covers the basics of generating and interpreting scatterplots. charts and interpreting\nThe term statistics often conjures up thoughts of numerical computations. However, before we perform various co mputations, we often get a feel for the data and the underlying patterns by first visualizing the data. In this course we will use only a few standard plots to communicate essential statistical ideas.\nIn fact, use a single function – point_plot – to generate all of our plots. Although R offers numerous, sophisticated plotting features to plot almost anything we can imagine, we will keep things simple and focus only on what we need to support the concepts that our course covers.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#learning-outcomes",
    "href": "modules/01-data-visualization/point-plots.html#learning-outcomes",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "",
    "text": "After working though this chapter, you will be able to:\nDemonstrate these outcomnes related to interpreting scatterplots:\n\ngiven a scatterplot of two numeric variables with the plot’s axes labeled, explain precisely what determines the location of a given point\ngiven a scatterplot of two numeric variables, the data frame on which it is based, and a specific point on the plot, identify the corresponding row in the data frame\ngiven a scatterplot of two numeric variables, the data frame on which it is based, and a specific row in the data frame, identify the corresponding point on the plot\ngiven a scatterplot of one numeric variable on the y-axis against a categorical variable on the x-axis, explain how many distinct possible values exist for the x-axis\ngiven a scatterplot one numeric variable on the y-axis against a categorical variable on the x-axis, explain the speific appearance of the points along the x-axis for each of the category levels\nexplain why the points are jittered along the x-axis when the x-axis has a categorical variable\nGiven a scatterplot of a numeric variable on the y-axis and a caregorical variable on the x-axis, identify a pair of points that would have overlapped if not for jittering\n\nDemonstrate these outcomnes related to generating scatterplots using R data frames:\n\nuse the point_plot function to\n\ngenerate a scatterplot of two numeric variables\ngenerate a scatterplot of one numerical variable on the y-axis against a categorical variable on the x-axis\ngenerate a scatterplot of numeric variables on both axes with the points colored based on a third variable\ndescribe a tilde-expression (a model expression)\ngiven a tilde-expression with one variable on either side of the tilde, state how they relate to the plot axes\ngiven a tilde expression with two variables on the right of the tilde, explain the mappings of the elements of the tidle expression to the elements of the plot\n\n\nDemonstrate these outconmes related to relationships between variables:\n\nGiven a scatterplot of two variables, determine if they have a positive or negative relationship or if they are unrelated.\nClearly explain a visual procedure you would adopt to determine the type of relationship between two numerical variables",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#plotting-the-relationship-between-two-numerical-variables",
    "href": "modules/01-data-visualization/point-plots.html#plotting-the-relationship-between-two-numerical-variables",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "2.1 Plotting the relationship between two numerical variables",
    "text": "2.1 Plotting the relationship between two numerical variables\nThe point_plot() function that we will rely on for plots in this course lives in the LSTbook package. We will be using the LSTextras package. Once we load the LSTextras package (which in turn loads the LSTbook package and all its dependencies), the point_plot() function becomes available for us to use.\nLet us first look at the price_demand data frame that is built into the LSTbook package.\nA hypothetical company has gathered data for a single product. Over time, and across very similar sales areas, the company has charged different prices for the same product, It has recorded the monthly sales for each price. The dataset price_demand contains the price and the corresponding sales quantities. We would like to study if the two variables price and sales_qty are related in any way.\nFirst let us see some rows from the dataset.\n\n\n\n\nTable 2.1: Price Demand data (first 15 rows)\n\n\n\n\n\n\nprice\nsales_qty\n\n\n\n\n11.8\n1260\n\n\n12.1\n1215\n\n\n13.6\n1214\n\n\n12.3\n1298\n\n\n12.4\n1160\n\n\n13.7\n1138\n\n\n12.6\n1174\n\n\n11.2\n1220\n\n\n11.7\n1313\n\n\n11.9\n1283\n\n\n13.3\n1170\n\n\n12.6\n1247\n\n\n12.6\n1283\n\n\n12.3\n1263\n\n\n11.8\n1259\n\n\n\n\n\n\n\n\nThe full dataset has 500 rows. With so many rows, we cannot easily get a handle on the overall pattern by looking at the data directly. Visualizing the data might help us. We will do several things in this section: - generate the plot using R code - understand how each element of the plot is related to the original dataset - understand the various elements of the code so that you can use similar code to generate plots that you many need - interpret the plot\nThe code and the plot appear below. For now, ignore the code. We will discuss it in detail after analyzing the plot itself.\n\nprice_demand |&gt; \n  point_plot(sales_qty ~ price)\n\n\n\n\n\n\n\nFigure 2.1: Price demand relationship\n\n\n\n\n\nFigure 2.1 shows the relationship between price and sales_qty – the two variables in the data frame. Note the following about the plot:\n\nThe plot has two axes – the x-axis along the width and the y-axis along tbe height.\nThe plot shows thge values of price on the x-axis and the values of sales_qty on the y-axis.\nIf you take any single point on the plot, you can drop a vertical line from the point to the x-axis and the point where it intersects the x-axis is the price corresponding to the point.\nSimilarly, if you draw a horizontal line from the point to intersect the y-axis, you get the sales_qty corresponding to that point.\nEach point of the plot corresponds to one row of the dataset. So how many points are on the plot? 500, since the dataset has 500 rows.\nThus given a point on the plot, we should be able to identify a corresponding row of the data frame\nGiven a row of the data frame, we should be able to identify a corresponding point on the plot.\n\n\n2.1.1 Finding the point corresponding to a row of the data frame\nLet us now try to find the point on the plot corrresponding to the first row in Figure 2.1.\nThe first row of the data frame has price = 11.8 and sales_qty = 1260. Therefore, we can find the point by drawing a vertical line from 11.8 on the x-axis to intersect a horizontal line from 1260 on the y-axis. Figure 2.2 illustrates this. We have highlighted the point by enlarging it.\n\n\n\n\n\n\nFigure 2.2: Finding the first row of the price_demand data frame on the plot: price = 11.8, sales_qty = 1260\n\n\n\nSimilarly, let us locate the point corresponding to row 11 (price = 13.3, sales_qty = 1170).\nfollowing the same approach we arrive at Figure 2.3.\n\n\n\n\n\n\nFigure 2.3: Finding the eleventh row of the price_demand data frame on the plot: price = 13.3, sales_qty = 1170\n\n\n\n\n\n2.1.2 Finding the price and sales_qty corresponding to a point on the plot\nWe can reverse the above process to find the variable values (and hence the actual row) corresponding to a point on the plot. We will simply drop a vertical line from the point down to the x-axis to get the price and draw a horizontal line from the point to the y-axis to determine the sales_qty. The figure will look similar to the prior two figures.\n\n\n2.1.3 Dissecting the code\nFigure 2.4 decodes the various elements of the code that we used to generate out first point plot in Figure 2.1. We see that we pass the data frame price_demand to the point_plot function through the pipe operator. We add the tilde expression for the plot as an additional argument.\nFigure 2.5 shows us that the point_plot function maps the variable on the left of the tilde operator to the y-axis of the plot and maps the variable to the right of the tilde expression to the x-axis of the plot.\n\n\n\n\n\n\nFigure 2.4: Passing the price_demand data frame as argument to the point_plot function through the pipe operator\n\n\n\n\n\n\n\n\n\nFigure 2.5: Using a tilde expression to map variables to the axes\n\n\n\n\n\n2.1.4 Interpreting the plot\nFigure 2.6 repeats the plot of price versus sales_qty.\n\nprice_demand |&gt; \n  point_plot(sales_qty ~ price)\n\n\n\n\n\n\n\nFigure 2.6: Price demand relationship (again)\n\n\n\n\n\nWe can see a clear trend showing that, in general, higher prices correspond to lower values of sales quantity.\nTo be sure, this is only a trend and not a strict rule. That is, given any two points, we do not have a guarantee that the point with the higher price will always have a lower sales quantity. However, this will be true for most of the pairs of points. Figure 2.7 illustrates this with some randomly selected pairs of points. We see that most of the lines point down and to the right – meaning that for most pairs of points, the one with higher price has a lower sales_qty. However, we also see a few examples where the opposite is true.\nThis is why we only calls this a trend and not a strict rule about the relationship between price and sales_qty.\n\n\n\n\n\n\nFigure 2.7: Line segments connecting some pairs of points showing that for most pairs of points the point with higher price has lower sales_qty\n\n\n\n\n\n2.1.5 Positive and negative relationships\nIn general, when we discuss the relationship between two numerical variables, if the general trend is that higher values of one correspond to higher values of the other, then we have a positive relationship. On the other hand, is higher values of one variable correspond to lower values of the other then we have a negative relationship.\nWhat kind of relationship does Figure 2.6 show – positive or negative?\nYou got it right if you said negative – higher values of price correspond to lower values of sales_qty.\nFigure 2.8 shows two positively correlated variables. Here, higher values of one variable are generally related to higher values of the other. We used the advertising_sales_channel data frame for this plot.\n\nadvertising_sales_channel |&gt; \n  point_plot(weekly_sales_k ~ advertising_spend_k)\n\n\n\n\n\n\n\nFigure 2.8: Weekly sales and advertising spend have a positive relationship – higher advertising generally has higher weekly sales\n\n\n\n\n\nThe data frame returns_dpo contains hypothetical data on various firms’ daily stock returns and the number of days they take to pay their suppliers. Figure 2.9 shows the relationship between these variables. We cannot spot any trend. Higher or lower values of one variable do not indicate higher or lower values of the other. Hence there is neither a positive relationship nor a negative one.\n\nreturns_dpo |&gt; \n  point_plot(daily_return ~ dpo)\n\n\n\n\n\n\n\nFigure 2.9: Days payment outstanding (dpo) does not have a relationship to the daily stock return\n\n\n\n\n\n\n\n2.1.6 Putting a number on the relationship: The correlation coefficient\nThus far, we have been talking about relationships between variables. Exact sciences like mathematics and statistics like to be precise and assign numbers where possible. Statistics commonly uses the correlation coefficient to quantify the relationship between two numerical variables. This section just introduces the correlation coefficient and shows you how to compute it using R. We will get into the mechanics of the actual computation later in the book.\nThe correlation coefficient can take a value between -1 and +1. A value of 0 signifies a complete absence of relationship. -1 signifies a perfect negative correlation and +1 signifies a perfact positive correlation. We discuss these ideas below.\nWe look at some examples now. Revisiting the price_demand data frame, let us use R to compute the correlation coefficient between the variables price and sales_qty.\n\nprice_demand |&gt; \n  summarize(price_sales_qty_correlation = cor(price, sales_qty))\n\n\n  \n\n\n\nWe are computing a summary and hence use the summarize function (see Section 1.7.6). Since the summary we are computing now is the correlation coefficient, we use the cor function within the summarize function. We have chosen to call the comoputed result price_sales_qty_correlation. We could have named it anything we wanted, but chose the sensible approach of giving it a meaningul name.\nFigure 2.1 had already shown us the negative relationship between price and sales_qty. The correlation confirms it, … and makes it more precise.\nLet us compute the correlation coefficient between advertising spend and the weekly sales from the advertising_sales_channel data frame. From Figure 2.8 we saw that these two variables are positively related. Let us see what the correlation coefficient says.\n\nadvertising_sales_channel |&gt; \n  summarize(ad_sales_corr = cor(advertising_spend_k, weekly_sales_k))\n\n\n  \n\n\n\nSure enough, we see a positive correlation coefficient.\n\n\n2.1.7 Perfect correlation\nConsider Figure 2.10 showing strong, but imperfect, correlation.\n\nprice_demand |&gt; \n  point_plot(sales_qty ~ price)\n\n\n\n\n\n\n\nFigure 2.10: Price and sales_qty are not perfectly correlated – correlation coefficient is -0.75\n\n\n\n\n\nWe have already established that price and sales_qty are negatively related. That is, there is a general pattern that higher values of price have lower values of sales_qty.\nIn Figure 2.10, if we look only at the points corresponding to a particular value of price, say 13, the points along this vertical line range from approximately 1100 to 1320. That is, given a value of price, we cannot be sure about the exact value of the corresponding value of sales_qty. However, knowing price does give us some idea about sale_qty. This shows relationship, but not perfect correlation.\nWhen the correlation coefficient is +1 or -1, we have perfect correlation. In this case, knowing the value of one variable enables us to precisely know the value of the other variable as well. This happens when all the points lie on a straight line instead of being dispersed as a cloud as in Figure 2.10.\n\n\n\n\n\n\nFigure 2.11: Perfect correlation arises when all the points lie perfectly on a straight line – the variables in this plot have a corelation coefficient of +1\n\n\n\nFigure 2.11 shows a situation when all points lie on a perfect straight line. In this case, given a value for one variable, we can determine exactly the value of the other. We can determine it visually or by plugging the value of the known variable into the following equation and calculating the value of the other variable.\n\ntotal_cost = 10000 + 45*units_produced\n\nIn Figure 2.11 the line slopes upward as it goes from left to right. WHen the line on which all points fall slopes down as it goes right, the correlation is still perfect, but the line slopes down as it goes to the right and so we have a correlation coefficien of -1.\n\n\n2.1.8 How the point cloud looks for various correlation coefficients\nFigure 2.12 shows the point plots for various values of correlation coefficients between -1 and +1.\n\n\n\n\n\n\nFigure 2.12: Point plots for various correlation-coefficients (r)\n\n\n\nYou can see the following: - in the plot wigth correlation coefficient = -1 (r = -1), the points align perfectly on a straight line sloping downwards as the x values increase - at +1 they align perfactly on a line sloping upwards as the x values increase - as the coefficient goes from -1 to 0, the points get more scattered away from perfect alignment on a straightline until at 0 there is no pattern whatsoever - as the correlation coefficient increases from 0 to +1, we see that the tendency to align on a straight line increases until perfect alignment at r = 1.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#plotting-a-numerical-variable-against-a-categorical-one",
    "href": "modules/01-data-visualization/point-plots.html#plotting-a-numerical-variable-against-a-categorical-one",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "2.2 Plotting a numerical variable against a categorical one",
    "text": "2.2 Plotting a numerical variable against a categorical one\nIn this course, we will only assign a numerical variable to the y-axis. Thus, if a plot has a categorical variable, it will only occupy the x-axis.\nNumerical and categorical variables differ in how they behave in plots. In Figure 2.8 we have the variable advertising_spend_k on the x-axis. Being a numerical variable, it can potentially take on any value on the x-axis. On the other hand, look at Figure 2.13.\n\n\n\n\n\n\nFigure 2.13: Plotting a categorical variable against a numerical one – using the data frame acct_type_balance (not plotted using the point_plot function)\n\n\n\nHere we have the variable bank_account_type on the x-axis and it can take on only one of two values “Checking” and “Savings”. Thios is why each of the rows in the data frame falls along one of two perfectly vertical assortment of points If the row corresponds to a “Checking” account, it falls on the left set of points and on the right set otherwise.\n\n2.2.1 point_plot and “jittering”\nIf we have two Checking accounts with the same or very similar balance, their points will overlap. Because of this overplotting, the plot in Figure 2.13 does not enable us to look at all the points.\nTo overcome this problem, the point_plot function plots this chart a bit differently. See Figure 2.14.\n\nacct_type_balance |&gt; \n  point_plot(balance ~ bank_account_type)\n\n\n\n\n\n\n\nFigure 2.14: Plot of account type against account bakance using the point_plot function: it jitters the points along the x-axis to reduce overplotting\n\n\n\n\n\nTo enable us to see the points more clearly, the point_plot function randomly “jitters” the points along the x-axis to more clearly separate points that might be overplotted or close together.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#bringing-a-third-variable-into-the-plot",
    "href": "modules/01-data-visualization/point-plots.html#bringing-a-third-variable-into-the-plot",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "2.3 Bringing a third variable into the plot",
    "text": "2.3 Bringing a third variable into the plot\nTwo-dimensional plots can normally accommodate only two variables – one on each axis. However, we can often get more insights if we manage to get more variables into our plots. The point_plot functions allows us to bring one more dimension to our plots through color.\nIn Figure 2.8, we saw the positive relationship between weekly_sales_k and advertising_spend_k in the advertising_sales_channel data frame. That data frame has another variable channel. Each row corresponds to one of two sales channels – Search Ads and Retail Promo. We might want to study separately for each channel the relationship between advertising spend and the weekly sales.\n\nadvertising_sales_channel |&gt; \n  point_plot(weekly_sales_k ~ advertising_spend_k + channel)\n\n\n\n\n\n\n\nFigure 2.15: Determining the color of points based on a third variable – the last variable in the tilde expression for the plot achieves this\n\n\n\nFigure 2.15 inserts, through color, the variable channel into the plot of weekly_sales_k against advertising_spend_k from the advertising_sales_channel data frame. From it we can clearly see that the points corresponding to each sales channel clearly shows the positive relationship that we saw earlier. But this plot also shows that for a given value of advertising_spend_k the Retail Promo channel generally has a comparatively higher weekly_sales_k value. Adding the variable channel through color enabled us to see more into the data than before.\n\n2.3.1 Understanding the tilde expression to add a third variable\nFigure 2.16 shows the tilde expression we used in a point plot of two variables to color the points based on a third variable.\n\n\n\n\n\n\nFigure 2.16: To color the points based on the value of a third variable, we just add it to the end of the tilde-expression after a plus sign\n\n\n\nFigure 2.17 shows another example. In this, we use the mpg data frame which contains data on cars. We plot the city mileage of cars (variable cty) against their engine displacement (variable displ). We color each point based on the class of the vehicle (variable class).\n\nmpg |&gt; \n  point_plot(cty ~ displ + class)\n\n\n\n\n\n\n\nFigure 2.17: The scatterplot shows the relationship between displ and cty from the mpg data frame – the variable class in the tilde expression determines the color of the points",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/01-data-visualization/point-plots.html#bringing-a-fourth-variable-into-play",
    "href": "modules/01-data-visualization/point-plots.html#bringing-a-fourth-variable-into-play",
    "title": "2  Exploring Relationships through Scatterplots",
    "section": "2.4 Bringing a fourth variable into play",
    "text": "2.4 Bringing a fourth variable into play\nWe can bring a fourth variable into play as well through facets. Replacing the tilde expression above with:\ncty ~ displ + class + drv\ngenerates Figure 2.18.\n\n\n\n\n\n\nFigure 2.18: The scatterplot shows the relationship between displ and cty from the mpg data frame – the variable class in the tilde expression determines the color of the points and the variable drv divides the plot into facets\n\n\n\nIn this figure, we use the mpg data frame and generate a basic scatterplot of the rerlationship between the highway mileage (variable hwy) and the engine displacement (variabledispl). The third variable class detrermines the color of the points. The fourth variable drv segments the plots into facets – one for each possible value of drv.",
    "crumbs": [
      "Data, Visualization, and Patterns",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Relationships through Scatterplots</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/02-overview.html",
    "href": "modules/02-variability/02-overview.html",
    "title": "Module 2: Variation",
    "section": "",
    "text": "Learning Goals\nVariation plays a very important role in statistics. Some even go to the extent of viewing the discipline of statistics itself as the study of variation.\nThis module discusses variation generally by visualizing it through violin plots. It then introduces the key statistical concepts of variance and standard deviation and how to compute these using R.",
    "crumbs": [
      "Variation",
      "Module 2: Variation"
    ]
  },
  {
    "objectID": "modules/02-variability/02-overview.html#learning-goals",
    "href": "modules/02-variability/02-overview.html#learning-goals",
    "title": "Module 2: Variation",
    "section": "",
    "text": "Explain what variability in a variable represents\nPlot and interpret violin plots\nIdentify broad characteristics of distribution shapes\nCompute variance and standard deviation of a small set of numbers by hand\nUse R to compute the variance and standard deviation of variables in a data frame",
    "crumbs": [
      "Variation",
      "Module 2: Variation"
    ]
  },
  {
    "objectID": "modules/02-variability/02-overview.html#structure-of-this-module",
    "href": "modules/02-variability/02-overview.html#structure-of-this-module",
    "title": "Module 2: Variation",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nViolin plots\nVariance and standard deviation",
    "crumbs": [
      "Variation",
      "Module 2: Variation"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html",
    "href": "modules/02-variability/violin-plots.html",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "",
    "text": "Learning outcomes\nThis chapter talks about visualizing variability.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html#learning-outcomes",
    "href": "modules/02-variability/violin-plots.html#learning-outcomes",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "",
    "text": "After completing this chapter you will be able to:\nDemonstrate these outcomes related to shapes of distributions:\n\nGiven a distribution as histogram, explain what the height of a bar represents\nGiven a distribution as a violin plot, explain what the width of a violin at any point represents\nGiven a violin plot, identify the point(s) having the highest and lowest densities\nIdentify whether a given distribution shown either as a histogram or as a violin plot or as a density plot is\n\nuniform\nsymmaetric bell shaped\nmulti-modal\nskewed and in which way\n\n\nDemonstrate these outcomes related to generating violin plots using R:\n\nGiven a data frame and a numerical variable, write R code to generate a violin plot to study the distribution of the variable\nExplain the two important parts of the plot area in a violin plot\nIn the plot, identify which part represents the data and whih part represents the annotation\nExplain the tilde espression used to generate violin plots of single variables\nGenerate a violin plot to comnpare the distributions of a single numerical variable corresponding to a categorical variable\n\nWe have already looks at the concept of variable. We have used the term to refer to a column of a data frame. We explained the rationale for the name from the fact that the values in a single column of a data frame vary. That is, not all the rows of a column have the same value. For example, we see in Table 3.1 that the values in the column minutes vary – that is, not all the values in the column are the same. The same is true of every column.\n\n\n\n\nTable 3.1: Boston Marathon data (20 randonmly selected rows).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nname\ncountry\ntime\nsex\nminutes\n\n\n\n\n1904\nMichael Spring\nUnited States\n02:38:04\nmale\n158\n\n\n2015\nCaroline Rotich\nKenya\n02:24:55\nfemale\n145\n\n\n1973\nJon Anderson\nUnited States\n02:16:03\nmale\n136\n\n\n1897\nJohn J. McDermott\nUnited States\n02:55:10\nmale\n175\n\n\n1973\nJacqueline A. Hansen\nUnited States\n03:05:59\nfemale\n186\n\n\n1957\nJohn J. Kelley\nUnited States\n02:20:05\nmale\n140\n\n\n2008\nRobert Kipkoech Cheruiyot\nKenya\n02:07:46\nmale\n128\n\n\n2016\nLemi Berhanu\nEthiopia\n02:12:45\nmale\n133\n\n\n1975\nBill Rodgers\nUnited States\n02:09:55\nmale\n130\n\n\n1912\nMichael J. Ryan\nUnited States\n02:21:18\nmale\n141\n\n\n1942\nBernard Joseph (Joe) Smith\nUnited States\n02:26:51\nmale\n147\n\n\n1985\nLisa Larsen-Weidenbach\nUnited States\n02:34:06\nfemale\n154\n\n\n1929\nJohn C. Miles\nCanada\n02:33:08\nmale\n153\n\n\n1986\nRobert de Castella\nAustralia\n02:07:51\nmale\n128\n\n\n1931\nJames P. Henigan\nUnited States\n02:46:45\nmale\n167\n\n\n1994\nUta Pippig\nGermany\n02:21:45\nfemale\n142\n\n\n1972\nOlavi Suomalainen\nFinland\n02:15:39\nmale\n136\n\n\n2019\nLawrence Cherono\nKenya\n02:07:57\nmale\n128\n\n\n2001\nCatherine Ndereba\nKenya\n02:23:53\nfemale\n144\n\n\n2015\nLelisa Desisa\nEthiopia\n02:09:17\nmale\n129\n\n\n\n\n\n\n\n\nTake care to note that we are not saying that every value in a column has to be distinct. Values can repeat. However we have variety in a column as long as all the values are not the same. Very rarely will you come across data frames in which all the values of a column are the same. In this case the column does not serve any useful purpose for data analysis.\nIn this course, we deal only with variability in numerical variables. Statistics deals with variability in categorical variables as well, but this book does not consider this in any depth.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html#distribution",
    "href": "modules/02-variability/violin-plots.html#distribution",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "3.1 Distribution",
    "text": "3.1 Distribution\nBefore we look at the distribution of variables in a data frame, let us look at variability in sets of numbers to understand the idea of distribution. In this chapter we focus on visualization alone. The next chapter goes into measuring variation precisely.\n\n3.1.1 Uniform distribution\nSuppose we have 1000 numbers with each one being an integer between 1 and 100. Some of the numbers in this data might be 45, 23, 46, 89, 1, 23, 45, 56, and so on. If each number occurs exactly 10 times – that is we have 10 ones, 10 twos and so on up to 10 hundreds, we have what is called as the uniform distrigution. In a uniform distribution, each number in the range of the list occurs the same number of times.\nFigure 3.1 shows a histogram of a set of numbers. A histogram breaks up the whole range of the data points into bins. In our example, the numbers range from 1 to 100. A histogram might break this up into some number of bins. If the number of bins is 10, then the numbers between 1 and 10 will fall unto 1 bin. The numbers between 11 and 20 into the next bin and so on. In this example, the bin-width is 10 because each bin spans a range of size 10. The x-axis of the histogram shows the number values and a bar for each bin. The y axis shows how many of the numbers fall into each bin and so the pattern formed by the heights of the bars helps us to see the distribution of the numbers.\nFigure 3.1 shows numbers that are perfectly uniformly distributed. We have used a bin-width of 1 to have each individual integer fall into its own bin. In a uniform distribution every number in the overall range of the numbers occurs exactly the same number of times.\n\n\n\n\n\n\nFigure 3.1: Perfect uniform distribution – each number between 1 and 100 occurs the same number of times (10 times), as you can see from the height of the bars\n\n\n\nOur current example deals with integers, but in general when we have numerical variables, we will deal with numbers that also have a fractional part. Histograms work in the same way for those too, except that we cannot have one bin for each distinct value as there is potentially an inficite number of values in any range.\nFigure 3.2 shows an example of a set of numbers distributed approximately uniformly. For example, Each number does not occur exactly the same number of times, but they are loosely similar. For example, the number 2 seems to appear around 13 times and the number 25 seems to occur around 18 times. In practice, uniformly distributed numbers will look more like Figure 3.2 than Figure 3.1.\n\n\n\n\n\n\nFigure 3.2: Approximate uniform distribution – each number between 1 and 100 occurs more or less the same number of times\n\n\n\nA numerical variable does not necessarily have to be distributed uniformly. That is each number in the range does not have to occur an equal number of times exactly or approximately. We will now look at some other commmon possibilities.\n\n\n3.1.2 Bell shaped distributions\nFigure 3.3 and Figure 3.4 show two bell shaped distributions. Both have their peaks around the middle of the range between 1 and 100. Which means that numbers closer to the middle – or closer to 50 – occur more frequently than those far away from 50.\n\n\n\n\n\n\nFigure 3.3: Symmetric bell shaped distribution with a peak at around 50\n\n\n\n\n\n\n\n\n\nFigure 3.4: Symmetric bell shaped distribution with a peak at around 50 and higher spread than in Figure 3.3\n\n\n\nBoth Figure 3.3 and Figure 3.4 show bell shaped distributions that are also symmetric. That is, the shape to the left of the peak is approximately similar to the shape on the right.\nSo, both of these are bell shaped and symmetric, and yet they are obviously different? Can you describe how they are different?\nWe see that the spread* of the numbers is quite different. In Figure 3.3 the numbers are more concentrated near the middle than is the case in Figure 3.4. Consequently, the peak in the first figure is higher. For example, the numbers 47 and 51 occur nearly 50 times. In Figure 3.4, the higest frequency is around 35. We call the ends of the bell shae as tails. We see that Figure 3.4 has fatter tails.\nWe have seen three different possibilities for how a set of 1000 numbers in the range 1 to 100 can be distributed. Even though the averages of a set of numbers and even their range might be the same they can still be distributed very differently.\n\n\n3.1.3 Multi-modal distributions\nIn Figure 3.3 and Figure 3.4 we saw distributions that each had a single peak. This does not have to be the case. shows a case where we have two peaks.\n\n\n\n\n\n\nFigure 3.5: Distribution with more than one peak – multi-modal distribution\n\n\n\n\n\n3.1.4 Skewed distributions\nThus far, we have seen distributions in which the shape has always been symmetric – explicitly so in the bell-shaped examples, but also tru for the uniform examples. Figure 3.6 and Figure 3.7 show examples of asymmetric distributions. The first has a long tail on the right and is said to be right-skewed and the second has its tail on the left and is said to be left-skewed.\n\n\n\n\n\n\nFigure 3.6: Asymmetric distribution with a right skew\n\n\n\n\n\n\n\n\n\nFigure 3.7: Asymmetric distribution with a left skew",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html#violin-plots",
    "href": "modules/02-variability/violin-plots.html#violin-plots",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "3.2 Violin plots",
    "text": "3.2 Violin plots\nNow that we have clarified the idea of a distribution let us look at distributions of numerical variables through violin plots. These plots serve the same function as the histograms we have looked at in the previous section, but we can do more. Let us start off by looking at a violin plot of the variable minutes from the Boston_marathon data frame\n\nBoston_marathon |&gt; \n  point_plot(minutes ~ 1, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.8: Violin plot of the minutes variable from the Boston_marathon data frame\n\n\n\n\n\nThe width of the violin at any point on the y-axis shows the relative frequency of data values close to that point. The widest part of the violin occurs at around 145 minutes. This means that the maximum concentration of winning times in the dataset is close to 145 minutes. The plot does not tell us exactly what the frequency is. The width only shows the relative frequency. Looking at where the violin is very narrow, we can say that very few runners took more than 175 minutes to complete the race. Also, relatively low number sof people took less than about 125 minutes. We can see a higher density of points around the broad regions of the violin and a low density in the narrow areas of the violin.\n\n3.2.1 Elements of the plot\nUntil now, the plots we have generated using point plot have only shown the actual points. However, in Figure 3.8 we see two distinct elements:\n\na jittered plot of the individual points\na violin-shaped solid area – an annotation encompassing the points themselves\n\nThe points represent the raw data and the violin adds an annotation that interprets the data in some way. Here, the interpretation is a violin plot that hels us visualize the distribution of the values.\n\n\n3.2.2 Examining the code\nWe examine the code now. In the code for Figure 3.8, we can see two arguments for the point_plot function. The first argument specifies the tilde expression mapping the axes. The second specifies the type of annotation we want. Figure 3.9 shows the code with the two arguments labeled.\n\n\n\n\n\n\nFigure 3.9: The code to generate Figure 3.8 passes two arguments to the point_plot function – a tilde expression and an annotation specification\n\n\n\nFigure 3.10 explains the tilde expression used for the plot. Mapping of minutes to the y-axis conforms to what we did before. This plot deals with just a single variable – we have no variable on the x-axis. When there is no variable on the x-axis, we conventionally that by just using “1” for the x-axis variable.\n\n\n\n\n\n\nFigure 3.10: The tilde expression of Figure 3.9 we place minutes on the y-axis and since the x-axis has no variable, we just state “1”\n\n\n\nFigure 3.11 explains the second argument in generating a violin plot.\n\n\n\n\n\n\nFigure 3.11: We specify that we want a violin annotation",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html#violin-plots-and-distribution-shapes",
    "href": "modules/02-variability/violin-plots.html#violin-plots-and-distribution-shapes",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "3.3 Violin plots and distribution shapes",
    "text": "3.3 Violin plots and distribution shapes\nThe violin plot displays vertical reflectional symmmetry. We can get all the information that the plot conveys just by slicing the plot vertically down the middle and looking at either of the two chunks.\nWhen we first talked about the various distribution shapes like uniform, symmatric bell-shaped, and skewed, we used histograms to convey the ideas. We can deduce the same information from violin plots as well. In the next section, we will see a few more examples. We will discuss distribution shapes in the context of those examples.\n\n3.3.1 More examples\nWe see a few more examples to reinforce the idea. Figure 3.12 shows a violin plot of the variable price from the price_demand data frame.\n\nprice_demand |&gt; \n  point_plot(price ~ 1, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.12: Violin plot of the price variable from the price_demand data frame\n\n\n\n\n\nIf we look at only the left or right half of Figure 3.12, turn it right by 90 degrees and get rid of the display of the individual points, we get Figure 3.13 – a symmetric bell-shaped distribution.\n\n\n\n\n\n\nFigure 3.13: Violin from Figure 3.12 split and rotated 90 degrees right to reveal the distribution in the form we had seen before\n\n\n\nFigure 3.14 shows a violin plot of the balance variable from the acct_type_balance data frame.\n\nacct_type_balance |&gt; \n  point_plot(balance ~ 1, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.14: Violin plot of the balance variable from the acct_type_balance data frame\n\n\n\n\n\nWe can see from Figure 3.12 that price is bell-spaped and unimoda,but not symmetric. It is skewed towards the lower values because the tail is at the bottom. If we slice and rotate it as before then we would say that it is skewed left.\nNext we look at the distribution of advertising_spend_k from the advertising_sales_channel data frame. Figure 3.15 shows the violin plot. From it we see that advertising_spend_k follows a nearly uniform distribution, but not poerfectly so. It is mildly bimodal.\n\nadvertising_sales_channel |&gt; \n  point_plot(advertising_spend_k ~ 1, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.15: Violin plot of the advertising_spend_k variable from the advertising_sales_channel data frame",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/violin-plots.html#comparing-distributions-with-violin-plots",
    "href": "modules/02-variability/violin-plots.html#comparing-distributions-with-violin-plots",
    "title": "3  Using Violin Plots to Visualize Distribution",
    "section": "3.4 Comparing distributions with violin plots",
    "text": "3.4 Comparing distributions with violin plots\nThus far, we have examined distributions of individual variables in isloation. In statistics we often want to compare distributions. For example, in the acct_type_balance data frame, how does the distribution of balance for Checking accounts compare with that for Savings accounts?\n\nacct_type_balance |&gt; \n  point_plot(balance ~ bank_account_type, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.16: Violin plots of the balance variable for different accoiunt types – from the price_demand data frame\n\n\n\n\n\nSince we want to compare the two violins corresponding to the two different account types, we now have bank_account_type on the x-axis and our tilde expression reflects this.\nFigure 3.16 shows several things: - account balances in Savings accounts are generally larger - account balances of Savings accounts are distributed in an almost symmetrical bell shape with the maximum density around $6,000. - account balances of Checking accounts are generally smaller than those of Savings accounts - account balances of Checking accounts are bell shaped, but skewed towards lower values with the maximum density around $5,400 or so.\n\nAs another example of comparing distributions, let us use the advertising_sales_channel data frame to compare the distribution of weekly_sales_k for different channels .\n\n\nadvertising_sales_channel |&gt; \n  point_plot(weekly_sales_k ~ channel, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 3.17: Violin plots of the weekly_sales_k variable for different sales channels – from the advertising_sales_channel data frame\n\n\n\n\n\nFigure 3.17 tells us the following: - weekly_sales_k values are generally lower for the Retail Promo channel - for the Retail Promo channel weekly_sales_k is distributed symmateically with three short peaks - weekly_sales_k values are almost uniformly distributed for the Search Ads channel and have a much larger range than the Retail Sales channel",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Violin Plots to Visualize Distribution</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html",
    "href": "modules/02-variability/variance-and-sd.html",
    "title": "4  Variance and standard deviation",
    "section": "",
    "text": "Learning outcomes\nThis chapter discusses the key statistical concepts of and standard deviation.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#learning-outcomes",
    "href": "modules/02-variability/variance-and-sd.html#learning-outcomes",
    "title": "4  Variance and standard deviation",
    "section": "",
    "text": "After completing this chapter you will be able to:\n\nGiven two sets of numbers with 10 or less nymbers, identify which has higher variance and eplain why\nGenerate a set of numbers with zero variance\nGiven a set of no more than five numbers (at most two digits) compute their variance by hand\nExplain the units of measurement for variance\nExplain the relationship between variance and standard deviation\nExplain the units of measurement for standard deviation\nUse R to compute the variance and standard deviation of a numerical variable of a data frame",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#variable",
    "href": "modules/02-variability/variance-and-sd.html#variable",
    "title": "4  Variance and standard deviation",
    "section": "4.1 Variable",
    "text": "4.1 Variable\nWe use the term “variable” to describe a column of a data frame. Why? Let us make things more concrete. A data frame with data on employees of a company might have a column named “salary” to store the salaries of employees. Salaries of employees vary. That is, we know that not everyone in the company has the same salary (generally speaking). So, it makes eminent sense to call a column a “variable.”\nSimilarly, the same data frame might have another column named “Position” that stores the position of each employee. Some example values might be “Manager”, “Sales associate”, and “IT Specialist.” Here too we see “variability” because not all the values in the column are the same. So the term “variable” applies to columns whether they contain numeric or categorical values. However, for the rest of this document, we concern ourselves only with numerical variables.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#amount-of-variation",
    "href": "modules/02-variability/variance-and-sd.html#amount-of-variation",
    "title": "4  Variance and standard deviation",
    "section": "4.2 Amount of Variation",
    "text": "4.2 Amount of Variation\nMeasuring the amount of variation plays an important role in statistics. If a column has no variation, then we cannot make much use of the information in the column. When a column does have variation, we often want to know the extent of variation in the values of the column. We want to ascribe a specific number – that is, measure the variation in a column. We call this measure the variance of the values in a column. When the numbers in a column are all the same, then we say that the column has no variance – that is, the variance is zero.\nConsider the following three sets of 8 numbers each:Set 1: (1, 2, 2, 1, 2, 1, 2, 1)Set 2: (2, 6, 8, 2, 9, 2, 10, 9)Set 3: (1, 3, 2, 3, 2, 2, 3, 4)Which of the three sets has the highest amount of variation? That is, which set has numbers that differ by the most (among the three sets)?\nCan you order the sets in increasing order of their overall variability?\nOverall, we can see that the numbers in Set 1 are much closer to each other than those in the other two sets. The numbers in Set 2 are the furthest apart from each other and those in Set 3 are in-between. Therefore the order is: Variance of Set 1 &lt; Variance of Set 3 &lt; Variance of Set 2",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#measuring-variation-through-variance",
    "href": "modules/02-variability/variance-and-sd.html#measuring-variation-through-variance",
    "title": "4  Variance and standard deviation",
    "section": "4.3 Measuring Variation through variance",
    "text": "4.3 Measuring Variation through variance\nFrom the foregoing, we see that the variance of a set of numbers is a measure of how much the numbers differ from each other. A set of numbers in which the elements are generally very close together has a low variance and a set that contains numbers that differ from each other by a lot has high variance. Over and above just describing what low and high variance look oike, we would like to assign a specific number to the variance of a set of numbers.\nIn reality we will compute the variance using the R function . We describe two procedures below just to give you a good feel for what the variance represents.\n\n4.3.1 Computing Variance: Method 1 – Using pairwise differences\nWe first look at an intuitive (but impractical) method for computing the variance of a set of numbers. Variance represents how different each member of a set of numbers is from the rest. So the difference between each number and the rest of them plays a central role. Our first approach to computing the variance involves looking at every possible pair of numbers in our set and seing how different the two numbers of the pair are from each other. We then combine all of these pairwise differences into a single number to represent the overall variability inthe set.\n\n4.3.1.1 Steps to compute variance using pairwise differences\nLet us assume that we want to compute the variance of the set of 4 numbers (2, 4, 5, 9)\n\nIdentify each possible pair of numbers from the set.\n(2, 4), (2, 5), (2, 9)\n(4, 5), (4, 9)\n(5, 9)\nThere are 6 such pairs.\nCompute the difference for each pair and square it.\n\n\n\nPair\nDifference\nDifference squared\n\n\n\n\n(2, 4)\n2\n4\n\n\n(2, 5)\n3\n9\n\n\n(2, 9)\n7\n49\n\n\n(4, 5)\n1\n1\n\n\n(4, 9)\n5\n25\n\n\n(5, 9)\n4\n16\n\n\n\nFind the average of the squared differences.\nThe sum of the squared differences is:\n4 + 9 + 49 + 1 + 25 + 16 = 104\nThe average is:\n104 / 6 = 17.33333\nDivide the average from the prior step by 2.\n17.33333 / 2 = 8.66667\nSo, the variance of the set (2, 4, 5, 9) is 8.66667.\n\nWhat are the characteristics of a set of numbers with zero variance? Think a little before reading on.\nIf the variance has to be zero then. the average of the differences also has to be zero, which means each of the differences has to be zero. That in turn means that all the numbers have to be the same. This makes a lot of sense, because if there is zero or no variance, then the numbers do not vary!\n\n\n\n4.3.2 Computing Variance: Method 2 – Using deviations from the mean\nWe can also compute the variance using a different aproach – which also gives the same result as method 1. Most people will use this description of variance. Here, instead of finding the pairwise differences, we see how much each number in our set differs from the average (referred to in statistics as the mean). We then combine these individual differences into a number that represents the overall variance in the set.\nAgain, we use the same set of numbers (2, 4, 5, 9). Let us suppose that these numbers represent the prices of some items in a shop (in the US) and therefore they represent USD amounts.\n\n4.3.2.1 Steps to compute variance using deviations from the mean\n\nFind the average of the numbers.\nThe sum of the numbers is:\n2 + 4 + 5 + 9 = 20\nThe average is: 20 / 4 = 5\nFind the squared deviations from the mean for each number.\n\n\n\nNumber\nDifference from mean\nDifference squared\n\n\n\n\n2\n3\n9\n\n\n4\n1\n1\n\n\n5\n0\n0\n\n\n9\n4\n16\n\n\n\nDivide the total of the squared deviations from the prior step by (n − 1), where n is the number of elements.\nThe sum of the squared deviations is: 9 + 1 + 0 + 16 = 26\nDividing by (n − 1): 26 / 3 = 8.66667\n\n\n\n\n4.3.3 Unit of measurement for variance\nWhenever we measure anything, we measure it in terms of some unit of measurement. For example, we might use meter as the unit of measurement for people’s heights, and USD as the unit of measurement for US company profits. In fact, in any data frame, whenever we see a numerical variable, we need to be aware of its unit of measurement.\n\n\n4.3.4 Computing Variance: Preferred Method – Using R!\nWe have a data frame named variance_example that has a variable num with the values (2, 4, 5, 9). We can use the following R code to compute its variance.\n\nvariance_example |&gt; \n  summarize(num_var = var(num))\n\n\n  \n\n\n\nHere is another example using the mpg data frame to compute the variance of one of its variables.\n\nmpg |&gt;\n  summarize(hwy_var = var(hwy))\n\n\n  \n\n\n\nIn the previous section we computed the variance of a set of numbers that represented USD values. What are the units for variance?\nIn the two methods where we computed the variance, we found some differences in each step. These differences were differences between USD amounts and hence the differences have units of USD. We then squared the differences. The unit of measurement for the squares wiuld be USD-squared. We then averaged these in one approach and divided by a number in the other approach to get the variance. Therefore, variance also has USD-squared as its unit of measurement.\nIn general, when we compute the variance of a variable measured in some units (say, u, the variance has u-squared as its unit of measurement. That is, if we have a variable expressed in meters, then the variance of the variable will be in meters-squared.\nWe generally have a clear mental notion of units like USD and meters, but not units like USD-sauared or meters-squared.",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/02-variability/variance-and-sd.html#another-measure-of-spread-standard-deviation",
    "href": "modules/02-variability/variance-and-sd.html#another-measure-of-spread-standard-deviation",
    "title": "4  Variance and standard deviation",
    "section": "4.4 Another measure of spread: Standard Deviation",
    "text": "4.4 Another measure of spread: Standard Deviation\nWe have seen that variance has squared units and we cannot easily relate to these. Statisticians have therefore given us another measure of variability that has the same units as the original variable. This is the standard deviation and we compute it as the square-root of the variance.\nIf a variable is measured in inches, its variance has inches-squared as its unit. However, when we compute its square root, we get the standard deviation with inches as units.\nWe can now mentally relate the original variable values to the standard deviation and therefore statisticians use this measure vey widely. This is not to say that variance is not widely used as well. It is, and we will use it extensively in this course as well.\n\n4.4.1 Computing standard deviation using R\nWe can use the sd function inside the sumamrize function to compute the standard deviation\nLet us comnpute the standard deviation of the city mileage (variable cty) in the mpg data frame.\n\nmpg |&gt;\n  summarize(cty_sd = sd(cty))\n\n\n  \n\n\n\nJust to confirm that the computed standard deviation is in fact the square root of the variance let us compute both.\n\nmpg |&gt;\n  summarize(cty_var = var(cty), cty_sd = sd(cty))",
    "crumbs": [
      "Variation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variance and standard deviation</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/03-overview.html",
    "href": "modules/03-introduction-to-models/03-overview.html",
    "title": "Module 3: Introduction to models",
    "section": "",
    "text": "Learning Goals\nThis module gently introduces the idea of a model, and then discusses the simplest possible model – the mean. It also introduces the concepts of outcome and explanatory variables. It extends the idea of tne mean as the model with no explanatory variable to cover the case of a model with one categorical explanatory variable and shows that in this case the model is the category mean.\n**After completing this module, you will be able to:",
    "crumbs": [
      "Introduction to models",
      "Module 3: Introduction to models"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/03-overview.html#learning-goals",
    "href": "modules/03-introduction-to-models/03-overview.html#learning-goals",
    "title": "Module 3: Introduction to models",
    "section": "",
    "text": "Explain the term model, generally and in our context\nIntuitively explain why the mean is the best model in the absence of any other information",
    "crumbs": [
      "Introduction to models",
      "Module 3: Introduction to models"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/03-overview.html#structure-of-this-module",
    "href": "modules/03-introduction-to-models/03-overview.html#structure-of-this-module",
    "title": "Module 3: Introduction to models",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nMean as a model\nCategory mean as model",
    "crumbs": [
      "Introduction to models",
      "Module 3: Introduction to models"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models.html",
    "href": "modules/03-introduction-to-models/path-to-models.html",
    "title": "5  The Simplest Model",
    "section": "",
    "text": "Learning outcomes\nThis chapter helps you to dip your toes into the vast and important topic of models.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models.html#learning-outcomes",
    "href": "modules/03-introduction-to-models/path-to-models.html#learning-outcomes",
    "title": "5  The Simplest Model",
    "section": "",
    "text": "After completing this chapter you will be able to:\n\nExplain the term model from a general perspective, and from the perspective of statistics\nExplain why, given the values of only one variable and in the absence of any other information, the mean is the best model\nExplain in what sense the mean is the best model\nGiven a model equation, identify the outcome variable and the explanatory variable(s)\nDescribe the terms outcome variable and **explanatory* variable\nMatch the terminology between the two pairs (outcome variable, explanatory variable) and (independentvariable, dependent variable)\nGiven a model with a single numerical explanatory variable and a value for the explanatory variable, compute the model value.\nExplain why we place a hat on the outcome variable in a model expression\nExplain why we call a model’s output an estimate\nList and describe the two uses of models covered in this chapter",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models.html#companies-commit-when-they-make-decisions",
    "href": "modules/03-introduction-to-models/path-to-models.html#companies-commit-when-they-make-decisions",
    "title": "5  The Simplest Model",
    "section": "5.1 Companies commit when they make decisions",
    "text": "5.1 Companies commit when they make decisions\nGood decision-making lies at the heart of effective business management.\nIn practice, companies must commit to decisions before they know how it wil play out in the real-world. Many important business decisions cannot be revised freely once they are made. Examples include setting prices, determining how much inventory to stock, choosing staffing levels, or deciding where to locate a factory.\nOnce a decision is made, it influences many individual events that unfold over time.\nConsider pricing. A company sets a single price for a product, and then thousands of customers independently decide whether or not to buy at that price. One decision by the firm affects many separate outcomes.\nThis has an important implication: we cannot judge the quality of a decision based on a single outcome.\nSuppose a company sets a price of $10 and the first customer who walks in buys the product. Was $10 the right price? We cannot really say. That single outcome may simply be good luck. If we observe the decisions of 1,000 customers, however, a clearer picture begins to emerge. Over many independent events, good decisions tend to perform well on average.\nIn business, then, decisions must be made in advance, under uncertainty, and their quality must be judged by how well they balance out across many realizations and not by whether they happen to succeed in one particular instance.\nThis perspective will guide how we think about models in this chapter. A good model is not one that gets lucky once. It is one that performs reliably across many possible outcomes.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models.html#sec-mean-game",
    "href": "modules/03-introduction-to-models/path-to-models.html#sec-mean-game",
    "title": "5  The Simplest Model",
    "section": "5.2 Let’s play Are you ready to commit?: Deciding under uncertainty",
    "text": "5.2 Let’s play Are you ready to commit?: Deciding under uncertainty\nLet us play a game that captures an important feature of real business decisions: you must commit to a choice before uncertainty is resolved.\nSuppose I have the following set of 10 numbers:\n4, 5, 7, 1, 8, 4, 3, 9, 5, 4\nYou should think of this set as representing possible outcomes—such as sales on a given day, demand from a customer, or the cost of a transaction. You do not know which value will occur next.\nHere are the rules of the game:\n\nYou commit to a single number. This is your guess, and you must choose it before any outcomes are revealed. You are not allowed to change it later.\nI then randomly select one number from the set and compute the difference between your committed number and the selected value.\nI square the difference to obtain the penalty for that round.\nWe repeat steps 2 and 3, twenty times, and add up all the penalties.\n\nThe result of step 4 is your total penalty.\nYour goal is to keep this total penalty as small as possible.\nLet us look at one simulated play.\n\n\n\ndecision =\n5\n\n\n\n\n\n\nrandom_choice\ndiff\npenalty\n\n\n\n\n7\n-2\n4\n\n\n1\n4\n16\n\n\n4\n1\n1\n\n\n1\n4\n16\n\n\n7\n-2\n4\n\n\n3\n2\n4\n\n\n3\n2\n4\n\n\n5\n0\n0\n\n\n1\n4\n16\n\n\n4\n1\n1\n\n\n6\n-1\n1\n\n\n4\n1\n1\n\n\n6\n-1\n1\n\n\n4\n1\n1\n\n\n3\n2\n4\n\n\n1\n4\n16\n\n\n3\n2\n4\n\n\n4\n1\n1\n\n\n1\n4\n16\n\n\n1\n4\n16\n\n\n\n\n\n\nTotal penalty =\n127\n\n\n\nLet us see another play with a different decision.\n\n\n\ndecision =\n7\n\n\n\n\n\n\nrandom_choice\ndiff\npenalty\n\n\n\n\n1\n6\n36\n\n\n7\n0\n0\n\n\n1\n6\n36\n\n\n3\n4\n16\n\n\n2\n5\n25\n\n\n2\n5\n25\n\n\n2\n5\n25\n\n\n6\n1\n1\n\n\n5\n2\n4\n\n\n3\n4\n16\n\n\n4\n3\n9\n\n\n2\n5\n25\n\n\n1\n6\n36\n\n\n2\n5\n25\n\n\n2\n5\n25\n\n\n2\n5\n25\n\n\n4\n3\n9\n\n\n4\n3\n9\n\n\n6\n1\n1\n\n\n5\n2\n4\n\n\n\n\n\n\nTotal penalty =\n352\n\n\n\nAnd another:\n\n\n\ndecision =\n2\n\n\n\n\n\n\nrandom_choice\ndiff\npenalty\n\n\n\n\n7\n-5\n25\n\n\n6\n-4\n16\n\n\n2\n0\n0\n\n\n2\n0\n0\n\n\n4\n-2\n4\n\n\n2\n0\n0\n\n\n2\n0\n0\n\n\n3\n-1\n1\n\n\n1\n1\n1\n\n\n4\n-2\n4\n\n\n3\n-1\n1\n\n\n6\n-4\n16\n\n\n1\n1\n1\n\n\n2\n0\n0\n\n\n2\n0\n0\n\n\n3\n-1\n1\n\n\n1\n1\n1\n\n\n4\n-2\n4\n\n\n7\n-5\n25\n\n\n1\n1\n1\n\n\n\n\n\n\nTotal penalty =\n101\n\n\n\nAnd, finally:\n\n\n\ndecision =\n8\n\n\n\n\n\n\nrandom_choice\ndiff\npenalty\n\n\n\n\n4\n4\n16\n\n\n7\n1\n1\n\n\n4\n4\n16\n\n\n4\n4\n16\n\n\n4\n4\n16\n\n\n2\n6\n36\n\n\n7\n1\n1\n\n\n1\n7\n49\n\n\n4\n4\n16\n\n\n2\n6\n36\n\n\n1\n7\n49\n\n\n4\n4\n16\n\n\n2\n6\n36\n\n\n4\n4\n16\n\n\n4\n4\n16\n\n\n6\n2\n4\n\n\n4\n4\n16\n\n\n7\n1\n1\n\n\n6\n2\n4\n\n\n4\n4\n16\n\n\n\n\n\n\nTotal penalty =\n377\n\n\n\nOf the four, plays, the third play had the lowest total penalty and emerged as the best.\nHowever, can we do even better? What approach will help us to reduce this total difference or error?",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models.html#average-or-mean-as-the-decision-in-the-long-run",
    "href": "modules/03-introduction-to-models/path-to-models.html#average-or-mean-as-the-decision-in-the-long-run",
    "title": "5  The Simplest Model",
    "section": "5.3 Average or mean as the decision in the long run",
    "text": "5.3 Average or mean as the decision in the long run\nIt turns out that when we have a large number of tries, we will get the best results, that is the lowest penalty when we always guess the average of the numbers. In this case the average is 3.5. Here are the results of one trial.\n\n\n\ndecision =\n3.5\n\n\n\n\n\n\nrandom_choice\ndiff\npenalty\n\n\n\n\n2\n1.5\n2.25\n\n\n5\n-1.5\n2.25\n\n\n4\n-0.5\n0.25\n\n\n2\n1.5\n2.25\n\n\n3\n0.5\n0.25\n\n\n3\n0.5\n0.25\n\n\n7\n-3.5\n12.25\n\n\n1\n2.5\n6.25\n\n\n5\n-1.5\n2.25\n\n\n4\n-0.5\n0.25\n\n\n6\n-2.5\n6.25\n\n\n2\n1.5\n2.25\n\n\n3\n0.5\n0.25\n\n\n4\n-0.5\n0.25\n\n\n1\n2.5\n6.25\n\n\n5\n-1.5\n2.25\n\n\n3\n0.5\n0.25\n\n\n7\n-3.5\n12.25\n\n\n4\n-0.5\n0.25\n\n\n3\n0.5\n0.25\n\n\n\n\n\n\nTotal penalty =\n59\n\n\n\nWe can see that the total error is smaller than any of the others. What we have said above does not mean that we are guaranteed to get the smallest total error if we commit to the average. Clearly, if someone were lucky enough that the random picks all ended up exactly equal to their decision just by chance, then their total error will be zero! But this is rxtremely unlikely. Also, it is possible that most of the random picks end up close to the decision. Then too the total penalty can be lower than what we would obtain by choosing the average as the decision.\nHowever, those occurrences are unlikely with a large number of trials, as happens in business where we have numerous customers, shipments, and such. Statistically and realistically, the best course would be to pick the average.\nIf we repeat this game a very large number of times with different choices for decision, we will get a total penalty each time.\nFigure 5.1 shows the result of 10,000 plays of the game.\n\n\n\n\n\n\nFigure 5.1: Average of total_penalty vs various values for decision based on 10,000 plays of the game – shows that the lowest total penalty overall occurs when decision equals the mean of the numbers\n\n\n\nNow you know why I titled the chapter The simplest Model!",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models.html#real-world-connection-planning-for-customer-demand",
    "href": "modules/03-introduction-to-models/path-to-models.html#real-world-connection-planning-for-customer-demand",
    "title": "5  The Simplest Model",
    "section": "5.4 Real world connection: Planning for Customer Demand",
    "text": "5.4 Real world connection: Planning for Customer Demand\nHow does this small game relate to the real world? Let us consider a business example.\nConsider a company that operates a customer support center. The company must decide how many customer support agents to schedule.\n\n5.4.1 What the company must do\n\nThe staffing decision must be made in advance\nOnce the schedule is set, it cannot be easily changed\nThe company must choose one number to use for many days\n\n\n\n5.4.2 What the company knows\n\nThe company has historical data on the number of calls received per day. For example, 420, 340, 510, 450, 395, 470, 650, …\nSome days are busy, others are quiet\nAt this point, the company does not have data to explain why call volume changes or to predict it. It simply observes that call volume varies\n\n\n\n5.4.3 Why the decision matters\n\nIf too few agents are scheduled:\n\ncustomers wait longer\nservice quality suffers\n\nIf too many agents are scheduled:\n\nagents sit idle\nlabor costs increase\n\n\n\n\n5.4.4 How the company measures mistakes\n\nBeing slightly wrong is not very costly\nBeing very wrong is much more costly\nThe company therefore measures the cost of a decision as:\n\nthe square of the difference between planned calls and actual calls\n\n\n\n\n5.4.5 How the decision is evaluated\n\nThe decision is not judged by a single day\nA decision that works well once may simply be lucky\nInstead, the company looks at performance across many days\nDaily call volume varies, but the staffing decision stays the same\n\n\n\n5.4.6 The conclusion\n\nThe company must choose a single number to plan for\nThe number that minimizes total long-run cost is the average of past daily call volumes\nChoosing a smaller number leads to frequent large shortages\nChoosing a larger number leads to frequent overstaffing\nThe average balances these errors over time\n\n\n\n5.4.7 Why this matters\n\nThe goal is not to be exactly right on any one day\nThe goal is to make a decision that performs well on average\nThis is why the average is the best choice when no additional information is used\n\nThe commitment game we just played follows this same logic: one decision, many outcomes, and performance judged over repeated realizations.\nYou should take away a few things from this chapter: - When we have to estimate something, the average serves as a good basis, unless we are told additional things that can help us to narrow down our estimate - Much of statiustics",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models.html#optional-enrichment-topic-how-does-skew-affect-the-situation",
    "href": "modules/03-introduction-to-models/path-to-models.html#optional-enrichment-topic-how-does-skew-affect-the-situation",
    "title": "5  The Simplest Model",
    "section": "5.5 Optional enrichment topic: How does skew affect the situation?",
    "text": "5.5 Optional enrichment topic: How does skew affect the situation?\nWe had used the numbers:\n4, 5, 7, 1, 8, 4, 3, 9, 5, 4 \nin our game. That set has numbers across the entire range. Is it the case that the average performed well because of this? Could it be that the average will not perform well if most of the numbers fall within in a small range and a few extreme cases tend to push the average up?\nFor, example, let us take the following set of numbers:\n2, 1, 2, 1, 3, 3, 2, 1, 10, 9\nThe mean is 3.4 – almost the same as the previous list of numbers we had used. However, this set has eight of its numbers less than or equal to 3. Perhaps a decision below the average will work better?\nLet us play the game 10,000 times and see which decision performs best.\nFigure 5.2 shows the results. We see that the mean is still the best decision.\n\n\n\n\n\n\nFigure 5.2: Average of total_penalty vs various values for decision based on 10,000 plays of the game – shows that the lowest total penalty overall occurs when decision equals the mean of the numbers\n\n\n\nWhy does this happen? Choosing the average definitely leads to small penalty increases most of the time. However, if we went below the average, the relatively rare cases in which the ramdom choice is high, it incurs a very large penalty and that offsets any benefits of going below the average, because we square the difference and that amplifies the error.\nWhat if we did not square the error and instead treated the absolute difference as the penalty. In this case, the median is the best choice. Pretty cool result if you as me! We will not go further into that topic, but it has its applications. In most practical applications, we use the sqaured-difference as the penalty.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/path-to-models.html#optional-enrichment-topic-removing-outliers-in-skewed-distributions",
    "href": "modules/03-introduction-to-models/path-to-models.html#optional-enrichment-topic-removing-outliers-in-skewed-distributions",
    "title": "5  The Simplest Model",
    "section": "5.6 Optional enrichment topic: Removing outliers in skewed distributions",
    "text": "5.6 Optional enrichment topic: Removing outliers in skewed distributions\nContinuting from the prior discussion, when faced with a skewed distribution we have a peculiar situation where our intuition tells us to make a decision where most points lie, but cold computation tells us otherwise.\nOne may feel that it makes no sense to choose the mean and accumulate a slightly higher penalty in every event just to balance out the huge penalties that come rarely. But that is what makes sense when we use the quadrative penalty function.\nIf the penalty is quadratic, but we still want to not decide on the average, then we can eliminate the extreme points and reduce the skew. Then we can use the average as before. From a practical viewpoint what that would mean is that we take steps to avoid extreme events by some means.\nCan you think of an experiment to compare removing outliers and not removing them in the context of skewed distriubutions to see if using the mean in both cases reduces the total penalty significantly?\nYou can delve deeper into this topic if you are interested.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Simplest Model</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/category-means.html",
    "href": "modules/03-introduction-to-models/category-means.html",
    "title": "6  Category Means",
    "section": "",
    "text": "6.1 Staffing kiosks at malls and beaches\nIn this chapter, we extend the idea of the average as the best model a little further. In the previous chapter, we were given only a set of numbers and had to make a decision without any additional information. Our decision had to be the same in all situations.\nWe consider a case where we are given a hint before we make a decision.\nIn this scenario, a company operates sales kiosks in two different locations, which we will call Beach and Mall. The company has historical data for several days showing how many customers arrived at each kiosk for service. (We will assume data from 10 days for ease of presentation. The exact number of days is not important for our discussion.)\nThe company uses this information to decide how many workers to assign to each kiosk. The key difference from the previous chapter is that the decision no longer has to be the same everywhere. Instead, the company can make one decision for the Mall kiosks and a different decision for the Beach kiosks, using the information it has about location.\nHere is the data:\nIn this scenario, we need to decide an a customer level to use for determininbg staff level. If we are given only the data in column 1, then based on the previous chaopter, our best choice is the average. However, if we are given the entire table, then we have additional information and can determine a different number for each kiosk. For the Mall kiosk, we can use the average of the customers column for only the Mall rows and similarly for the Beach kiosk.\nIn this case, having additional information helped us to make a better decision. The average number of customers is quite different for the two kisoks. Therefore, going with the overall average will not work very well for either kiosk. Using the specific kiosk means will fare much better.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Category Means</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/category-means.html#staffing-kiosks-at-malls-and-beaches",
    "href": "modules/03-introduction-to-models/category-means.html#staffing-kiosks-at-malls-and-beaches",
    "title": "6  Category Means",
    "section": "",
    "text": "customers\nkiosk\n\n\n\n\n89\nMall\n\n\n95\nMall\n\n\n131\nMall\n\n\n101\nMall\n\n\n103\nMall\n\n\n134\nMall\n\n\n109\nMall\n\n\n75\nMall\n\n\n86\nMall\n\n\n91\nMall\n\n\n177\nBeach\n\n\n151\nBeach\n\n\n152\nBeach\n\n\n143\nBeach\n\n\n123\nBeach\n\n\n194\nBeach\n\n\n155\nBeach\n\n\n81\nBeach\n\n\n161\nBeach\n\n\n126\nBeach\n\n\n\n\n\n\n\n\nkiosk\navg_customers\n\n\n\n\nMall\n101\n\n\nBeach\n146",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Category Means</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/category-means.html#optional-challenge",
    "href": "modules/03-introduction-to-models/category-means.html#optional-challenge",
    "title": "6  Category Means",
    "section": "6.2 Optional challenge",
    "text": "6.2 Optional challenge\nCan you think of an experiment based on the one we did in the previous chapter to compare the performance of the overall mean approach and the kiosk mean approach?",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Category Means</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/intro-to-models.html",
    "href": "modules/03-introduction-to-models/intro-to-models.html",
    "title": "7  What is a model?",
    "section": "",
    "text": "7.1 Mathematical models\nNow that we have looked at the overall mean and category means as good rules for decisions (in the absence of more intofmation), we are ready to learn about models.\nPeople use the word model in many senses. In one of the more common usages, a model is an approximate representation of something for some purpose.\nFigure 7.1 and Figure 7.2 show the image of a house and a miniature model of it. The miniature is a good representation of the house in some respoects. For example, it depicts the overall shape and some exterior details quite accurately. If someone were about to build a new house and the architect showed them this model, they would get a good idea about the overall external appearance of the house and the color scheme as well. However, this model differs from the actual house in many ways as well – one cannot actually step inside it! We cannot get a feel for how roomy the interios=r will feel. We do not know how the house will look in its actual surroundings. It is accurate in some respects and inaccurate in others.\nShould we consider the model in Figure 7.2 good? In the words of statistician George E. P. Box, “All models are wrong. But some are useful for certain purposes.”\nAll models are wrong in the sense that they are only representations. However, as the miniature model of the house in Figure 7.2, they can still be useful for some purpose.\nWe will be building many models in this course and should remember the folloiwng:\nHow to apply this philosophy:\nIn this course, we will be building a specific kind of models – statistical models. However, before getting to statistical models, we will consider some simpler mathematical models. The term mathematical model can connote many different things. Let us clarify the sense in which we use the term. Suppose I have a data frame homes with data about many homes. For each home, we have its age, floor-area_sqft, number_of_bedrooms, number_of_bathrooms, and price. Here are the initial rows in the data frame.\nhomes |&gt; \n  head()\nWe might build a model to determine the value of price given the values of one or more of the other variables. In this case, let us use the variable floor_area_sqft. Remember, this is a model and so we do not expect it be be exact. We do not aim for the model to determine the actual price, but only to compute a good estimate of the price. (We have obviously simplified the situation for pedagogical purposes. House prices depend on many other factors. But that need not hold us back. You will still learn the underlying concepts.).\nWhat might the model look like? Equation 7.1 shows the model. For now, do not worry about how we arrived the model.\n\\[\n\\widehat{\\text{price}} = 61{,}399 + 394\\,\\text{floor\\_area\\_sqft}\n\\tag{7.1}\\]\nIn the model, we have placed a hat over price because the model computes not the actual price of a home with the specified floor area and instead only computes an estimate. In statistics, we place a hat over quantities that estimate other quantities.\nEquation 7.1 shows a linear model. We call it linear because all variables are only raised to the fist power. We have not squared or raised a variable to any other power. In this course we build only linear models.\nWe call the equation that Equation 7.1 shows a model function.\nIn our context, we use the term model to denote a mathematical equation of the kind that Equation 7.1 shows. Our models help us to determine the value of a variable in a data frame for any given instance. Specifically, in the house price example that we just considered, the model in Equation 7.1 enables us to estimate the price of a home, given its floor area.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>What is a model?</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/intro-to-models.html#mathematical-models",
    "href": "modules/03-introduction-to-models/intro-to-models.html#mathematical-models",
    "title": "7  What is a model?",
    "section": "",
    "text": "7.1.1 Response and Explanatory variables\nWe call the variable whose value the model estimates as the response variable. The variables based on which the model estimates the reponse variable are called explanatory variables. We can also refer to the response variable as the dependent variable, and explanatory variables as independent variables.\n\n\n7.1.2 Using the model\nThe house in the fifth row of the data frame has floor area of 1480. Its actual price is $553,000. What price does the model estimate? Well, we can plug the floor area into the model – Equation 7.1 – and find out. If we plug it in, we get $645,999. The model overestimates this price.\nThe house in the fourteenth row of the data frame has floor area of 1293 Its actual price is $561,000. What price does the model estimate? Well, we can plug the floor area into the model – Equation 7.1 – and find out. If we plug it in, we get $572,134. Quite close.\nYou should try out some more cases and wee how the model does. We will learn a lot more about such models, including precisely measuring their quality as we go forward.\nI won’t blame you if you are wondering “We already have the prices in the data frame. Why do we need a model to calculate these?”\nConsider again the home in the fifth row. Why did the model overestimate the price? Well, you should consider that this particular home was not the only one whose floor area was in the vicinity of 1480. The dataset has other homes with similar floor areas and their prices vary. If we look at homes that have floor areas between 1430 and 1530, we see that their proces range from $553,000 to $647,000. In fact, many homes below 1480 square feet in floor area have higher prices. How could that be? Well, variables other than floor area play a role in price too. So our model makes an estimate based on the whole data frame.\nThe next section addresses two main uses of models.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>What is a model?</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/intro-to-models.html#purpose-of-a-model",
    "href": "modules/03-introduction-to-models/intro-to-models.html#purpose-of-a-model",
    "title": "7  What is a model?",
    "section": "7.2 Purpose of a model",
    "text": "7.2 Purpose of a model\nWe use models in two main ways:\n\nPrediction: Suppose we have built a model using the data in a data frame to estimate the price of a home, given its floor_area_sqft. We can potentially use this model to estimate the price of any home whose floor area we know and shose price we do not know – even if this home is not in the dataset.\nExplanation: Our model in Equation 7.1 tells us that the price of a home is related to its floor area. This makes sense and we could have said this even without a model. However, when people gather data about phenomena that they do not fully understand, and want to understand, they often build models to understand and explain which variables seem to be related and in what way to a variable of interest.\n\nSo, for our purposes in this course, we can define the term model as:\n\nA model is a simple story we tell about data to help us summarize, explain, or predict.\n\nYou might have noted that both of the above purposes actually require us to use a model in contexts that go outside the data that we based the model on. When we start using models outside of the data we used to build the model, we come face to face with the essence of statistics. We will get to that later in the course.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>What is a model?</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/intro-to-models.html#average-is-a-model-too",
    "href": "modules/03-introduction-to-models/intro-to-models.html#average-is-a-model-too",
    "title": "7  What is a model?",
    "section": "7.3 Average is a model too!",
    "text": "7.3 Average is a model too!\nIn Equation 7.1, we had a response variable and an explanatory variable. What if we do not have any explanatory variable? In the context of our home prices example, not having an explanatory variable is equivalent to saying:\n\n“I have a dataset of 100 homes”\n“Here are their prices”\n“I am not telling you anything else about these homes”\n“I have selected a random home from my dataset”\n“Guess the price of the home I nhave in mind”\n\nBased on the game we played in Chapter 5, you first compute the average of the prices as $733,440. Your best model to make your guess is:\n\\[\n\\widehat{\\text{price}} = 733{,}440\n\\tag{7.2}\\]\nThat is, like in the game we played, no matter which home I choose, you always state the average as your guess. Equation 7.2 shows the model function when we have no\nThe big difference between the models in Equation 7.1 and Equation 7.2 is that the second one has no explanatory variable. When we are not given any additional helpful information to estimate a value, our best estimate – best model – is the average!",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>What is a model?</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/visualizing-simple-models.html",
    "href": "modules/03-introduction-to-models/visualizing-simple-models.html",
    "title": "8  Visualizing Mean Models",
    "section": "",
    "text": "8.1 Business scenario – fuel efficiency of vehicle fleet\nIn the prior chapters, we have established the overall mean as the best model when we have no other explanatory variable, and the category mean as the simplestbest model* when we have a categorical explanatory variable.\nIn this chapter we use the point_plot function to visualize these models.\nIn Chapter 5, we used a set of numbers to play a game. Now we will extend the idea to actual data frames.\nYou manage a company that operates a fleet of rental cars used primarily for city driving. The fleet includes a wide range of vehicles, from compact sedans to larger SUVs, each with different city fuel efficiency.\nFor planning purposes, the company must commit to one single number to represent the expected city fuel efficiency of a car drawn at random from the fleet. This number is used repeatedly—for estimating fuel costs, setting reimbursement rates, and budgeting operating expenses.\nThe company does not know in advance which specific car will be rented on a given day. Some days the car will be more fuel-efficient than expected, and on other days less fuel-efficient.\nIf the company assumes fuel efficiency that is too high, it underestimates fuel costs. If it assumes fuel efficiency that is too low, it overestimates costs and ties up capital unnecessarily. Larger errors in either direction are more costly than smaller ones.\nThe decision is not evaluated based on a single rental. Instead, it is judged by how well it performs across many rentals over time.\nIn this setting, the question becomes: “What single number should the company commit to so that total long-run cost is as small as possible?”\nThe data about your fleet is in the mpg data. It has many variables, but we focus here on the variable cty showing us the city miles per gallon of each car.\nAssuming (artificially) that the company is only allowed to use the variable cty what single number should they commit to?",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualizing Mean Models</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/visualizing-simple-models.html#business-scenario-fuel-efficiency-of-vehicle-fleet",
    "href": "modules/03-introduction-to-models/visualizing-simple-models.html#business-scenario-fuel-efficiency-of-vehicle-fleet",
    "title": "8  Visualizing Mean Models",
    "section": "",
    "text": "8.1.1 Committing to one number for the entire fleet\nBased on our discussion in Chapter 5, we know that the the company should commit to the average of the variable cty. We can compute the average using the summarize and mean functions as the code below shows.\n\nmpg |&gt; \n  summarize(avg_cty = mean(cty))\n\n\n  \n\n\n\nWe can also visualize it thus:\n\nmpg |&gt; \n  point_plot(cty ~ 1,\n             annot = \"model\",\n             interval = \"none\")\n\n\nThis generates Figure 8.1.\n\n\n\n\n\n\nFigure 8.1: Visualizing mean cty as the model\n\n\n\nThe model shows the scatterplot of the points as well as a line. This line represents the model. you can see it falls exactly at the average of the cty values.\nTake a look at the code:\nThe tilde expression\ncty ~ 1\nin the code sets cty ans=d the response variable and, by placing 1 on the RHS, shows that we have no explanatory variables.\nThe part\nannot = \"model\nCauses a model to be visualized as well. We know that the mean is our model and this plot visualizes it by drawing a line at the average cty.\nFor now, you can ignore:\ninterval = \"none\"\nSo, if we have to commit to one number as our estimate for the city mileage irrespective of other details about a car like its class, drv or anything else, our best commitment is to the overall average. As we have seen in Chapter 7, this is a model with just a response variable and no explanatory variables. We can write it as Equation 8.1.\n\\[\n\\widehat{\\text{cty}} = 16.9\n\\tag{8.1}\\]\nWe placed a hat on cty because this is just an estimate or a model value and not an actual data point.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualizing Mean Models</span>"
    ]
  },
  {
    "objectID": "modules/03-introduction-to-models/visualizing-simple-models.html#we-can-do-better-with-more-information",
    "href": "modules/03-introduction-to-models/visualizing-simple-models.html#we-can-do-better-with-more-information",
    "title": "8  Visualizing Mean Models",
    "section": "8.2 We can do better with more information",
    "text": "8.2 We can do better with more information\nInsted of having to commit to one single number for the fuel efficiency for the entire fleet, what if we could use a different number for each class of vehicle? In out mpg dataset, we have vehicles of different classes – variable class. This has values like Compact, SUV and Pickup truck.\nBased on Chapter 6, we know that for each class of vehicle, we should commit to the average of cty for that class.\nWe can compute these averages easily:\n\nmpg |&gt; \n  summarize(avg_cty = mean(cty), .by = class)\n\n\n  \n\n\n\nNote how the above code is very similar to what we would use for computing the overall average, but we have added:\n.by = class\nTo show that we do not want just one average for the whole data frame, but we want separate averages for each class of vehicle. As simple as that!\nLet us visualize this model:\n\nmpg |&gt; \n  point_plot(cty ~ class, annot = \"model\",\n             interval = \"none\")\n\nFigure 8.2 shows the model visualization.\n\n\n\n\n\n\nFigure 8.2: Visualizing the more sophisticated model of mean cty by class as the model when we have class as an explanatory variable\n\n\n\nHow do we express this model as an equation like we did with Equation 8.1?\nEquation 8.2 shows the model as an equation.\n\\[\n\\widehat{\\text{cty}} =\n\\begin{cases}\n20.1 & \\text{if } \\text{class} = \\text{\"compact\"} \\\\\n18.8 & \\text{if } \\text{class} = \\text{\"midsize\"} \\\\\n13.5 & \\text{if } \\text{class} = \\text{\"SUV\"} \\\\\n15.4 & \\text{if } \\text{class} = \\text{\"2seater\"} \\\\\n15.8 & \\text{if } \\text{class} = \\text{\"minivan\"} \\\\\n13.0 & \\text{if } \\text{class} = \\text{\"pickup\"} \\\\\n20.4 & \\text{if } \\text{class} = \\text{\"subcompact\"}\n\\end{cases}\n\\tag{8.2}\\]\nInstead of class, we can have drv as the explanatory variable as well. Here are code snippets that will compute the model and plot it.\n\nmpg |&gt; \n  summarize(avg_cty = mean(cty), .by = drv)\n\n\n  \n\n\n\nNote that we only had to change the .by to reflect that we are now compouting averages by drv.\n\nmpg |&gt; \n  point_plot(cty ~ drv, annot = \"model\",\n             interval = \"none\")\n\nWe just changed the RHS of the tilde expression to drv instead of class.\nFigure 8.3 shows the visualization.\n\n\n\n\n\n\nFigure 8.3: Visualizing the model of mean cty by drv as the model when we have drv as an explanatory variable\n\n\n\nEquation 8.3 shows the model in equation form.\n\\[\n\\widehat{\\text{cty}} =\n\\begin{cases}\n20.0 & \\text{if } \\text{drv} = \\text{\"f\"} \\\\\n14.3 & \\text{if } \\text{drv} = \\text{\"4\"} \\\\\n14.1 & \\text{if } \\text{drv} = \\text{\"r\"} \\\\\n\\end{cases}\n\\tag{8.3}\\]\nThe mpg data frame has another column fl to represent the fuel type of the vehicle. We now want to use fl as the explanatory variable. Try out the following using the above as examples.\n\nWrite R code to compute the average of cty for each fuel type\nWrite R code to visualize the model with cty as the response variable and fl as the explanatory variable.\nWrite out the model equation for this scenario.",
    "crumbs": [
      "Introduction to models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualizing Mean Models</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/04-overview.html",
    "href": "modules/04-more-on-models/04-overview.html",
    "title": "Module 4: Linear Regression Models",
    "section": "",
    "text": "Learning Goals\nThis module fills in the details on top of the intutional foundation for a linear model that the previous module laid.It introduces the idea of the least squares best fit line and its corresponding model. The module covers R code for building linear models for the case of a single numerical explanatory variable, a single categorical variable and a combination of one numerical and one categorical explanatory variable.It shows how to distill the model function based on the regression output.",
    "crumbs": [
      "More on models -- least squares",
      "Module 4: Linear Regression Models"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/04-overview.html#learning-goals",
    "href": "modules/04-more-on-models/04-overview.html#learning-goals",
    "title": "Module 4: Linear Regression Models",
    "section": "",
    "text": "a\nb\nc",
    "crumbs": [
      "More on models -- least squares",
      "Module 4: Linear Regression Models"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/04-overview.html#structure-of-this-module",
    "href": "modules/04-more-on-models/04-overview.html#structure-of-this-module",
    "title": "Module 4: Linear Regression Models",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nmention that the model gives an estimated value, but defer detailed discussion until later\nR code for building model with no explanatory variable and reconstructing the model function\nR code for building model with one numeric explanatory variable and reconstructing the model function\nR code for building model with one numeric explanatory variable and one categorical explanatory variable and reconstructing the model function",
    "crumbs": [
      "More on models -- least squares",
      "Module 4: Linear Regression Models"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/line-of-best-fit.html",
    "href": "modules/04-more-on-models/line-of-best-fit.html",
    "title": "9  Line of best fit",
    "section": "",
    "text": "9.1 The beach kiosk staffing problem\nIn prior chapters we have looked at the situation when we have a numerical response variable and either no explanatory variable or a single categorical explanatory variable. In this chapter we look at the important case when the response variable and the explanatory variable are numerical.\nWe again consider a situation where the company gets a hint before making a decision—but now the hint is numerical, not categorical.\nThe same company operates sales kiosks at the Beach location. Management has noticed that customer traffic at the beach kiosk varies strongly with the day’s high temperature.\nOver many past days, the company has recorded:\nLet us assume that data are available for 25 different days. (As before, for understanding the ideas, the exact number of days is not important.) In the data\nTable 9.1 shows the historical data.\nBased this, the company must decide how many workers to schedule for tomorrow. It can use an extremely reliable temperature forecast for its city.\nThe problem now is this:",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line of best fit</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/line-of-best-fit.html#the-beach-kiosk-staffing-problem",
    "href": "modules/04-more-on-models/line-of-best-fit.html#the-beach-kiosk-staffing-problem",
    "title": "9  Line of best fit",
    "section": "",
    "text": "the day’s high temperature (in °F), and\nthe number of customers who visited the beach kiosk that day.\n\n\n\n\n\n\nTable 9.1: Daily temperature and customer counts\n\n\n\n\n\nday\ntemperature\ncustomers\n\n\n\n\nD01\n58\n98\n\n\nD02\n60\n104\n\n\nD03\n62\n122\n\n\nD04\n64\n113\n\n\nD05\n66\n117\n\n\nD06\n68\n133\n\n\nD07\n70\n126\n\n\nD08\n72\n115\n\n\nD09\n74\n123\n\n\nD10\n76\n128\n\n\nD11\n78\n145\n\n\nD12\n80\n141\n\n\nD13\n58\n106\n\n\nD14\n60\n107\n\n\nD15\n62\n105\n\n\nD16\n64\n127\n\n\nD17\n66\n120\n\n\nD18\n68\n103\n\n\nD19\n70\n128\n\n\nD20\n72\n121\n\n\nD21\n74\n120\n\n\nD22\n76\n130\n\n\nD23\n78\n127\n\n\nD24\n80\n132\n\n\nD25\n58\n98\n\n\n\n\n\n\n\n\n\nGiven the high temperature for a day, how many customers should the company plan for?\n\n\n9.1.1 How does this differ from earlier examples we have studied?\nIn Chapter 5, we used no hints at all, and the mean turned out to be the best model.\nIn Chapter 6, our hint was categorical, and the mean within each category turned out to be the best model.\nThis time, the situation is fundamentally different. Our hint—temperature—is numerical, and so none of the earlier “mean-based” approaches apply directly.\nOne possibility would be to convert temperature into categories such as Cold, Warm, and Hot, and then apply the category-means approach. While this would work mechanically, it is somewhat artificial. Two days that differ by only one degree could end up in different categories. For example, calling 50°F Cold and 51°F Warm introduces an arbitrary cutoff that has no real business justification.\nTemperature does not naturally fall into clear bins. Can we do better?\n\n\n9.1.2 Toward a rule-based model\nRather than assigning a few separate numbers, the company now wants a rule that:\n\ntakes the day’s temperature as input and produces a staffing recommendation as output.\n\nMany such rules are possible. The company wants the rule that, across all past days, does the best overall job of matching actual customer counts.\nWe know that with higher temperatures more people will hit the beach. This naturally leads us to consider a model of the form that Equation 9.1 shows. We just need to replace a and b with actual numbers.\n\\[\n\\widehat{\\text{customers}} = a +  b\\,\\text{temperature}\n\\tag{9.1}\\]\nIf we fix the values of a and b, this model produces a predicted number of customers for any temperature. For example, if we fix a value of 15 for a and 2 for b, then Equation 9.2 shows our model to determine the number of customers for any temperature.\n\\[\n\\widehat{\\text{customers}} = 15 +  2\\,\\text{temperature}\n\\tag{9.2}\\]\nIts prediction changes smoothly as temperature changes, and it avoids arbitrary cutoffs. We just arbitrarily fixed a and b without any knowledge of how well those choices would perform. What do we mean by how well those choices would perform?\nIf we use the model for many days, then for each day the model gives a prediction and for each day there is an actual number of customers who turn up. The difference between the actual value and the model’s prediction is called a residual.\nSome residuals are positive and others are negative. If we simply added them up, large positive and negative errors could cancel each other out. Squaring the residuals ensures that all errors contribute positively and that larger errors are penalized more heavily.\nWe want the total of all the squared residuals to be as low as we can make it.\nWe can understand things better if we visualize things.",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line of best fit</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/line-of-best-fit.html#visualizing-the-model",
    "href": "modules/04-more-on-models/line-of-best-fit.html#visualizing-the-model",
    "title": "9  Line of best fit",
    "section": "9.2 Visualizing the model",
    "text": "9.2 Visualizing the model\nGraphically, Equation 9.2 represents a straight line. Let us generate a scatterplot of the data first. Figure 9.1 shows the plot.\n\nkiosk |&gt; \n  point_plot(customers ~ temperature)\n\n\n\n\n\n\n\nFigure 9.1: Scatterplot of temperature against number of customers from the kiosk data frame\n\n\n\nLet us then add the model line from Equation 9.2 too on top of it. Right now, we need not concern ourselves with the code. Figure 9.2 shows the scatterplot with the tentative model line.\n\n\n\n\n\n\nFigure 9.2: Adding a line showing the kiosk model with a = 15 and b = 1\n\n\n\nThe model from Equation 9.2 when visualized in Figure 9.2 seems way off the mark. Can you see why?\nWell, the line represents the model predictions and they are way off from the general region where the points actually lie! For example, we see from Figure 9.2 that two days in our historical data have had that temperature. The number of customers for those two days have been 113 and 127. But the tentative model from Equation 9.2 predicts 138. That does not make sense. So, we clearly did not make great choices for a and b.\nIf we want the model predictions to reflect actual data, we would want the line to be within the point cloud. That would make the predictions fall in the general region of the actual values.\nLet us try again with different choices for a and b. How should we change the values? Should we increase or decrease each one?\nWe know that a represents where the line intercepts the y-axis at x = 0. In Figure 9.2, we cannot see that since the x-axis starts from around 55. Let us generate a plot that helps us to looks at both the axes starting from 0.\nEven though a temperature of 0°F is well outside our data range, visualizing the intercept helps us understand what changing a actually does to the line.\nFigure 9.3 shows the plot.\n\n\n\n\n\n\nFigure 9.3: Model with a = 15 and b = 2 plotted to reveal the origin of the plot so as to see the intercept\n\n\n\nFrom the plot, we can see that the line correctly intersects the y-axis at 15 since a = 15. To make the line go through the point cloud, it looks like we need to make the line less steep. The value for b determines the steepness of the line. So, let us decrease it to 1.5 and see what happens.\n\n\n\n\n\n\nFigure 9.4: Second attempt: model with a = 15 and b = 1.5\n\n\n\nMuch better! But can we do better? We can keep on trying – we have an infinite number of choices. Can we find the best line? What does that even mean?",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line of best fit</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/line-of-best-fit.html#what-do-we-mean-by-the-best-model",
    "href": "modules/04-more-on-models/line-of-best-fit.html#what-do-we-mean-by-the-best-model",
    "title": "9  Line of best fit",
    "section": "9.3 What do we mean by the best model?",
    "text": "9.3 What do we mean by the best model?\nIn the previous section, we tried a few values for a and b and our second attempt proved to be much better than the first. However, we want the best line. We now look at what that even means.\nWe are after good predictions. Therefore the difference between the actual value and the model prediction also called residual matters. The lower that difference is the better. Figure 9.5 shows our previous model (a = 15 and b = 1.5) visualized with the error or *residual for three chosen points.\n\n\n\n\n\n\nFigure 9.5: Model with a = 15 and b = 1.5 visualized along with the residual or error for three chosen points\n\n\n\nIn Figure 9.5, we see that for the point at temperature = 62, the actual data had customers = 105, whereas our model predicts 108. SO the model is off by -3 (if we compute the residual as in Equation 9.3.\n\\[\n\\text{residual} = \\text{actual value} - \\text{model prediction}\n\\tag{9.3}\\]\nWe have many points and so we want an overall measure of residuals that takes into account the residuals from all the points, points and not just a few points. Even if the model is spot on for a few points, it is not very useful if it is wildly off for many others.\nAs we did before, we will square the residuals from all the points and add them all up. That is how much error the model has when we consider all data points.\nClearly, we want a line that is as close to all the points as possible overall, and honestly, the line in Figure 9.5 seems to be pretty good at least visually.\nFor this model, the total of all the squared residuals is: 1,469.\nIs there a line that can give a lower value? In fact, what are the values for a and b for which we get the lowest possible total of all the squared residuals?\nSince there is an infinite number of possible choices for a and b, we need a systematic way to find the values that minimize the total of the squared residuals. Mathematically, this problem can be solved using calculus. Practically, we let software do this for us.\n\nkiosk |&gt; \n  model_train(customers ~ temperature) |&gt; \n  coef()\n\n(Intercept) temperature \n      22.06        1.42 \n\n\nWe see two values in the output: intercept and temperature. The first is a and the second is b! These are exactly the values of a and b that minimize the sum of squared residuals.\nEquation 9.4 shows the best model for our problem, given that we want a straight line and want to minimize the sum of squared residuals is:\n\\[\n\\widehat{\\text{customers}} = 22.06 +  1.42\\,\\text{temperature}\n\\tag{9.4}\\]\nThe optional enrichment topic at the end of the chapter talks about possibilities other than straight lines.\nLet us now see if the sum of squared residuals is smaller than what we obtained for our previous choices for a and b: 1,469.\nThe value is: 1,387. Lower, as we expected. But not by much. We had eyeballed very good values for a and b!\nWe found a line that minimizes the sum of squared residuals. Technically this is called the least-squares criterion and plays a very important role in statistics and data analysis.",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line of best fit</span>"
    ]
  },
  {
    "objectID": "modules/04-more-on-models/line-of-best-fit.html#optional-enrichment-topic-beond-straight-lines",
    "href": "modules/04-more-on-models/line-of-best-fit.html#optional-enrichment-topic-beond-straight-lines",
    "title": "9  Line of best fit",
    "section": "9.4 Optional enrichment topic: Beond straight lines",
    "text": "9.4 Optional enrichment topic: Beond straight lines\nIn this chapter, we have assumed that a straight line is a reasonable model for the relationship between temperature and customer traffic. In many business settings, this assumption works well: simple models are easy to interpret, easy to communicate, and often good enough for decision-making.\nHowever, not all relationships are well captured by a straight line.\n\n9.4.1 When a straight line may not be appropriate\nIn some situations, a scatterplot may reveal patterns such as:\n\ncurvature (the relationship bends),\ndiminishing returns (the effect of increases slows down), or\nthreshold effects (behavior changes after a certain point).\n\nFor example, at very high temperatures, customer traffic at a beach kiosk might level off or even decline as conditions become uncomfortable. A straight line cannot capture this kind of behavior well, no matter how its slope and intercept are chosen.\nIn such cases, forcing a linear model may lead to systematic residual patterns, indicating that the model is missing something important.\n\n\n9.4.2 A flexible alternative: LOWESS smoothing\nOne way to explore nonlinear relationships is through a technique called LOWESS (short for locally weighted scatterplot smoothing).\nFigure 9.6 shows an example of a LOWESS model using the mpg data frame. In this data frame the displacement of an engine is related to the highway mileage hwy, but not in a linear way. The LOWESS model better describes it than a straight line would.\n\n\n\n\n\n\nFigure 9.6: LOWESS model for displacement vs cty in the mpg data frame showing how it captures the curvature in the relationship better than a lien can\n\n\n\nRather than fitting a single global rule like a straight line, LOWESS works by:\n\nfocusing on a small neighborhood of points around each value of the explanatory variable, and\nfitting simple local models that adapt to the data in that neighborhood.\n\nThe result is a smooth curve that follows the overall pattern of the data without requiring us to specify a particular functional form in advance.\nLOWESS is especially useful for:\n\nexploratory analysis,\nvisualizing trends, and\ndiagnosing whether a straight-line model is reasonable.\n\nIt is not intended to replace regression models in all cases, but rather to help us understand the structure of the data.\n\n\n9.4.3 Models as choices, not defaults\nThe key takeaway is not that linear models are “wrong,” but that models are choices.\n\nA straight line is a good choice when the relationship is approximately linear and interpretability matters.\nA nonlinear model may be a better choice when the data clearly suggest curvature or changing behavior.\nFlexible tools like LOWESS help us see what the data are trying to tell us before we commit to a specific model.\n\nIn all cases, the central modeling question remains the same:\n\nDoes this model capture the important structure in the data well enough to support the decisions we want to make?\n\nThis mindset—treating models as purposeful approximations rather than automatic formulas—will guide us throughout the rest of this book.",
    "crumbs": [
      "More on models -- least squares",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line of best fit</span>"
    ]
  },
  {
    "objectID": "modules/05-regression-no-ci/05-overview.html",
    "href": "modules/05-regression-no-ci/05-overview.html",
    "title": "Module 5: Linear Regression Models",
    "section": "",
    "text": "Learning Goals\nThis module fills in the details on top of the intutional foundation for a linear model that the previous module laid.It introduces the idea of the least squares best fit line and its corresponding model. The module covers R code for building linear models for the case of a single numerical explanatory variable, a single categorical variable and a combination of one numerical and one categorical explanatory variable.It shows how to distill the model function based on the regression output.",
    "crumbs": [
      "Linear Regression Models",
      "Module 5: Linear Regression Models"
    ]
  },
  {
    "objectID": "modules/05-regression-no-ci/05-overview.html#learning-goals",
    "href": "modules/05-regression-no-ci/05-overview.html#learning-goals",
    "title": "Module 5: Linear Regression Models",
    "section": "",
    "text": "a\nb\nc",
    "crumbs": [
      "Linear Regression Models",
      "Module 5: Linear Regression Models"
    ]
  },
  {
    "objectID": "modules/05-regression-no-ci/05-overview.html#structure-of-this-module",
    "href": "modules/05-regression-no-ci/05-overview.html#structure-of-this-module",
    "title": "Module 5: Linear Regression Models",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nNotion of model function\nmention that the model gives an estimated value, but defer detailed discussion until later\nR code for building model with no explanatory variable and reconstructing the model function\nR code for building model with one numeric explanatory variable and reconstructing the model function\nR code for building model with one numeric explanatory variable and one categorical explanatory variable and reconstructing the model function",
    "crumbs": [
      "Linear Regression Models",
      "Module 5: Linear Regression Models"
    ]
  },
  {
    "objectID": "modules/05-regression-no-ci/line-model.html",
    "href": "modules/05-regression-no-ci/line-model.html",
    "title": "10  Line Models",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.",
    "crumbs": [
      "Linear Regression Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Line Models</span>"
    ]
  },
  {
    "objectID": "modules/05-regression-no-ci/interpreting-coefficients.html",
    "href": "modules/05-regression-no-ci/interpreting-coefficients.html",
    "title": "11  Interpreting coefficients",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Linear Regression Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Interpreting coefficients</span>"
    ]
  },
  {
    "objectID": "modules/06-variance-analysis/06-overview.html",
    "href": "modules/06-variance-analysis/06-overview.html",
    "title": "Module 6: Explanatory Power of a Model",
    "section": "",
    "text": "Learning Goals\nWe turn our attention now to measuring the quality of a model based on the notion of a model’s ability to explain the variability in the response variable.",
    "crumbs": [
      "Explanatory Power of a Model",
      "Module 6: Explanatory Power of a Model"
    ]
  },
  {
    "objectID": "modules/06-variance-analysis/06-overview.html#learning-goals",
    "href": "modules/06-variance-analysis/06-overview.html#learning-goals",
    "title": "Module 6: Explanatory Power of a Model",
    "section": "",
    "text": "a\nb\nc",
    "crumbs": [
      "Explanatory Power of a Model",
      "Module 6: Explanatory Power of a Model"
    ]
  },
  {
    "objectID": "modules/06-variance-analysis/06-overview.html#structure-of-this-module",
    "href": "modules/06-variance-analysis/06-overview.html#structure-of-this-module",
    "title": "Module 6: Explanatory Power of a Model",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nWhat does it mean for a model to explain variability in the response variable?\nComputing the model values and residuals\nthe variance equation\nR-squared",
    "crumbs": [
      "Explanatory Power of a Model",
      "Module 6: Explanatory Power of a Model"
    ]
  },
  {
    "objectID": "modules/07-populations-samples/07-overview.html",
    "href": "modules/07-populations-samples/07-overview.html",
    "title": "Module 7: Population, Samples and Inference",
    "section": "",
    "text": "Learning Goals\nAt this point, we have learned about models in general, and linear regression models in particular. In addition, we can also now use R to build linear models and to construct the model function based on the output of the R code. We have also looked at the quality of a model based on its R-squared.\nUntil now we have looked at a mdoel as a mathematical function into which we can plug in the value of explanatory variable(s) and get an estimated value for the response variable.\nWe now transition from mathematical functions to statistical models. We first define the terms between population and sample and reveal that much of statistics deals with inference, that is, learning something about the population based on just a sample.",
    "crumbs": [
      "Populations, Samples and Inference",
      "Module 7: Population, Samples and Inference"
    ]
  },
  {
    "objectID": "modules/07-populations-samples/07-overview.html#learning-goals",
    "href": "modules/07-populations-samples/07-overview.html#learning-goals",
    "title": "Module 7: Population, Samples and Inference",
    "section": "",
    "text": "a\nb\nc",
    "crumbs": [
      "Populations, Samples and Inference",
      "Module 7: Population, Samples and Inference"
    ]
  },
  {
    "objectID": "modules/07-populations-samples/07-overview.html#structure-of-this-module",
    "href": "modules/07-populations-samples/07-overview.html#structure-of-this-module",
    "title": "Module 7: Population, Samples and Inference",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nPopulation and sample\nStatistc",
    "crumbs": [
      "Populations, Samples and Inference",
      "Module 7: Population, Samples and Inference"
    ]
  },
  {
    "objectID": "modules/07-populations-samples/pop-vs-sample.html",
    "href": "modules/07-populations-samples/pop-vs-sample.html",
    "title": "12  Population vs Sample",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Populations, Samples and Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Population vs Sample</span>"
    ]
  },
  {
    "objectID": "modules/07-populations-samples/sampling-variability.html",
    "href": "modules/07-populations-samples/sampling-variability.html",
    "title": "13  Sampling Variability",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Populations, Samples and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sampling Variability</span>"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/08-overview.html",
    "href": "modules/08-probability-variation/08-overview.html",
    "title": "Module 8: Probability as Variation",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Probability as Variation",
      "Module 8: Probability as Variation"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/08-overview.html#learning-goals",
    "href": "modules/08-probability-variation/08-overview.html#learning-goals",
    "title": "Module 8: Probability as Variation",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Probability as Variation",
      "Module 8: Probability as Variation"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/08-overview.html#structure-of-this-module",
    "href": "modules/08-probability-variation/08-overview.html#structure-of-this-module",
    "title": "Module 8: Probability as Variation",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Probability as Variation",
      "Module 8: Probability as Variation"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/randomness.html",
    "href": "modules/08-probability-variation/randomness.html",
    "title": "14  Randomness",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Probability as Variation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Randomness</span>"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/variability.html",
    "href": "modules/08-probability-variation/variability.html",
    "title": "15  Variability",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Probability as Variation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variability</span>"
    ]
  },
  {
    "objectID": "modules/08-probability-variation/distributions.html",
    "href": "modules/08-probability-variation/distributions.html",
    "title": "16  Distributions",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Probability as Variation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "modules/09-sampling-variation/09-overview.html",
    "href": "modules/09-sampling-variation/09-overview.html",
    "title": "Module 9: Sampling Variation and Estimators",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Sampling Variation and Estimators",
      "Module 9: Sampling Variation and Estimators"
    ]
  },
  {
    "objectID": "modules/09-sampling-variation/09-overview.html#learning-goals",
    "href": "modules/09-sampling-variation/09-overview.html#learning-goals",
    "title": "Module 9: Sampling Variation and Estimators",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Sampling Variation and Estimators",
      "Module 9: Sampling Variation and Estimators"
    ]
  },
  {
    "objectID": "modules/09-sampling-variation/09-overview.html#structure-of-this-module",
    "href": "modules/09-sampling-variation/09-overview.html#structure-of-this-module",
    "title": "Module 9: Sampling Variation and Estimators",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Sampling Variation and Estimators",
      "Module 9: Sampling Variation and Estimators"
    ]
  },
  {
    "objectID": "modules/09-sampling-variation/se-estimators.html",
    "href": "modules/09-sampling-variation/se-estimators.html",
    "title": "17  SE Estimators",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Sampling Variation and Estimators",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SE Estimators</span>"
    ]
  },
  {
    "objectID": "modules/09-sampling-variation/sampling-dist-slopes.html",
    "href": "modules/09-sampling-variation/sampling-dist-slopes.html",
    "title": "18  Sampling Distributions Slopes",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Sampling Variation and Estimators",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sampling Distributions Slopes</span>"
    ]
  },
  {
    "objectID": "modules/10-confidence-bands/10-overview.html",
    "href": "modules/10-confidence-bands/10-overview.html",
    "title": "Module 10: Confidence Intervals and Bands",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Confidence Intervals and Bands",
      "Module 10: Confidence Intervals and Bands"
    ]
  },
  {
    "objectID": "modules/10-confidence-bands/10-overview.html#learning-goals",
    "href": "modules/10-confidence-bands/10-overview.html#learning-goals",
    "title": "Module 10: Confidence Intervals and Bands",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Confidence Intervals and Bands",
      "Module 10: Confidence Intervals and Bands"
    ]
  },
  {
    "objectID": "modules/10-confidence-bands/10-overview.html#structure-of-this-module",
    "href": "modules/10-confidence-bands/10-overview.html#structure-of-this-module",
    "title": "Module 10: Confidence Intervals and Bands",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Confidence Intervals and Bands",
      "Module 10: Confidence Intervals and Bands"
    ]
  },
  {
    "objectID": "modules/10-confidence-bands/ci-slope.html",
    "href": "modules/10-confidence-bands/ci-slope.html",
    "title": "19  CI Slope",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Confidence Intervals and Bands",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>CI Slope</span>"
    ]
  },
  {
    "objectID": "modules/10-confidence-bands/ci-bands.html",
    "href": "modules/10-confidence-bands/ci-bands.html",
    "title": "20  CI Bands",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Confidence Intervals and Bands",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>CI Bands</span>"
    ]
  },
  {
    "objectID": "modules/11-signal-noise/11-overview.html",
    "href": "modules/11-signal-noise/11-overview.html",
    "title": "Module 11: Signal and Noise",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Signal, Noise, and R-Squared",
      "Module 11: Signal and Noise"
    ]
  },
  {
    "objectID": "modules/11-signal-noise/11-overview.html#learning-goals",
    "href": "modules/11-signal-noise/11-overview.html#learning-goals",
    "title": "Module 11: Signal and Noise",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Signal, Noise, and R-Squared",
      "Module 11: Signal and Noise"
    ]
  },
  {
    "objectID": "modules/11-signal-noise/11-overview.html#structure-of-this-module",
    "href": "modules/11-signal-noise/11-overview.html#structure-of-this-module",
    "title": "Module 11: Signal and Noise",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Signal, Noise, and R-Squared",
      "Module 11: Signal and Noise"
    ]
  },
  {
    "objectID": "modules/11-signal-noise/variance-decomposition.html",
    "href": "modules/11-signal-noise/variance-decomposition.html",
    "title": "21  Variance Decomposition",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Signal, Noise, and R-Squared",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Variance Decomposition</span>"
    ]
  },
  {
    "objectID": "modules/11-signal-noise/r-squared.html",
    "href": "modules/11-signal-noise/r-squared.html",
    "title": "22  R-Squared",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Signal, Noise, and R-Squared",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>R-Squared</span>"
    ]
  },
  {
    "objectID": "modules/12-hypothesis-testing/12-overview.html",
    "href": "modules/12-hypothesis-testing/12-overview.html",
    "title": "Module 12: Hypothesis Testing",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Hypothesis Testing via Regression",
      "Module 12: Hypothesis Testing"
    ]
  },
  {
    "objectID": "modules/12-hypothesis-testing/12-overview.html#learning-goals",
    "href": "modules/12-hypothesis-testing/12-overview.html#learning-goals",
    "title": "Module 12: Hypothesis Testing",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Hypothesis Testing via Regression",
      "Module 12: Hypothesis Testing"
    ]
  },
  {
    "objectID": "modules/12-hypothesis-testing/12-overview.html#structure-of-this-module",
    "href": "modules/12-hypothesis-testing/12-overview.html#structure-of-this-module",
    "title": "Module 12: Hypothesis Testing",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Hypothesis Testing via Regression",
      "Module 12: Hypothesis Testing"
    ]
  },
  {
    "objectID": "modules/12-hypothesis-testing/t-tests-regression.html",
    "href": "modules/12-hypothesis-testing/t-tests-regression.html",
    "title": "23  t-tests for Regression",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Hypothesis Testing via Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>t-tests for Regression</span>"
    ]
  },
  {
    "objectID": "modules/12-hypothesis-testing/interpretation.html",
    "href": "modules/12-hypothesis-testing/interpretation.html",
    "title": "24  Interpretation",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Hypothesis Testing via Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Interpretation</span>"
    ]
  },
  {
    "objectID": "modules/13-business-applications/13-overview.html",
    "href": "modules/13-business-applications/13-overview.html",
    "title": "Module 13: Business Applications",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Business Applications of Regression",
      "Module 13: Business Applications"
    ]
  },
  {
    "objectID": "modules/13-business-applications/13-overview.html#learning-goals",
    "href": "modules/13-business-applications/13-overview.html#learning-goals",
    "title": "Module 13: Business Applications",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Business Applications of Regression",
      "Module 13: Business Applications"
    ]
  },
  {
    "objectID": "modules/13-business-applications/13-overview.html#structure-of-this-module",
    "href": "modules/13-business-applications/13-overview.html#structure-of-this-module",
    "title": "Module 13: Business Applications",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Business Applications of Regression",
      "Module 13: Business Applications"
    ]
  },
  {
    "objectID": "modules/13-business-applications/marketing.html",
    "href": "modules/13-business-applications/marketing.html",
    "title": "25  Marketing",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Business Applications of Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Marketing</span>"
    ]
  },
  {
    "objectID": "modules/13-business-applications/finance.html",
    "href": "modules/13-business-applications/finance.html",
    "title": "26  Finance",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Business Applications of Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Finance</span>"
    ]
  },
  {
    "objectID": "modules/13-business-applications/hr.html",
    "href": "modules/13-business-applications/hr.html",
    "title": "27  Human Resources",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Business Applications of Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Human Resources</span>"
    ]
  },
  {
    "objectID": "modules/13-business-applications/operations.html",
    "href": "modules/13-business-applications/operations.html",
    "title": "28  Operations",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Business Applications of Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Operations</span>"
    ]
  },
  {
    "objectID": "modules/14-projects/14-overview.html",
    "href": "modules/14-projects/14-overview.html",
    "title": "Module 14: Projects",
    "section": "",
    "text": "Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Projects and Datasets",
      "Module 14: Projects"
    ]
  },
  {
    "objectID": "modules/14-projects/14-overview.html#learning-goals",
    "href": "modules/14-projects/14-overview.html#learning-goals",
    "title": "Module 14: Projects",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Projects and Datasets",
      "Module 14: Projects"
    ]
  },
  {
    "objectID": "modules/14-projects/14-overview.html#structure-of-this-module",
    "href": "modules/14-projects/14-overview.html#structure-of-this-module",
    "title": "Module 14: Projects",
    "section": "Structure of This Module",
    "text": "Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Projects and Datasets",
      "Module 14: Projects"
    ]
  },
  {
    "objectID": "modules/14-projects/project-guidelines.html",
    "href": "modules/14-projects/project-guidelines.html",
    "title": "29  Project Guidelines",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Projects and Datasets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Project Guidelines</span>"
    ]
  },
  {
    "objectID": "modules/14-projects/datasets.html",
    "href": "modules/14-projects/datasets.html",
    "title": "30  Datasets",
    "section": "",
    "text": "The mean of a numerical variable is the “best guess” of its value when no other information is available — in the specific sense that it minimizes the sum of squared errors.\nlibrary(ggplot2) mean(mtcars$mpg)",
    "crumbs": [
      "Projects and Datasets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "modules/Appendices/01-overview.html",
    "href": "modules/Appendices/01-overview.html",
    "title": "31  Appendices",
    "section": "",
    "text": "31.1 Learning Goals\nThis module introduces the basic building blocks of statistical thinking: data frames, variables, and point plots. We begin with visualization because patterns reveal what models will later formalize.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "modules/Appendices/01-overview.html#learning-goals",
    "href": "modules/Appendices/01-overview.html#learning-goals",
    "title": "31  Appendices",
    "section": "",
    "text": "Understand data frames, variables, and instances\n\nCreate and interpret point plots\n\nRecognize patterns: direction, form, strength",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "modules/Appendices/01-overview.html#structure-of-this-module",
    "href": "modules/Appendices/01-overview.html#structure-of-this-module",
    "title": "31  Appendices",
    "section": "31.2 Structure of This Module",
    "text": "31.2 Structure of This Module\n\nData frames\n\nPoint plots\n\nRelationships between variables",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Appendices</span>"
    ]
  }
]